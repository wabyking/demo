Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=10, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output1', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.3862944841384888 0.6931472420692444, 0.6931472420692444
Loss in 100 steps: 1.2567952871322632 0.571567952632904, 0.6852272748947144
Loss in 200 steps: 1.2033872604370117 0.5607032775878906, 0.6426839232444763
Loss in 300 steps: 1.151348352432251 0.5415176749229431, 0.6098307371139526
Loss in 400 steps: 1.1565438508987427 0.5594688057899475, 0.5970751047134399
Loss in 500 steps: 1.1830217838287354 0.585991621017456, 0.5970302224159241
Loss in 600 steps: 1.1840895414352417 0.586637556552887, 0.5974520444869995
Loss in 700 steps: 1.1999911069869995 0.6036170721054077, 0.5963740944862366
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=10, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output1', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.1489293575286865 0.554327130317688, 0.594602108001709
Loss in 100 steps: 1.1385529041290283 0.5443422198295593, 0.5942108035087585
Loss in 200 steps: 1.1509102582931519 0.5562506914138794, 0.5946594476699829
Loss in 300 steps: 1.1233724355697632 0.5335756540298462, 0.5897967219352722
Loss in 400 steps: 1.1348035335540771 0.5516999959945679, 0.5831034183502197
Loss in 500 steps: 1.163947582244873 0.5785368084907532, 0.5854107737541199
Loss in 600 steps: 1.1343246698379517 0.5520325303077698, 0.5822920799255371
Loss in 700 steps: 1.1322647333145142 0.5566056370735168, 0.5756591558456421
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=10, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output1', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.1327462196350098 0.5599281191825867, 0.5728181600570679
Loss in 100 steps: 1.0978318452835083 0.5279272794723511, 0.569904625415802
Loss in 200 steps: 1.1215068101882935 0.5482423305511475, 0.5732645392417908
Loss in 300 steps: 1.090591311454773 0.5253917574882507, 0.5651995539665222
Loss in 400 steps: 1.0921250581741333 0.5346124172210693, 0.5575125813484192
Loss in 500 steps: 1.1213423013687134 0.5544009208679199, 0.5669415593147278
Loss in 600 steps: 1.1125195026397705 0.5474668145179749, 0.5650525689125061
Loss in 700 steps: 1.1100744009017944 0.5472425222396851, 0.5628319382667542
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=10, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output1', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0802146196365356 0.5250387191772461, 0.5551759600639343
Loss in 100 steps: 1.085398554801941 0.5312855243682861, 0.55411297082901
Loss in 200 steps: 1.0952345132827759 0.5311650633811951, 0.5640694499015808
Loss in 300 steps: 1.072258472442627 0.5138559937477112, 0.5584024786949158
Loss in 400 steps: 1.0871639251708984 0.5365625619888306, 0.5506013035774231
Loss in 500 steps: 1.1103901863098145 0.5489442944526672, 0.5614457726478577
Loss in 600 steps: 1.0862972736358643 0.5265769958496094, 0.5597203373908997
Loss in 700 steps: 1.0901548862457275 0.5345442891120911, 0.5556106567382812
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=10, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output1', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0930527448654175 0.5481046438217163, 0.5449481010437012
Loss in 100 steps: 1.065338134765625 0.5161163210868835, 0.5492217540740967
Loss in 200 steps: 1.0813602209091187 0.52093106508255, 0.560429036617279
Loss in 300 steps: 1.0567411184310913 0.508385956287384, 0.5483551025390625
Loss in 400 steps: 1.064233660697937 0.524278998374939, 0.539954662322998
Loss in 500 steps: 1.0835474729537964 0.5302453637123108, 0.5533021688461304
Loss in 600 steps: 1.0803077220916748 0.5270563364028931, 0.553251326084137
Loss in 700 steps: 1.0725889205932617 0.5218342542648315, 0.5507546663284302
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=10, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output1', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0558428764343262 0.5225978493690491, 0.5332450270652771
Loss in 100 steps: 1.0546966791152954 0.521574854850769, 0.5331218242645264
Loss in 200 steps: 1.0701642036437988 0.5192269086837769, 0.5509371757507324
Loss in 300 steps: 1.0497533082962036 0.5057756900787354, 0.5439777374267578
Loss in 400 steps: 1.0558463335037231 0.5213872194290161, 0.534459114074707
Loss in 500 steps: 1.0713813304901123 0.5229120254516602, 0.5484693050384521
Loss in 600 steps: 1.0613446235656738 0.5117633938789368, 0.5495811700820923
Loss in 700 steps: 1.0664818286895752 0.5205248594284058, 0.5459567904472351
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=10, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output1', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0650845766067505 0.5388667583465576, 0.5262178182601929
Loss in 100 steps: 1.0416744947433472 0.5047100186347961, 0.536964476108551
Loss in 200 steps: 1.065855622291565 0.509444534778595, 0.5564112067222595
Loss in 300 steps: 1.0476466417312622 0.5072740316390991, 0.5403726100921631
Loss in 400 steps: 1.049243688583374 0.5188009738922119, 0.5304426550865173
Loss in 500 steps: 1.0685161352157593 0.5242686867713928, 0.5442474484443665
Loss in 600 steps: 1.0697914361953735 0.5231962203979492, 0.5465952157974243
Loss in 700 steps: 1.0499299764633179 0.5063932538032532, 0.5435368418693542
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=10, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output1', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0333749055862427 0.5112348198890686, 0.5221400856971741
Loss in 100 steps: 1.0303248167037964 0.5080937743186951, 0.5222311615943909
Loss in 200 steps: 1.0536147356033325 0.5100936889648438, 0.5435211658477783
Loss in 300 steps: 1.0280827283859253 0.4925064444541931, 0.535576343536377
Loss in 400 steps: 1.0360697507858276 0.5116527676582336, 0.5244168639183044
Loss in 500 steps: 1.0767881870269775 0.5342501401901245, 0.5425381064414978
Loss in 600 steps: 1.0373623371124268 0.4955690801143646, 0.5417932271957397
Loss in 700 steps: 1.0538932085037231 0.512310802936554, 0.5415824055671692
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=10, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output1', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.05153489112854 0.5349652171134949, 0.5165696144104004
Loss in 100 steps: 1.0301481485366821 0.5001146197319031, 0.5300337076187134
Loss in 200 steps: 1.053746223449707 0.5045878887176514, 0.5491583943367004
Loss in 300 steps: 1.0367270708084106 0.5016841888427734, 0.5350428223609924
Loss in 400 steps: 1.0350453853607178 0.5115402340888977, 0.5235050916671753
Loss in 500 steps: 1.0716103315353394 0.5315361022949219, 0.5400742292404175
Loss in 600 steps: 1.0462292432785034 0.5040822625160217, 0.5421469211578369
Loss in 700 steps: 1.048819661140442 0.5086333155632019, 0.54018634557724
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=10, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output1', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0201199054718018 0.5035275220870972, 0.5165923833847046
Loss in 100 steps: 1.0242995023727417 0.5066416263580322, 0.5176578164100647
Loss in 200 steps: 1.0370763540267944 0.5012669563293457, 0.535809338092804
Loss in 300 steps: 1.0303651094436646 0.49776068329811096, 0.532604455947876
Loss in 400 steps: 1.0304498672485352 0.5092273354530334, 0.5212225317955017
Loss in 500 steps: 1.064515233039856 0.5284476280212402, 0.5360676050186157
Loss in 600 steps: 1.0535563230514526 0.5135339498519897, 0.5400223731994629
Loss in 700 steps: 1.0496442317962646 0.5120851993560791, 0.5375587940216064
