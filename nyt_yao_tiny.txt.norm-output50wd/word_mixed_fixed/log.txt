Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=20, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50wd', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0.0001, years=30)
Loss in 0 steps: 1.3862944841384888 0.6931472420692444, 0.6931472420692444
Loss in 100 steps: 1.373679280281067 0.6768665909767151, 0.6968127489089966
Loss in 200 steps: 1.3710179328918457 0.6707562208175659, 0.700261652469635
Loss in 300 steps: 1.3658151626586914 0.6651206612586975, 0.7006945610046387
Loss in 400 steps: 1.3684072494506836 0.6683782339096069, 0.7000288367271423
Loss in 500 steps: 1.3669326305389404 0.6652876734733582, 0.701645016670227
Loss in 600 steps: 1.3667551279067993 0.6653629541397095, 0.7013922333717346
Loss in 700 steps: 1.3643180131912231 0.6612799167633057, 0.7030380964279175
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=20, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50wd', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0.0001, years=30)
Loss in 0 steps: 1.37069833278656 0.668225884437561, 0.702472448348999
Loss in 100 steps: 1.3667824268341064 0.6644564867019653, 0.7023258805274963
Loss in 200 steps: 1.3682423830032349 0.6656662821769714, 0.7025760412216187
Loss in 300 steps: 1.3639070987701416 0.6606579422950745, 0.7032492160797119
Loss in 400 steps: 1.3668346405029297 0.663833498954773, 0.7030010223388672
Loss in 500 steps: 1.3656891584396362 0.6609230637550354, 0.7047659158706665
Loss in 600 steps: 1.3617910146713257 0.656279981136322, 0.7055110931396484
Loss in 700 steps: 1.3607465028762817 0.6535037755966187, 0.7072427272796631
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=20, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50wd', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0.0001, years=30)
Loss in 0 steps: 1.364898443222046 0.658087968826294, 0.7068105340003967
Loss in 100 steps: 1.3606973886489868 0.6533122658729553, 0.7073851227760315
Loss in 200 steps: 1.3618122339248657 0.6542796492576599, 0.7075327634811401
Loss in 300 steps: 1.3593956232070923 0.6511734127998352, 0.7082222104072571
Loss in 400 steps: 1.362998127937317 0.655640721321106, 0.7073572874069214
Loss in 500 steps: 1.3595280647277832 0.6502658128738403, 0.7092622518539429
Loss in 600 steps: 1.358008623123169 0.6488260626792908, 0.709182620048523
Loss in 700 steps: 1.3550560474395752 0.6445268392562866, 0.7105292677879333
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=20, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50wd', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0.0001, years=30)
Loss in 0 steps: 1.3634002208709717 0.6542599201202393, 0.7091402411460876
Loss in 100 steps: 1.3582603931427002 0.6483226418495178, 0.7099379301071167
Loss in 200 steps: 1.3578299283981323 0.6477468013763428, 0.7100830674171448
Loss in 300 steps: 1.356967568397522 0.6468772888183594, 0.7100903391838074
Loss in 400 steps: 1.3613181114196777 0.6523863077163696, 0.7089318037033081
Loss in 500 steps: 1.3597854375839233 0.6498047709465027, 0.7099806070327759
Loss in 600 steps: 1.3542287349700928 0.643134593963623, 0.711094081401825
Loss in 700 steps: 1.3543434143066406 0.6424911618232727, 0.7118522524833679
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=20, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50wd', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0.0001, years=30)
Loss in 0 steps: 1.3607690334320068 0.6501901745796204, 0.7105787396430969
Loss in 100 steps: 1.3569588661193848 0.6459611058235168, 0.7109978199005127
Loss in 200 steps: 1.3562707901000977 0.6453569531440735, 0.7109137773513794
Loss in 300 steps: 1.3559035062789917 0.6449876427650452, 0.7109159231185913
Loss in 400 steps: 1.3612388372421265 0.6517091393470764, 0.7095298171043396
Loss in 500 steps: 1.3602699041366577 0.6500298380851746, 0.7102401256561279
Loss in 600 steps: 1.3568789958953857 0.6461946964263916, 0.7106842994689941
Loss in 700 steps: 1.3533531427383423 0.6411370038986206, 0.7122161388397217
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=20, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50wd', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0.0001, years=30)
Loss in 0 steps: 1.3605618476867676 0.6499388217926025, 0.7106232047080994
Loss in 100 steps: 1.3565895557403564 0.6454362273216248, 0.7111532688140869
Loss in 200 steps: 1.3569163084030151 0.6458617448806763, 0.7110546827316284
Loss in 300 steps: 1.3566240072250366 0.6457160115242004, 0.710908055305481
Loss in 400 steps: 1.3612024784088135 0.651751697063446, 0.7094508409500122
Loss in 500 steps: 1.3577059507369995 0.6467410326004028, 0.7109649181365967
Loss in 600 steps: 1.3526344299316406 0.6407631039619446, 0.711871325969696
Loss in 700 steps: 1.3537099361419678 0.6412063241004944, 0.7125035524368286
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=20, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50wd', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0.0001, years=30)
Loss in 0 steps: 1.3597592115402222 0.648664116859436, 0.7110950946807861
Loss in 100 steps: 1.3566882610321045 0.6455145478248596, 0.7111736536026001
Loss in 200 steps: 1.356735348701477 0.6454777121543884, 0.7112576365470886
Loss in 300 steps: 1.3561116456985474 0.6449998617172241, 0.7111117243766785
Loss in 400 steps: 1.3606623411178589 0.6508839130401611, 0.7097784280776978
Loss in 500 steps: 1.359995722770691 0.6494846940040588, 0.7105109691619873
Loss in 600 steps: 1.3576850891113281 0.6470731496810913, 0.710611879825592
Loss in 700 steps: 1.3535263538360596 0.6410475373268127, 0.712478756904602
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=20, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50wd', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0.0001, years=30)
Loss in 0 steps: 1.359135389328003 0.6477937698364258, 0.7113417387008667
Loss in 100 steps: 1.356622576713562 0.6453322768211365, 0.7112904191017151
Loss in 200 steps: 1.3565903902053833 0.6452353000640869, 0.7113550901412964
Loss in 300 steps: 1.3562785387039185 0.6452497839927673, 0.7110288143157959
Loss in 400 steps: 1.36069917678833 0.6510364413261414, 0.7096627354621887
Loss in 500 steps: 1.3568509817123413 0.6455041766166687, 0.7113466858863831
Loss in 600 steps: 1.352152943611145 0.6401566863059998, 0.7119962573051453
Loss in 700 steps: 1.3531467914581299 0.6406801342964172, 0.7124667763710022
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=20, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50wd', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0.0001, years=30)
Loss in 0 steps: 1.3606863021850586 0.6499792337417603, 0.7107071280479431
Loss in 100 steps: 1.357119083404541 0.6459202170372009, 0.7111988067626953
Loss in 200 steps: 1.3572102785110474 0.6459637880325317, 0.7112464904785156
Loss in 300 steps: 1.3566513061523438 0.6456905007362366, 0.7109608054161072
Loss in 400 steps: 1.3618767261505127 0.6523302793502808, 0.7095463275909424
Loss in 500 steps: 1.3565247058868408 0.6450641751289368, 0.7114605903625488
Loss in 600 steps: 1.3554527759552002 0.6441709995269775, 0.7112817764282227
Loss in 700 steps: 1.3536529541015625 0.6411257386207581, 0.7125272154808044
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=20, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50wd', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0.0001, years=30)
Loss in 0 steps: 1.3608371019363403 0.6500223278999329, 0.710814893245697
Loss in 100 steps: 1.3566354513168335 0.645407497882843, 0.7112278938293457
Loss in 200 steps: 1.356811761856079 0.6455625295639038, 0.7112493515014648
Loss in 300 steps: 1.3563274145126343 0.6454996466636658, 0.7108278274536133
Loss in 400 steps: 1.3609788417816162 0.6511864066123962, 0.70979243516922
Loss in 500 steps: 1.356213092803955 0.6447444558143616, 0.7114685773849487
Loss in 600 steps: 1.3574286699295044 0.6465840935707092, 0.7108446359634399
Loss in 700 steps: 1.3536553382873535 0.6412479281425476, 0.7124072909355164
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=20, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50wd', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0.0001, years=30)
Loss in 0 steps: 1.3606561422348022 0.6500681042671204, 0.7105880379676819
Loss in 100 steps: 1.3566466569900513 0.6452992558479309, 0.7113474607467651
Loss in 200 steps: 1.3564598560333252 0.6452435255050659, 0.7112162709236145
Loss in 300 steps: 1.3558213710784912 0.6447221040725708, 0.7110992670059204
Loss in 400 steps: 1.360868215560913 0.6510523557662964, 0.7098158597946167
Loss in 500 steps: 1.3576359748840332 0.6464436054229736, 0.71119225025177
Loss in 600 steps: 1.3568804264068604 0.6460107564926147, 0.7108696699142456
Loss in 700 steps: 1.3535677194595337 0.6411339640617371, 0.7124338746070862
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=20, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50wd', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0.0001, years=30)
Loss in 0 steps: 1.3614445924758911 0.650973379611969, 0.7104712128639221
Loss in 100 steps: 1.3563966751098633 0.644955575466156, 0.711441159248352
Loss in 200 steps: 1.3571324348449707 0.6458993554115295, 0.7112331986427307
Loss in 300 steps: 1.3571258783340454 0.6461610794067383, 0.7109647393226624
Loss in 400 steps: 1.3608790636062622 0.6510571241378784, 0.709821879863739
Loss in 500 steps: 1.3566439151763916 0.6454276442527771, 0.711216390132904
Loss in 600 steps: 1.3533844947814941 0.6417812705039978, 0.7116032838821411
Loss in 700 steps: 1.355144739151001 0.6430274844169617, 0.7121171951293945
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=20, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50wd', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0.0001, years=30)
Loss in 0 steps: 1.3606173992156982 0.6499127745628357, 0.7107046246528625
Loss in 100 steps: 1.3576356172561646 0.6465761661529541, 0.7110594511032104
Loss in 200 steps: 1.3578656911849976 0.6469558477401733, 0.710909903049469
Loss in 300 steps: 1.355970025062561 0.6449540257453918, 0.7110159397125244
Loss in 400 steps: 1.3616257905960083 0.6520245671272278, 0.7096012830734253
Loss in 500 steps: 1.3580663204193115 0.647085964679718, 0.7109801173210144
Loss in 600 steps: 1.3544422388076782 0.6431402564048767, 0.7113019824028015
Loss in 700 steps: 1.3546472787857056 0.6426095962524414, 0.7120376825332642
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=20, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50wd', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0.0001, years=30)
Loss in 0 steps: 1.3604183197021484 0.6498891115188599, 0.7105292677879333
Loss in 100 steps: 1.356554627418518 0.6452900767326355, 0.7112644910812378
Loss in 200 steps: 1.3567135334014893 0.6455004811286926, 0.7112131714820862
Loss in 300 steps: 1.3572267293930054 0.6465513706207275, 0.7106754779815674
Loss in 400 steps: 1.3615936040878296 0.6518144011497498, 0.7097791433334351
Loss in 500 steps: 1.3580563068389893 0.6471357345581055, 0.710920512676239
Loss in 600 steps: 1.3548126220703125 0.643528401851654, 0.7112842798233032
Loss in 700 steps: 1.355889081954956 0.6440380811691284, 0.7118509411811829
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=20, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50wd', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0.0001, years=30)
Loss in 0 steps: 1.3611761331558228 0.6507809162139893, 0.710395097732544
Loss in 100 steps: 1.3571088314056396 0.6460580825805664, 0.7110506892204285
Loss in 200 steps: 1.356203317642212 0.644990861415863, 0.7112123966217041
Loss in 300 steps: 1.3564263582229614 0.6454925537109375, 0.7109337449073792
Loss in 400 steps: 1.360988974571228 0.6512693762779236, 0.7097195386886597
Loss in 500 steps: 1.3575316667556763 0.6466355919837952, 0.7108961939811707
Loss in 600 steps: 1.3573017120361328 0.6465235948562622, 0.7107780575752258
Loss in 700 steps: 1.356200933456421 0.6447131633758545, 0.7114876508712769
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=20, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50wd', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0.0001, years=30)
Loss in 0 steps: 1.3610363006591797 0.65088951587677, 0.710146963596344
Loss in 100 steps: 1.3575515747070312 0.646744966506958, 0.7108065485954285
Loss in 200 steps: 1.3575270175933838 0.6466583013534546, 0.7108687162399292
Loss in 300 steps: 1.357779622077942 0.6473802328109741, 0.710399329662323
Loss in 400 steps: 1.3622846603393555 0.652961790561676, 0.7093230485916138
Loss in 500 steps: 1.358973503112793 0.6484631896018982, 0.71051025390625
Loss in 600 steps: 1.356189250946045 0.645216166973114, 0.7109731435775757
Loss in 700 steps: 1.3561336994171143 0.6448225975036621, 0.7113111615180969
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=20, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50wd', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0.0001, years=30)
Loss in 0 steps: 1.3606390953063965 0.6505585312843323, 0.7100805640220642
Loss in 100 steps: 1.3575830459594727 0.646718442440033, 0.7108645439147949
Loss in 200 steps: 1.3565549850463867 0.6456100940704346, 0.7109449505805969
Loss in 300 steps: 1.3572485446929932 0.6466752290725708, 0.7105733156204224
Loss in 400 steps: 1.3620644807815552 0.652561366558075, 0.7095030546188354
Loss in 500 steps: 1.3582812547683716 0.6477078199386597, 0.7105734348297119
Loss in 600 steps: 1.3565800189971924 0.6456695199012756, 0.7109105587005615
Loss in 700 steps: 1.3565016984939575 0.6453971862792969, 0.7111045122146606
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=20, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50wd', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0.0001, years=30)
Loss in 0 steps: 1.362119436264038 0.652362585067749, 0.7097567915916443
Loss in 100 steps: 1.3575093746185303 0.6466665863990784, 0.7108427286148071
Loss in 200 steps: 1.3565460443496704 0.645562469959259, 0.7109835147857666
Loss in 300 steps: 1.3572663068771362 0.6464319229125977, 0.7108343839645386
Loss in 400 steps: 1.3609325885772705 0.6509732007980347, 0.7099593281745911
Loss in 500 steps: 1.3588354587554932 0.6483679413795471, 0.7104676365852356
Loss in 600 steps: 1.356195330619812 0.6451143622398376, 0.7110810279846191
Loss in 700 steps: 1.3585721254348755 0.6477870941162109, 0.7107850909233093
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=20, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50wd', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0.0001, years=30)
Loss in 0 steps: 1.3618953227996826 0.6522612571716309, 0.7096340656280518
Loss in 100 steps: 1.357979416847229 0.6471747756004333, 0.7108045220375061
Loss in 200 steps: 1.3560307025909424 0.6447762846946716, 0.7112544178962708
Loss in 300 steps: 1.3580230474472046 0.6475642919540405, 0.7104586958885193
Loss in 400 steps: 1.362713098526001 0.6532096862792969, 0.7095032334327698
Loss in 500 steps: 1.3595046997070312 0.6492125988006592, 0.7102920413017273
Loss in 600 steps: 1.3568097352981567 0.6459499001502991, 0.7108597755432129
Loss in 700 steps: 1.3579707145690918 0.6470344066619873, 0.7109362483024597
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=20, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50wd', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0.0001, years=30)
Loss in 0 steps: 1.3591612577438354 0.648916482925415, 0.7102447152137756
Loss in 100 steps: 1.3576480150222778 0.6468051075935364, 0.7108427882194519
Loss in 200 steps: 1.3571540117263794 0.646223247051239, 0.7109307646751404
Loss in 300 steps: 1.3585116863250732 0.6481576561927795, 0.7103539705276489
Loss in 400 steps: 1.3624708652496338 0.6530319452285767, 0.7094389200210571
Loss in 500 steps: 1.358083724975586 0.6476189494132996, 0.7104647159576416
Loss in 600 steps: 1.3558460474014282 0.6447337865829468, 0.7111122608184814
Loss in 700 steps: 1.3575178384780884 0.6465686559677124, 0.710949182510376
