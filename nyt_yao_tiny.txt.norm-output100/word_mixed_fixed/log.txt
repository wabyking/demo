Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.3862944841384888 0.6931472420692444, 0.6931472420692444
Loss in 100 steps: 1.2510244846343994 0.5651077628135681, 0.6859166622161865
Loss in 200 steps: 1.2016897201538086 0.5675259828567505, 0.6341636776924133
Loss in 300 steps: 1.1418792009353638 0.5425987839698792, 0.5992802977561951
Loss in 400 steps: 1.1408237218856812 0.5556032061576843, 0.5852205753326416
Loss in 500 steps: 1.1595367193222046 0.5691341161727905, 0.5904027819633484
Loss in 600 steps: 1.137744426727295 0.5519631505012512, 0.5857812762260437
Loss in 700 steps: 1.1217304468154907 0.5468429923057556, 0.5748875141143799
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.1440783739089966 0.5677309632301331, 0.5763473510742188
Loss in 100 steps: 1.0985065698623657 0.5272806286811829, 0.5712260007858276
Loss in 200 steps: 1.122326374053955 0.5511210560798645, 0.5712053775787354
Loss in 300 steps: 1.0830987691879272 0.5213876962661743, 0.5617110729217529
Loss in 400 steps: 1.0875024795532227 0.5322047472000122, 0.5552977323532104
Loss in 500 steps: 1.114937663078308 0.5511510968208313, 0.5637865662574768
Loss in 600 steps: 1.0890552997589111 0.5254771709442139, 0.5635781288146973
Loss in 700 steps: 1.094762921333313 0.5380305647850037, 0.5567324757575989
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0857348442077637 0.5344303250312805, 0.5513045191764832
Loss in 100 steps: 1.0637266635894775 0.5134059190750122, 0.5503206849098206
Loss in 200 steps: 1.0902354717254639 0.5308634638786316, 0.5593720078468323
Loss in 300 steps: 1.0620362758636475 0.5148890018463135, 0.5471473336219788
Loss in 400 steps: 1.0593904256820679 0.5190806984901428, 0.5403096675872803
Loss in 500 steps: 1.0935816764831543 0.5413134098052979, 0.5522683262825012
Loss in 600 steps: 1.070062518119812 0.5159783959388733, 0.554084062576294
Loss in 700 steps: 1.0692411661148071 0.5229407548904419, 0.54630047082901
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0582776069641113 0.5241733193397522, 0.5341042876243591
Loss in 100 steps: 1.0510917901992798 0.5108593106269836, 0.5402324795722961
Loss in 200 steps: 1.0743472576141357 0.5221635103225708, 0.5521837472915649
Loss in 300 steps: 1.0406270027160645 0.5010859966278076, 0.5395409464836121
Loss in 400 steps: 1.052065134048462 0.5190112590789795, 0.5330539345741272
Loss in 500 steps: 1.072511911392212 0.5288989543914795, 0.5436128973960876
Loss in 600 steps: 1.0502623319625854 0.504329264163971, 0.5459332466125488
Loss in 700 steps: 1.0598316192626953 0.519850492477417, 0.5399811863899231
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.051106333732605 0.5235542058944702, 0.5275520086288452
Loss in 100 steps: 1.0329095125198364 0.5011178255081177, 0.5317916870117188
Loss in 200 steps: 1.0518354177474976 0.5063524842262268, 0.5454828143119812
Loss in 300 steps: 1.0281622409820557 0.4961433410644531, 0.5320188999176025
Loss in 400 steps: 1.0339614152908325 0.5070456862449646, 0.5269158482551575
Loss in 500 steps: 1.052411675453186 0.515609860420227, 0.5368016958236694
Loss in 600 steps: 1.0416216850280762 0.49928411841392517, 0.5423375368118286
Loss in 700 steps: 1.04001784324646 0.505007803440094, 0.535010039806366
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0347062349319458 0.515068531036377, 0.5196377635002136
Loss in 100 steps: 1.026811122894287 0.4990686774253845, 0.5277423858642578
Loss in 200 steps: 1.0514209270477295 0.5100813508033752, 0.541339635848999
Loss in 300 steps: 1.0244518518447876 0.49539148807525635, 0.529060423374176
Loss in 400 steps: 1.026279330253601 0.5031234622001648, 0.523155927658081
Loss in 500 steps: 1.0387322902679443 0.5050776600837708, 0.5336546897888184
Loss in 600 steps: 1.0300430059432983 0.4921630024909973, 0.5378799438476562
Loss in 700 steps: 1.038146734237671 0.506912350654602, 0.5312344431877136
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0300452709197998 0.5122677683830261, 0.5177775025367737
Loss in 100 steps: 1.0129032135009766 0.4907400608062744, 0.5221631526947021
Loss in 200 steps: 1.0389704704284668 0.5000240802764893, 0.5389464497566223
Loss in 300 steps: 1.0218396186828613 0.49560534954071045, 0.5262342095375061
Loss in 400 steps: 1.0235817432403564 0.5019846558570862, 0.5215970873832703
Loss in 500 steps: 1.040989637374878 0.5099528431892395, 0.5310367941856384
Loss in 600 steps: 1.03593111038208 0.5010013580322266, 0.5349296927452087
Loss in 700 steps: 1.0215458869934082 0.4938066601753235, 0.5277392268180847
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0127413272857666 0.4994686245918274, 0.5132727026939392
Loss in 100 steps: 1.0065431594848633 0.48649582266807556, 0.5200473666191101
Loss in 200 steps: 1.035207748413086 0.5000786781311035, 0.5351291298866272
Loss in 300 steps: 1.0062967538833618 0.4842170178890228, 0.5220797061920166
Loss in 400 steps: 1.0107749700546265 0.49417996406555176, 0.5165950059890747
Loss in 500 steps: 1.0533733367919922 0.5215054154396057, 0.5318678021430969
Loss in 600 steps: 1.0093408823013306 0.4785591661930084, 0.5307815670967102
Loss in 700 steps: 1.0281171798706055 0.5008394122123718, 0.5272778868675232
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0209012031555176 0.5082708597183228, 0.5126304030418396
Loss in 100 steps: 1.004606008529663 0.4876804053783417, 0.5169256329536438
Loss in 200 steps: 1.027739405632019 0.4967820346355438, 0.5309573411941528
Loss in 300 steps: 1.013484001159668 0.4923042953014374, 0.5211797952651978
Loss in 400 steps: 1.0090312957763672 0.493200421333313, 0.5158309936523438
Loss in 500 steps: 1.051217794418335 0.5207251906394958, 0.5304925441741943
Loss in 600 steps: 1.016188621520996 0.4853825271129608, 0.5308060646057129
Loss in 700 steps: 1.0279109477996826 0.5029250979423523, 0.5249860286712646
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0021302700042725 0.49202507734298706, 0.5101051926612854
Loss in 100 steps: 1.0033429861068726 0.4863496720790863, 0.5169932842254639
Loss in 200 steps: 1.019249677658081 0.4915873110294342, 0.5276622772216797
Loss in 300 steps: 1.0119168758392334 0.49209651350975037, 0.5198203921318054
Loss in 400 steps: 1.0092438459396362 0.4930492639541626, 0.5161945223808289
Loss in 500 steps: 1.0430716276168823 0.5154417157173157, 0.5276298522949219
Loss in 600 steps: 1.0287774801254272 0.4996478259563446, 0.5291296243667603
Loss in 700 steps: 1.0270566940307617 0.5029933452606201, 0.5240632891654968
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.002471923828125 0.49286794662475586, 0.5096039175987244
Loss in 100 steps: 0.9950507879257202 0.4806098937988281, 0.5144408941268921
Loss in 200 steps: 1.0235456228256226 0.4971749782562256, 0.5263707637786865
Loss in 300 steps: 1.0019820928573608 0.4836994409561157, 0.5182827115058899
Loss in 400 steps: 1.0053845643997192 0.4920831322669983, 0.5133013725280762
Loss in 500 steps: 1.0433878898620605 0.5173892378807068, 0.5259987115859985
Loss in 600 steps: 1.0166525840759277 0.4877230226993561, 0.5289295315742493
Loss in 700 steps: 1.0154894590377808 0.49301978945732117, 0.522469699382782
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9952684640884399 0.48837754130363464, 0.5068908929824829
Loss in 100 steps: 0.9924876093864441 0.4794783592224121, 0.513009250164032
Loss in 200 steps: 1.0173524618148804 0.49420252442359924, 0.5231499671936035
Loss in 300 steps: 0.9992482662200928 0.4836398661136627, 0.5156084895133972
Loss in 400 steps: 1.0023975372314453 0.48980844020843506, 0.5125890374183655
Loss in 500 steps: 1.0354572534561157 0.5104665756225586, 0.5249907374382019
Loss in 600 steps: 1.0087213516235352 0.48208874464035034, 0.5266326069831848
Loss in 700 steps: 1.0103960037231445 0.4898175895214081, 0.5205784440040588
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0112485885620117 0.500760018825531, 0.5104885101318359
Loss in 100 steps: 1.003332495689392 0.4892437160015106, 0.5140887498855591
Loss in 200 steps: 1.0302984714508057 0.5073843002319336, 0.5229141712188721
Loss in 300 steps: 0.9903255105018616 0.47628575563430786, 0.5140396952629089
Loss in 400 steps: 0.9967597126960754 0.4861278831958771, 0.5106318593025208
Loss in 500 steps: 1.0403823852539062 0.5158929228782654, 0.5244894027709961
Loss in 600 steps: 1.0008093118667603 0.4769105613231659, 0.5238987803459167
Loss in 700 steps: 1.0173166990280151 0.4968803822994232, 0.5204362869262695
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0003358125686646 0.49280205368995667, 0.507533848285675
Loss in 100 steps: 0.9891549348831177 0.4770016670227051, 0.5121532082557678
Loss in 200 steps: 1.0196598768234253 0.49915623664855957, 0.5205036401748657
Loss in 300 steps: 0.9886738657951355 0.47548022866249084, 0.5131936073303223
Loss in 400 steps: 0.9914721250534058 0.4816071391105652, 0.5098649859428406
Loss in 500 steps: 1.0274779796600342 0.5039860606193542, 0.5234920382499695
Loss in 600 steps: 1.0031015872955322 0.47938281297683716, 0.5237187743186951
Loss in 700 steps: 1.0078333616256714 0.4875340461730957, 0.5202993154525757
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0043370723724365 0.49804946780204773, 0.5062875151634216
Loss in 100 steps: 0.9850252270698547 0.4745528995990753, 0.5104723572731018
Loss in 200 steps: 1.0140982866287231 0.4959428906440735, 0.5181555151939392
Loss in 300 steps: 0.9873028993606567 0.4742029011249542, 0.5131000876426697
Loss in 400 steps: 0.9940078258514404 0.48539310693740845, 0.508614718914032
Loss in 500 steps: 1.0253260135650635 0.5031818747520447, 0.5221442580223083
Loss in 600 steps: 1.0115575790405273 0.48745912313461304, 0.5240985155105591
Loss in 700 steps: 1.0064756870269775 0.48727452754974365, 0.5192013382911682
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9964940547943115 0.4896353483200073, 0.5068586468696594
Loss in 100 steps: 0.9933985471725464 0.48325014114379883, 0.5101484060287476
Loss in 200 steps: 1.0060635805130005 0.4901893436908722, 0.5158742666244507
Loss in 300 steps: 0.9889053702354431 0.4785587787628174, 0.5103466510772705
Loss in 400 steps: 0.9896974563598633 0.48175281286239624, 0.5079445838928223
Loss in 500 steps: 1.0197710990905762 0.5000176429748535, 0.5197533965110779
Loss in 600 steps: 0.9936636686325073 0.4716132879257202, 0.5220503211021423
Loss in 700 steps: 1.0097649097442627 0.49165958166122437, 0.5181052684783936
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9919483661651611 0.4857929050922394, 0.5061554312705994
Loss in 100 steps: 0.9883777499198914 0.4783688485622406, 0.5100089907646179
Loss in 200 steps: 1.0126160383224487 0.49764111638069153, 0.5149749517440796
Loss in 300 steps: 0.9854581356048584 0.4748266339302063, 0.5106315612792969
Loss in 400 steps: 0.9897565245628357 0.48113253712654114, 0.5086240768432617
Loss in 500 steps: 1.031837821006775 0.5110512375831604, 0.5207865834236145
Loss in 600 steps: 0.9968373775482178 0.4761500656604767, 0.5206872820854187
Loss in 700 steps: 1.0044580698013306 0.48710867762565613, 0.5173494815826416
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0011372566223145 0.4955340623855591, 0.5056031942367554
Loss in 100 steps: 0.9873223900794983 0.47753477096557617, 0.5097876787185669
Loss in 200 steps: 1.012291431427002 0.4981174170970917, 0.5141739845275879
Loss in 300 steps: 1.0011212825775146 0.4881458878517151, 0.5129754543304443
Loss in 400 steps: 0.9882317781448364 0.4795166850090027, 0.508715033531189
Loss in 500 steps: 1.0303765535354614 0.511352002620697, 0.5190245509147644
Loss in 600 steps: 0.999995768070221 0.479736864566803, 0.5202588438987732
Loss in 700 steps: 0.9977782964706421 0.48108193278312683, 0.5166963338851929
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9799761772155762 0.4783265292644501, 0.5016496181488037
Loss in 100 steps: 0.9904418587684631 0.4809189438819885, 0.5095228552818298
Loss in 200 steps: 1.0053735971450806 0.4928008019924164, 0.5125727653503418
Loss in 300 steps: 0.9932165145874023 0.48283451795578003, 0.5103819966316223
Loss in 400 steps: 0.9833099246025085 0.47769325971603394, 0.5056166648864746
Loss in 500 steps: 1.0292284488677979 0.5101727247238159, 0.5190557837486267
Loss in 600 steps: 1.0036747455596924 0.4830148220062256, 0.520659863948822
Loss in 700 steps: 1.0052835941314697 0.4889181852340698, 0.5163654088973999
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9913308620452881 0.48477378487586975, 0.506557047367096
Loss in 100 steps: 0.9908726811408997 0.48100730776786804, 0.509865403175354
Loss in 200 steps: 1.0014921426773071 0.4903997480869293, 0.5110923051834106
Loss in 300 steps: 0.982175350189209 0.4736083447933197, 0.5085670351982117
Loss in 400 steps: 0.9788283109664917 0.4757947325706482, 0.5030335783958435
Loss in 500 steps: 1.0207475423812866 0.5044497847557068, 0.5162976980209351
Loss in 600 steps: 0.9970101118087769 0.4778856337070465, 0.5191245079040527
Loss in 700 steps: 0.9988102316856384 0.4825115203857422, 0.516298770904541
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.980280339717865 0.47747570276260376, 0.5028045773506165
Loss in 100 steps: 0.9776155352592468 0.469443142414093, 0.5081723928451538
Loss in 200 steps: 1.0166630744934082 0.5039236545562744, 0.5127395391464233
Loss in 300 steps: 0.9960121512413025 0.4871760904788971, 0.508836030960083
Loss in 400 steps: 0.9837674498558044 0.4783090054988861, 0.505458414554596
Loss in 500 steps: 1.0267415046691895 0.5086174011230469, 0.5181239247322083
Loss in 600 steps: 0.9907443523406982 0.47263991832733154, 0.5181044340133667
Loss in 700 steps: 0.9923528432846069 0.4762240946292877, 0.5161287784576416
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9851219058036804 0.4837987422943115, 0.5013231635093689
Loss in 100 steps: 0.9811039566993713 0.47370561957359314, 0.5073983073234558
Loss in 200 steps: 1.0033820867538452 0.49287354946136475, 0.5105085968971252
Loss in 300 steps: 0.9878993630409241 0.47931501269340515, 0.5085843205451965
Loss in 400 steps: 0.985611617565155 0.4796963632106781, 0.5059152245521545
Loss in 500 steps: 1.017407774925232 0.5017431974411011, 0.5156645774841309
Loss in 600 steps: 1.0005886554718018 0.48170414566993713, 0.518884539604187
Loss in 700 steps: 0.9914230704307556 0.4768424928188324, 0.5145806074142456
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9904475212097168 0.48724254965782166, 0.5032050609588623
Loss in 100 steps: 0.9819570183753967 0.47392719984054565, 0.5080298185348511
Loss in 200 steps: 1.0020118951797485 0.49301737546920776, 0.5089945197105408
Loss in 300 steps: 0.9896998405456543 0.4814913272857666, 0.5082085132598877
Loss in 400 steps: 0.9769405126571655 0.47209280729293823, 0.5048477649688721
Loss in 500 steps: 1.0308295488357544 0.514058530330658, 0.5167709589004517
Loss in 600 steps: 0.994464635848999 0.4763835072517395, 0.5180811882019043
Loss in 700 steps: 1.0001972913742065 0.4857185482978821, 0.514478862285614
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9735425114631653 0.47239062190055847, 0.5011518597602844
Loss in 100 steps: 0.9799894094467163 0.47274482250213623, 0.5072445869445801
Loss in 200 steps: 1.0050710439682007 0.49486076831817627, 0.5102102756500244
Loss in 300 steps: 0.9894368648529053 0.48113134503364563, 0.5083054900169373
Loss in 400 steps: 0.9768008589744568 0.4747318625450134, 0.5020689964294434
Loss in 500 steps: 1.0232973098754883 0.506730854511261, 0.5165665745735168
Loss in 600 steps: 0.9876037836074829 0.4709230959415436, 0.5166806578636169
Loss in 700 steps: 0.9908773303031921 0.47684380412101746, 0.5140334963798523
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9790198802947998 0.4794408977031708, 0.49957898259162903
Loss in 100 steps: 0.9807692766189575 0.47366103529930115, 0.507108211517334
Loss in 200 steps: 1.0058972835540771 0.49629858136177063, 0.5095987319946289
Loss in 300 steps: 0.9905406832695007 0.48282381892204285, 0.507716953754425
Loss in 400 steps: 0.9735086560249329 0.4689781665802002, 0.5045304298400879
Loss in 500 steps: 1.009737491607666 0.4965176582336426, 0.5132198929786682
Loss in 600 steps: 0.9896134734153748 0.47303298115730286, 0.5165804028511047
Loss in 700 steps: 0.9832963347434998 0.47064319252967834, 0.5126531720161438
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9775557518005371 0.47602078318595886, 0.5015349984169006
Loss in 100 steps: 0.9811421632766724 0.4730229675769806, 0.5081192255020142
Loss in 200 steps: 1.0020407438278198 0.49420440196990967, 0.5078363418579102
Loss in 300 steps: 0.9800006747245789 0.4746977984905243, 0.5053028464317322
Loss in 400 steps: 0.9877859354019165 0.48174402117729187, 0.506041944026947
Loss in 500 steps: 0.9894793033599854 0.47981148958206177, 0.5096678137779236
Loss in 600 steps: 0.9860776662826538 0.47014281153678894, 0.5159348845481873
Loss in 700 steps: 0.9996662139892578 0.48856595158576965, 0.5111003518104553
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9840997457504272 0.48087581992149353, 0.5032238960266113
Loss in 100 steps: 0.9833275675773621 0.47595998644828796, 0.5073674917221069
Loss in 200 steps: 0.9983636140823364 0.4909219741821289, 0.5074416399002075
Loss in 300 steps: 0.9855589270591736 0.47833383083343506, 0.5072250366210938
Loss in 400 steps: 0.9782512187957764 0.4744259715080261, 0.5038252472877502
Loss in 500 steps: 1.0037354230880737 0.49093639850616455, 0.5127991437911987
Loss in 600 steps: 0.9886066317558289 0.47165921330451965, 0.5169473886489868
Loss in 700 steps: 0.9813945889472961 0.4696153700351715, 0.5117791891098022
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9793259501457214 0.4784296154975891, 0.5008963346481323
Loss in 100 steps: 0.9826009273529053 0.47488173842430115, 0.5077191591262817
Loss in 200 steps: 0.9988900423049927 0.49103933572769165, 0.507850706577301
Loss in 300 steps: 0.9924325346946716 0.48459216952323914, 0.5078403949737549
Loss in 400 steps: 0.9767088890075684 0.47446754574775696, 0.5022414326667786
Loss in 500 steps: 1.0047473907470703 0.4948490560054779, 0.5098984837532043
Loss in 600 steps: 0.9941324591636658 0.47894734144210815, 0.5151851177215576
Loss in 700 steps: 0.9886794686317444 0.4776464104652405, 0.5110331773757935
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9784184694290161 0.47761228680610657, 0.5008062124252319
Loss in 100 steps: 0.973808765411377 0.46719568967819214, 0.50661301612854
Loss in 200 steps: 1.0022876262664795 0.49589478969573975, 0.5063928961753845
Loss in 300 steps: 0.9889591932296753 0.4836635887622833, 0.5052956342697144
Loss in 400 steps: 0.978715717792511 0.4736259877681732, 0.5050897002220154
Loss in 500 steps: 0.9971218705177307 0.4861060678958893, 0.511015772819519
Loss in 600 steps: 0.9830774664878845 0.46718117594718933, 0.5158962607383728
Loss in 700 steps: 0.985515832901001 0.4742415249347687, 0.5112743377685547
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9770050644874573 0.4770936965942383, 0.4999113380908966
Loss in 100 steps: 0.9800289869308472 0.4752381145954132, 0.5047909021377563
Loss in 200 steps: 0.9995922446250916 0.493533730506897, 0.5060585737228394
Loss in 300 steps: 0.9846899509429932 0.48000696301460266, 0.5046830177307129
Loss in 400 steps: 0.9733470678329468 0.46975263953208923, 0.5035943388938904
Loss in 500 steps: 1.0025615692138672 0.4892042279243469, 0.5133573412895203
Loss in 600 steps: 0.9969651699066162 0.4820318818092346, 0.5149332880973816
Loss in 700 steps: 0.9948193430900574 0.4836052358150482, 0.5112141966819763
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9672949314117432 0.4674731194972992, 0.49982184171676636
Loss in 100 steps: 0.9852907061576843 0.4788031578063965, 0.5064875483512878
Loss in 200 steps: 0.9984517693519592 0.49096953868865967, 0.5074821710586548
Loss in 300 steps: 0.9922897815704346 0.48725202679634094, 0.505037784576416
Loss in 400 steps: 0.9712677001953125 0.4676799774169922, 0.5035877227783203
Loss in 500 steps: 0.9986327290534973 0.4879191219806671, 0.5107135772705078
Loss in 600 steps: 0.9779296517372131 0.4646138548851013, 0.5133157968521118
Loss in 700 steps: 0.9937146306037903 0.48021480441093445, 0.5134997963905334
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9750893115997314 0.4756481945514679, 0.49944108724594116
Loss in 100 steps: 0.979817807674408 0.47340643405914307, 0.5064114332199097
Loss in 200 steps: 0.9911699295043945 0.48489704728126526, 0.5062729120254517
Loss in 300 steps: 0.9704281687736511 0.466406911611557, 0.5040212273597717
Loss in 400 steps: 0.9721918106079102 0.4707089364528656, 0.5014829039573669
Loss in 500 steps: 1.0000510215759277 0.48788246512413025, 0.5121686458587646
Loss in 600 steps: 0.9817699790000916 0.46678128838539124, 0.5149887204170227
Loss in 700 steps: 0.9896728992462158 0.4778274893760681, 0.5118454694747925
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9864153861999512 0.48680391907691956, 0.499611496925354
Loss in 100 steps: 0.985670268535614 0.47985777258872986, 0.505812406539917
Loss in 200 steps: 0.9993540644645691 0.492807537317276, 0.5065465569496155
Loss in 300 steps: 0.9838771820068359 0.47802630066871643, 0.5058508515357971
Loss in 400 steps: 0.981151819229126 0.47714412212371826, 0.5040076375007629
Loss in 500 steps: 1.0104398727416992 0.49916085600852966, 0.5112790465354919
Loss in 600 steps: 0.99192214012146 0.477380633354187, 0.5145414471626282
Loss in 700 steps: 0.9887728095054626 0.4779985547065735, 0.5107742547988892
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9761765003204346 0.4771972596645355, 0.49897927045822144
Loss in 100 steps: 0.9797892570495605 0.47379374504089355, 0.505995512008667
Loss in 200 steps: 1.0031358003616333 0.49681520462036133, 0.5063206553459167
Loss in 300 steps: 0.9895890951156616 0.4842677712440491, 0.5053213238716125
Loss in 400 steps: 0.9729543924331665 0.4716498553752899, 0.501304566860199
Loss in 500 steps: 1.0046675205230713 0.4936750531196594, 0.5109924077987671
Loss in 600 steps: 0.9885554313659668 0.47343453764915466, 0.5151209235191345
Loss in 700 steps: 0.9884141683578491 0.4774109721183777, 0.5110031366348267
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9866776466369629 0.48464635014533997, 0.5020312666893005
Loss in 100 steps: 0.9730002284049988 0.4680992364883423, 0.5049009919166565
Loss in 200 steps: 0.998881459236145 0.4938054084777832, 0.5050760507583618
Loss in 300 steps: 0.9863561391830444 0.4796082079410553, 0.5067479014396667
Loss in 400 steps: 0.9771891832351685 0.47576993703842163, 0.5014192461967468
Loss in 500 steps: 0.996663510799408 0.4880412220954895, 0.5086222290992737
Loss in 600 steps: 0.9766925573348999 0.462895005941391, 0.5137975215911865
Loss in 700 steps: 0.9961017966270447 0.48471176624298096, 0.5113900303840637
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9781129956245422 0.4771365225315094, 0.5009765028953552
Loss in 100 steps: 0.9733656644821167 0.46792301535606384, 0.50544273853302
Loss in 200 steps: 1.0003585815429688 0.494301438331604, 0.50605708360672
Loss in 300 steps: 0.9807517528533936 0.47749242186546326, 0.5032593607902527
Loss in 400 steps: 0.9850583672523499 0.48247960209846497, 0.5025787353515625
Loss in 500 steps: 0.9871349334716797 0.47808125615119934, 0.509053647518158
Loss in 600 steps: 0.9886622428894043 0.4737407863140106, 0.5149213671684265
Loss in 700 steps: 0.9836655855178833 0.4739772379398346, 0.5096883177757263
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9719040393829346 0.4714825451374054, 0.5004215240478516
Loss in 100 steps: 0.9716008305549622 0.46605393290519714, 0.5055468678474426
Loss in 200 steps: 0.9900367259979248 0.48472824692726135, 0.5053085088729858
Loss in 300 steps: 0.9829069972038269 0.478680282831192, 0.5042267441749573
Loss in 400 steps: 0.9715538024902344 0.4721657931804657, 0.49938803911209106
Loss in 500 steps: 0.9972447752952576 0.4888668954372406, 0.5083779692649841
Loss in 600 steps: 0.9887545704841614 0.4738744795322418, 0.5148800611495972
Loss in 700 steps: 0.9935476779937744 0.48121437430381775, 0.5123332738876343
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9776013493537903 0.477555513381958, 0.5000458359718323
Loss in 100 steps: 0.9691593647003174 0.46391820907592773, 0.5052411556243896
Loss in 200 steps: 0.9948850274085999 0.48878470063209534, 0.5061002373695374
Loss in 300 steps: 0.9858892560005188 0.4810114800930023, 0.5048776865005493
Loss in 400 steps: 0.9735365509986877 0.47232508659362793, 0.5012114644050598
Loss in 500 steps: 0.9826664328575134 0.47525230050086975, 0.5074141025543213
Loss in 600 steps: 0.9907724857330322 0.47641146183013916, 0.5143610835075378
Loss in 700 steps: 0.9900316596031189 0.4779845178127289, 0.5120471715927124
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9708962440490723 0.4715288281440735, 0.499367356300354
Loss in 100 steps: 0.9771816730499268 0.47176694869995117, 0.5054147243499756
Loss in 200 steps: 0.9872624278068542 0.48278534412384033, 0.5044770836830139
Loss in 300 steps: 0.9774256348609924 0.47512930631637573, 0.5022962689399719
Loss in 400 steps: 0.9808289408683777 0.4796431362628937, 0.5011857748031616
Loss in 500 steps: 0.9877972602844238 0.48348817229270935, 0.5043091177940369
Loss in 600 steps: 0.9712932705879211 0.45915138721466064, 0.5121418237686157
Loss in 700 steps: 0.9867912530899048 0.477046936750412, 0.5097443461418152
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9773189425468445 0.47724035382270813, 0.5000786185264587
Loss in 100 steps: 0.9736377596855164 0.4686734974384308, 0.504964292049408
Loss in 200 steps: 1.0017187595367432 0.4968720078468323, 0.5048466920852661
Loss in 300 steps: 0.9797667860984802 0.47689124941825867, 0.5028756260871887
Loss in 400 steps: 0.9800066351890564 0.4784631133079529, 0.5015435218811035
Loss in 500 steps: 0.9780303239822388 0.47381797432899475, 0.5042124390602112
Loss in 600 steps: 0.9749478101730347 0.4613056182861328, 0.5136422514915466
Loss in 700 steps: 1.0026443004608154 0.49207252264022827, 0.5105717182159424
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9781829714775085 0.47900572419166565, 0.4991772472858429
Loss in 100 steps: 0.9695796370506287 0.4668827950954437, 0.502696692943573
Loss in 200 steps: 0.9887122511863708 0.4844922721385956, 0.5042199492454529
Loss in 300 steps: 0.9763342142105103 0.47373461723327637, 0.5025995373725891
Loss in 400 steps: 0.9769788980484009 0.47614866495132446, 0.5008302330970764
Loss in 500 steps: 0.9997895359992981 0.49031269550323486, 0.5094768404960632
Loss in 600 steps: 0.9856796860694885 0.47255241870880127, 0.5131272673606873
Loss in 700 steps: 0.9847249388694763 0.4746251404285431, 0.5100996494293213
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9688221216201782 0.4704040288925171, 0.4984181225299835
Loss in 100 steps: 0.9698768258094788 0.46426641941070557, 0.505610466003418
Loss in 200 steps: 0.9891334176063538 0.48527786135673523, 0.5038555264472961
Loss in 300 steps: 0.9812018871307373 0.4774492383003235, 0.5037526488304138
Loss in 400 steps: 0.9704786539077759 0.4706869423389435, 0.4997917115688324
Loss in 500 steps: 1.0043308734893799 0.49510806798934937, 0.5092228055000305
Loss in 600 steps: 0.9858115315437317 0.47324225306510925, 0.5125693082809448
Loss in 700 steps: 0.9876732230186462 0.47732964158058167, 0.510343611240387
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.973377525806427 0.4749554991722107, 0.4984219968318939
Loss in 100 steps: 0.9758062958717346 0.4716314673423767, 0.5041748285293579
Loss in 200 steps: 1.0003939867019653 0.4955306649208069, 0.5048632621765137
Loss in 300 steps: 0.9774071574211121 0.4726737141609192, 0.5047333836555481
Loss in 400 steps: 0.9767653346061707 0.4759010970592499, 0.5008641481399536
Loss in 500 steps: 0.987216591835022 0.48001858592033386, 0.5071979761123657
Loss in 600 steps: 0.9862384796142578 0.47228872776031494, 0.5139498114585876
Loss in 700 steps: 0.9857956767082214 0.4754332900047302, 0.5103622674942017
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9774766564369202 0.4794342815876007, 0.49804237484931946
Loss in 100 steps: 0.975886881351471 0.4715040922164917, 0.504382848739624
Loss in 200 steps: 0.9937368035316467 0.4897807240486145, 0.5039560794830322
Loss in 300 steps: 0.9791222214698792 0.4753744900226593, 0.5037477612495422
Loss in 400 steps: 0.9716601371765137 0.47127678990364075, 0.5003833174705505
Loss in 500 steps: 0.9884413480758667 0.4797879457473755, 0.5086533427238464
Loss in 600 steps: 0.9896236062049866 0.47640690207481384, 0.5132167339324951
Loss in 700 steps: 0.9852806925773621 0.47585609555244446, 0.5094246864318848
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.959616482257843 0.4630378782749176, 0.4965786337852478
Loss in 100 steps: 0.9756618738174438 0.4710211157798767, 0.5046407580375671
Loss in 200 steps: 0.9935023188591003 0.48809829354286194, 0.505403995513916
Loss in 300 steps: 0.979551374912262 0.4756633937358856, 0.5038880705833435
Loss in 400 steps: 0.9722670912742615 0.4722500741481781, 0.5000169277191162
Loss in 500 steps: 0.9943144917488098 0.48643022775650024, 0.5078842043876648
Loss in 600 steps: 0.9747792482376099 0.46230170130729675, 0.5124775171279907
Loss in 700 steps: 0.9808007478713989 0.47076869010925293, 0.510032057762146
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.957452118396759 0.4624231159687042, 0.4950290322303772
Loss in 100 steps: 0.9712169170379639 0.4678932726383209, 0.5033236742019653
Loss in 200 steps: 0.986327588558197 0.4830062985420227, 0.5033211708068848
Loss in 300 steps: 0.9804840087890625 0.47802120447158813, 0.5024628639221191
Loss in 400 steps: 0.9765036106109619 0.476186066865921, 0.5003176331520081
Loss in 500 steps: 0.9797479510307312 0.47317397594451904, 0.5065739750862122
Loss in 600 steps: 0.9784272909164429 0.4674769937992096, 0.5109503269195557
Loss in 700 steps: 0.9913483262062073 0.48144182562828064, 0.5099064707756042
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9761380553245544 0.4773438274860382, 0.4987942576408386
Loss in 100 steps: 0.9680905938148499 0.46468302607536316, 0.5034075975418091
Loss in 200 steps: 0.9783776998519897 0.4769405424594879, 0.5014371275901794
Loss in 300 steps: 0.9659920930862427 0.46428683400154114, 0.5017052292823792
Loss in 400 steps: 0.9667580127716064 0.4680194556713104, 0.49873852729797363
Loss in 500 steps: 0.9734204411506653 0.4679713845252991, 0.505449116230011
Loss in 600 steps: 0.975621223449707 0.46426209807395935, 0.5113590359687805
Loss in 700 steps: 0.9745948314666748 0.4660320281982422, 0.5085628628730774
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9609667062759399 0.46470925211906433, 0.496257483959198
Loss in 100 steps: 0.976822555065155 0.47243282198905945, 0.5043897032737732
Loss in 200 steps: 0.9849681854248047 0.481891006231308, 0.5030770897865295
Loss in 300 steps: 0.9778643846511841 0.4745473563671112, 0.50331711769104
Loss in 400 steps: 0.9696256518363953 0.4699546694755554, 0.49967098236083984
Loss in 500 steps: 0.9873917102813721 0.4822410047054291, 0.5051507353782654
Loss in 600 steps: 0.9960430860519409 0.48179224133491516, 0.5142509341239929
Loss in 700 steps: 0.9945586919784546 0.48494869470596313, 0.5096099972724915
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9604198932647705 0.46617504954338074, 0.49424487352371216
Loss in 100 steps: 0.9707749485969543 0.4669325649738312, 0.5038424134254456
Loss in 200 steps: 0.9930239319801331 0.4886625409126282, 0.5043613314628601
Loss in 300 steps: 0.9716637134552002 0.46956104040145874, 0.5021026730537415
Loss in 400 steps: 0.9720087051391602 0.4727781414985657, 0.4992305636405945
Loss in 500 steps: 0.985136091709137 0.48287495970726013, 0.5022610425949097
Loss in 600 steps: 0.9850945472717285 0.4717995524406433, 0.5132949352264404
Loss in 700 steps: 0.993427574634552 0.48442342877388, 0.5090041160583496
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9673264622688293 0.46869876980781555, 0.4986276924610138
Loss in 100 steps: 0.9649239182472229 0.46126309037208557, 0.5036607980728149
Loss in 200 steps: 0.9850455522537231 0.4827404320240021, 0.5023051500320435
Loss in 300 steps: 0.9837884902954102 0.4794568419456482, 0.504331648349762
Loss in 400 steps: 0.9758896827697754 0.47553107142448425, 0.5003587007522583
Loss in 500 steps: 0.9876303672790527 0.480671763420105, 0.5069586634635925
Loss in 600 steps: 0.9877523183822632 0.47529178857803345, 0.5124605298042297
Loss in 700 steps: 0.986258327960968 0.47652098536491394, 0.5097373127937317
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9627611041069031 0.4663877487182617, 0.4963732957839966
Loss in 100 steps: 0.969228982925415 0.4650651216506958, 0.5041638612747192
Loss in 200 steps: 0.9883700013160706 0.4858319163322449, 0.5025380849838257
Loss in 300 steps: 0.9707911610603333 0.46901360154151917, 0.5017775297164917
Loss in 400 steps: 0.9706233739852905 0.4715609550476074, 0.4990624189376831
Loss in 500 steps: 0.9886981248855591 0.4820423722267151, 0.5066556334495544
Loss in 600 steps: 0.9943398833274841 0.4812784492969513, 0.5130613446235657
Loss in 700 steps: 0.9900268912315369 0.48036453127861023, 0.5096624493598938
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9771620035171509 0.4801870882511139, 0.49697503447532654
Loss in 100 steps: 0.9679570198059082 0.4630099833011627, 0.5049470067024231
Loss in 200 steps: 0.9846597909927368 0.48172926902770996, 0.5029304623603821
Loss in 300 steps: 0.9690431952476501 0.4669886529445648, 0.5020546317100525
Loss in 400 steps: 0.9676893353462219 0.4699983298778534, 0.49769097566604614
Loss in 500 steps: 0.9874375462532043 0.4817257225513458, 0.5057118535041809
Loss in 600 steps: 0.9811038374900818 0.4700416326522827, 0.5110622644424438
Loss in 700 steps: 0.9887100458145142 0.47864270210266113, 0.5100674629211426
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9717639684677124 0.47442036867141724, 0.49734359979629517
Loss in 100 steps: 0.9794101119041443 0.47644585371017456, 0.502964198589325
Loss in 200 steps: 0.9889053106307983 0.4852897524833679, 0.5036156177520752
Loss in 300 steps: 0.9683597087860107 0.4670957624912262, 0.5012638568878174
Loss in 400 steps: 0.9739726781845093 0.4757695496082306, 0.4982031583786011
Loss in 500 steps: 0.9830911755561829 0.4791310131549835, 0.5039602518081665
Loss in 600 steps: 0.9750996232032776 0.46392175555229187, 0.5111778974533081
Loss in 700 steps: 0.9801943898200989 0.47086700797080994, 0.5093272924423218
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9710611701011658 0.475147545337677, 0.4959135353565216
Loss in 100 steps: 0.9719794392585754 0.46898525953292847, 0.5029941201210022
Loss in 200 steps: 0.9945715069770813 0.490949809551239, 0.5036216974258423
Loss in 300 steps: 0.9642038345336914 0.46410131454467773, 0.5001025199890137
Loss in 400 steps: 0.980958878993988 0.4811342656612396, 0.499824583530426
Loss in 500 steps: 0.9763136506080627 0.4751799404621124, 0.5011337399482727
Loss in 600 steps: 0.9816487431526184 0.4702371656894684, 0.5114116072654724
Loss in 700 steps: 0.9926851391792297 0.4822603464126587, 0.510424792766571
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9685806632041931 0.47236335277557373, 0.4962173104286194
Loss in 100 steps: 0.9752748608589172 0.47104620933532715, 0.5042286515235901
Loss in 200 steps: 0.9940254092216492 0.4900810420513153, 0.5039443969726562
Loss in 300 steps: 0.9557384848594666 0.457656592130661, 0.49808186292648315
Loss in 400 steps: 0.9676965475082397 0.4700074791908264, 0.4976890981197357
Loss in 500 steps: 0.9877128601074219 0.4818756878376007, 0.5058372020721436
Loss in 600 steps: 0.9947724938392639 0.482112854719162, 0.5126597285270691
Loss in 700 steps: 0.9944860935211182 0.48368167877197266, 0.5108044147491455
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9781746864318848 0.4798758327960968, 0.4982988238334656
Loss in 100 steps: 0.9706036448478699 0.46654418110847473, 0.5040596127510071
Loss in 200 steps: 0.9992940425872803 0.49526986479759216, 0.5040242075920105
Loss in 300 steps: 0.9634063243865967 0.4634016156196594, 0.5000047087669373
Loss in 400 steps: 0.9725165963172913 0.4738326668739319, 0.498683899641037
Loss in 500 steps: 0.9846187233924866 0.4814886450767517, 0.5031300783157349
Loss in 600 steps: 0.9868308305740356 0.47475263476371765, 0.5120782256126404
Loss in 700 steps: 0.9955243468284607 0.4854346513748169, 0.5100896954536438
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9701846241950989 0.4743637442588806, 0.49582090973854065
Loss in 100 steps: 0.9674258828163147 0.463917076587677, 0.5035088062286377
Loss in 200 steps: 0.9967270493507385 0.49331486225128174, 0.5034122467041016
Loss in 300 steps: 0.9802969098091125 0.4788190722465515, 0.501477837562561
Loss in 400 steps: 0.9709426760673523 0.4704776704311371, 0.5004649758338928
Loss in 500 steps: 0.9874193072319031 0.4826737940311432, 0.5047454833984375
Loss in 600 steps: 0.9824116230010986 0.47065719962120056, 0.5117544531822205
Loss in 700 steps: 0.9775911569595337 0.4682629108428955, 0.5093282461166382
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9723907113075256 0.4754994213581085, 0.4968912899494171
Loss in 100 steps: 0.9723218679428101 0.4681907296180725, 0.5041310787200928
Loss in 200 steps: 0.9876479506492615 0.4852842390537262, 0.5023636817932129
Loss in 300 steps: 0.9597002267837524 0.4602917432785034, 0.4994085133075714
Loss in 400 steps: 0.9671938419342041 0.4682905972003937, 0.49890321493148804
Loss in 500 steps: 0.983441948890686 0.4782378375530243, 0.5052040219306946
Loss in 600 steps: 0.9823415875434875 0.47049540281295776, 0.5118462443351746
Loss in 700 steps: 0.9713940620422363 0.4625190198421478, 0.5088750123977661
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9684191346168518 0.47054576873779297, 0.49787336587905884
Loss in 100 steps: 0.967016875743866 0.4637206196784973, 0.5032962560653687
Loss in 200 steps: 1.0002578496932983 0.4958733320236206, 0.5043846368789673
Loss in 300 steps: 0.974795937538147 0.4744124114513397, 0.5003834962844849
Loss in 400 steps: 0.9701970219612122 0.4705584943294525, 0.49963852763175964
Loss in 500 steps: 0.9816760420799255 0.4780072569847107, 0.5036687850952148
Loss in 600 steps: 0.9851012229919434 0.47341620922088623, 0.5116850137710571
Loss in 700 steps: 0.9908813238143921 0.48218607902526855, 0.5086953639984131
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9728660583496094 0.4751543700695038, 0.4977116584777832
Loss in 100 steps: 0.9659502506256104 0.46301519870758057, 0.5029350519180298
Loss in 200 steps: 0.9877200722694397 0.485006183385849, 0.5027138590812683
Loss in 300 steps: 0.9622763395309448 0.4631800651550293, 0.4990962743759155
Loss in 400 steps: 0.9670333862304688 0.4696371257305145, 0.4973963499069214
Loss in 500 steps: 1.0004279613494873 0.4931570887565613, 0.5072708129882812
Loss in 600 steps: 0.981239378452301 0.4700632393360138, 0.5111761689186096
Loss in 700 steps: 0.9928040504455566 0.4827890396118164, 0.5100149512290955
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9609896540641785 0.46395671367645264, 0.49703285098075867
Loss in 100 steps: 0.9755881428718567 0.4720540940761566, 0.5035339593887329
Loss in 200 steps: 0.9834569096565247 0.4807005524635315, 0.5027563571929932
Loss in 300 steps: 0.9644069075584412 0.4655147194862366, 0.498892217874527
Loss in 400 steps: 0.9787935614585876 0.47836539149284363, 0.5004281401634216
Loss in 500 steps: 0.9817008972167969 0.4790124297142029, 0.5026885271072388
Loss in 600 steps: 0.9846581816673279 0.4731209874153137, 0.5115371942520142
Loss in 700 steps: 0.9876632690429688 0.47852379083633423, 0.5091394782066345
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9642838835716248 0.4695202112197876, 0.49476370215415955
Loss in 100 steps: 0.9664309620857239 0.46379294991493225, 0.502638041973114
Loss in 200 steps: 0.993461549282074 0.49047985672950745, 0.5029816627502441
Loss in 300 steps: 0.9743049740791321 0.4733632802963257, 0.5009416937828064
Loss in 400 steps: 0.9709261059761047 0.47261515259742737, 0.49831098318099976
Loss in 500 steps: 1.0003775358200073 0.4921925961971283, 0.5081849694252014
Loss in 600 steps: 0.9819283485412598 0.4694248139858246, 0.5125035643577576
Loss in 700 steps: 0.994219958782196 0.4846647083759308, 0.5095553398132324
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9669597744941711 0.47133374214172363, 0.4956260323524475
Loss in 100 steps: 0.9709135293960571 0.46686309576034546, 0.5040504336357117
Loss in 200 steps: 0.9893419742584229 0.48599836230278015, 0.5033435225486755
Loss in 300 steps: 0.9731665849685669 0.4713796079158783, 0.5017869472503662
Loss in 400 steps: 0.9817911386489868 0.48105448484420776, 0.500736653804779
Loss in 500 steps: 0.9880141019821167 0.483402818441391, 0.5046113133430481
Loss in 600 steps: 0.9845609068870544 0.47281408309936523, 0.511746883392334
Loss in 700 steps: 0.9902150630950928 0.4800164997577667, 0.5101985335350037
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9576905965805054 0.46392038464546204, 0.49377021193504333
Loss in 100 steps: 0.963638424873352 0.4609834849834442, 0.5026549100875854
Loss in 200 steps: 0.995104193687439 0.49273183941841125, 0.5023723840713501
Loss in 300 steps: 0.9670009016990662 0.4684980809688568, 0.4985028803348541
Loss in 400 steps: 0.9765084981918335 0.4784792363643646, 0.4980292320251465
Loss in 500 steps: 0.9978799819946289 0.4918431043624878, 0.5060368776321411
Loss in 600 steps: 0.9864229559898376 0.4751249849796295, 0.5112980008125305
Loss in 700 steps: 0.9862229824066162 0.4770984351634979, 0.5091245174407959
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9658586978912354 0.47242164611816406, 0.4934369921684265
Loss in 100 steps: 0.9610669016838074 0.4574354290962219, 0.5036314129829407
Loss in 200 steps: 1.0010956525802612 0.49820974469184875, 0.5028858780860901
Loss in 300 steps: 0.9575145244598389 0.45808693766593933, 0.4994276463985443
Loss in 400 steps: 0.9638960361480713 0.4659440517425537, 0.4979519248008728
Loss in 500 steps: 0.9840371608734131 0.48024314641952515, 0.5037940144538879
Loss in 600 steps: 0.983285129070282 0.4708550274372101, 0.5124300718307495
Loss in 700 steps: 0.993767261505127 0.48371589183807373, 0.5100514888763428
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9663862586021423 0.47089990973472595, 0.4954862594604492
Loss in 100 steps: 0.9660998582839966 0.4625107944011688, 0.5035891532897949
Loss in 200 steps: 0.989460289478302 0.4870387315750122, 0.502421498298645
Loss in 300 steps: 0.9793764352798462 0.4775416851043701, 0.5018347501754761
Loss in 400 steps: 0.9680220484733582 0.46997442841529846, 0.4980476498603821
Loss in 500 steps: 0.9833027720451355 0.47827330231666565, 0.5050294399261475
Loss in 600 steps: 0.9818793535232544 0.47122466564178467, 0.510654628276825
Loss in 700 steps: 0.9868038296699524 0.4771259129047394, 0.5096778869628906
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9691150784492493 0.47272512316703796, 0.4963899552822113
Loss in 100 steps: 0.9637050628662109 0.4611241817474365, 0.5025808215141296
Loss in 200 steps: 0.9914634227752686 0.4881298243999481, 0.5033336281776428
Loss in 300 steps: 0.969070553779602 0.4691774845123291, 0.49989306926727295
Loss in 400 steps: 0.96694016456604 0.4699014127254486, 0.49703872203826904
Loss in 500 steps: 0.9969262480735779 0.4928705394268036, 0.5040557384490967
Loss in 600 steps: 0.9757481813430786 0.46515917778015137, 0.5105890035629272
Loss in 700 steps: 0.9869460463523865 0.4784104824066162, 0.508535623550415
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9705314040184021 0.47492820024490356, 0.4956032335758209
Loss in 100 steps: 0.9690501093864441 0.4659585952758789, 0.5030914545059204
Loss in 200 steps: 0.9848080277442932 0.4823363721370697, 0.5024716854095459
Loss in 300 steps: 0.9631364345550537 0.46442943811416626, 0.49870699644088745
Loss in 400 steps: 0.9710351824760437 0.47205111384391785, 0.49898412823677063
Loss in 500 steps: 0.9887305498123169 0.48723673820495605, 0.5014938116073608
Loss in 600 steps: 0.9702509641647339 0.4602055549621582, 0.5100453495979309
Loss in 700 steps: 0.9895243644714355 0.48013970255851746, 0.5093846321105957
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9660952091217041 0.47084498405456543, 0.49525022506713867
Loss in 100 steps: 0.9776338934898376 0.4740726351737976, 0.5035613179206848
Loss in 200 steps: 0.9912770986557007 0.48788923025131226, 0.5033878684043884
Loss in 300 steps: 0.9605091214179993 0.4614892303943634, 0.49901992082595825
Loss in 400 steps: 0.9741004705429077 0.47599488496780396, 0.4981057047843933
Loss in 500 steps: 0.9966496825218201 0.4908226728439331, 0.505827009677887
Loss in 600 steps: 0.9779754281044006 0.4689130485057831, 0.5090624690055847
Loss in 700 steps: 1.0002983808517456 0.4911418557167053, 0.5091565251350403
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9724737405776978 0.4758928418159485, 0.4965808689594269
Loss in 100 steps: 0.9675856232643127 0.46492627263069153, 0.5026593804359436
Loss in 200 steps: 0.9856358766555786 0.48418015241622925, 0.5014558434486389
Loss in 300 steps: 0.9638638496398926 0.4647938907146454, 0.4990699589252472
Loss in 400 steps: 0.9785723686218262 0.4789559245109558, 0.49961647391319275
Loss in 500 steps: 0.9867588877677917 0.4814750552177429, 0.5052838325500488
Loss in 600 steps: 0.984615683555603 0.47552016377449036, 0.5090954899787903
Loss in 700 steps: 0.9995421171188354 0.4914555847644806, 0.5080865025520325
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9833129644393921 0.48667824268341064, 0.49663469195365906
Loss in 100 steps: 0.970666229724884 0.4678860008716583, 0.5027803182601929
Loss in 200 steps: 0.9931997656822205 0.49036553502082825, 0.5028342008590698
Loss in 300 steps: 0.9722301959991455 0.4723098576068878, 0.49992039799690247
Loss in 400 steps: 0.9806668758392334 0.47886088490486145, 0.5018060207366943
Loss in 500 steps: 0.9725472331047058 0.47179651260375977, 0.500750720500946
Loss in 600 steps: 0.9798783659934998 0.4692772924900055, 0.5106010437011719
Loss in 700 steps: 0.9896251559257507 0.4813896715641022, 0.5082354545593262
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.972240686416626 0.4763863682746887, 0.49585428833961487
Loss in 100 steps: 0.9721819162368774 0.4691714346408844, 0.5030104517936707
Loss in 200 steps: 0.9940147995948792 0.4894717335700989, 0.5045430064201355
Loss in 300 steps: 0.9602735638618469 0.4606238007545471, 0.49964988231658936
Loss in 400 steps: 0.9713837504386902 0.4719642996788025, 0.4994194507598877
Loss in 500 steps: 0.9990509152412415 0.49251192808151245, 0.506538987159729
Loss in 600 steps: 0.981461763381958 0.4712856411933899, 0.5101761221885681
Loss in 700 steps: 0.9854122400283813 0.4761035442352295, 0.5093088150024414
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9796475172042847 0.48178631067276, 0.49786117672920227
Loss in 100 steps: 0.9762545824050903 0.47281044721603394, 0.5034440755844116
Loss in 200 steps: 0.9856894612312317 0.4833809733390808, 0.5023085474967957
Loss in 300 steps: 0.969618022441864 0.4696280062198639, 0.4999900460243225
Loss in 400 steps: 0.9727907776832581 0.4735167324542999, 0.4992741644382477
Loss in 500 steps: 0.9897997379302979 0.484403133392334, 0.5053966641426086
Loss in 600 steps: 0.9715097546577454 0.4636816084384918, 0.5078281760215759
Loss in 700 steps: 0.993975043296814 0.4847753643989563, 0.5091997385025024
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9734272956848145 0.4764065444469452, 0.49702078104019165
Loss in 100 steps: 0.974016010761261 0.4702376425266266, 0.5037783980369568
Loss in 200 steps: 0.985359251499176 0.4827941954135895, 0.5025650262832642
Loss in 300 steps: 0.9722776412963867 0.4704985022544861, 0.5017790794372559
Loss in 400 steps: 0.9623494148254395 0.46565529704093933, 0.49669408798217773
Loss in 500 steps: 0.9860485792160034 0.483045756816864, 0.5030028223991394
Loss in 600 steps: 0.9864652156829834 0.47671714425086975, 0.5097480416297913
Loss in 700 steps: 0.9805238842964172 0.4722651541233063, 0.5082587599754333
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9686765670776367 0.4740234911441803, 0.4946530759334564
Loss in 100 steps: 0.9733209609985352 0.4712725877761841, 0.5020484328269958
Loss in 200 steps: 0.9920113682746887 0.4891038239002228, 0.5029075145721436
Loss in 300 steps: 0.9624694585800171 0.46497371792793274, 0.49749574065208435
Loss in 400 steps: 0.9616802334785461 0.46525779366493225, 0.49642249941825867
Loss in 500 steps: 0.9850810170173645 0.4818539619445801, 0.5032270550727844
Loss in 600 steps: 0.9762862920761108 0.46675804257392883, 0.5095282793045044
Loss in 700 steps: 0.9929502606391907 0.48440295457839966, 0.5085474252700806
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.96456378698349 0.4712485074996948, 0.49331527948379517
Loss in 100 steps: 0.9734906554222107 0.4699958562850952, 0.5034947991371155
Loss in 200 steps: 0.9940000176429749 0.49053966999053955, 0.5034602880477905
Loss in 300 steps: 0.9572898149490356 0.4595908224582672, 0.49769899249076843
Loss in 400 steps: 0.9675348997116089 0.4707276523113251, 0.4968072474002838
Loss in 500 steps: 0.9747269153594971 0.4742969274520874, 0.5004299879074097
Loss in 600 steps: 0.9678383469581604 0.45924320816993713, 0.5085951685905457
Loss in 700 steps: 0.9966663718223572 0.487300306558609, 0.5093660354614258
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.97622150182724 0.4792935252189636, 0.496927946805954
Loss in 100 steps: 0.97408127784729 0.4701339602470398, 0.5039473176002502
Loss in 200 steps: 0.9921282529830933 0.4892091155052185, 0.5029191374778748
Loss in 300 steps: 0.978705108165741 0.47728395462036133, 0.5014210939407349
Loss in 400 steps: 0.9648135900497437 0.46748459339141846, 0.4973290264606476
Loss in 500 steps: 0.9891944527626038 0.4869457185268402, 0.5022487044334412
Loss in 600 steps: 0.9755409359931946 0.46489232778549194, 0.5106485486030579
Loss in 700 steps: 0.9905970692634583 0.4814024567604065, 0.509194552898407
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9653195142745972 0.4703596234321594, 0.49495992064476013
Loss in 100 steps: 0.9665685892105103 0.46333885192871094, 0.5032296776771545
Loss in 200 steps: 0.9862351417541504 0.482816606760025, 0.503418505191803
Loss in 300 steps: 0.9557971358299255 0.4572622776031494, 0.49853482842445374
Loss in 400 steps: 0.976624608039856 0.4780938923358917, 0.4985307455062866
Loss in 500 steps: 0.9746808409690857 0.4721633493900299, 0.5025174021720886
Loss in 600 steps: 0.9776660203933716 0.46892234683036804, 0.5087437033653259
Loss in 700 steps: 0.9947987794876099 0.487220823764801, 0.5075778961181641
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9721176624298096 0.4768379032611847, 0.4952797293663025
Loss in 100 steps: 0.9583118557929993 0.4562620520591736, 0.5020497441291809
Loss in 200 steps: 0.9972532987594604 0.49428051710128784, 0.5029727220535278
Loss in 300 steps: 0.962885320186615 0.464664101600647, 0.4982212483882904
Loss in 400 steps: 0.957845151424408 0.4621419310569763, 0.49570325016975403
Loss in 500 steps: 0.9816049933433533 0.4777189791202545, 0.5038859844207764
Loss in 600 steps: 0.9834332466125488 0.4743182957172394, 0.5091149806976318
Loss in 700 steps: 0.9975463151931763 0.48977795243263245, 0.5077683329582214
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9694668650627136 0.47530418634414673, 0.4941626489162445
Loss in 100 steps: 0.9760051965713501 0.47304654121398926, 0.5029586553573608
Loss in 200 steps: 0.9930639863014221 0.49015891551971436, 0.5029051303863525
Loss in 300 steps: 0.9737427830696106 0.47292059659957886, 0.5008220672607422
Loss in 400 steps: 0.9657533168792725 0.4689873158931732, 0.4967659115791321
Loss in 500 steps: 0.9846437573432922 0.4836086332798004, 0.5010351538658142
Loss in 600 steps: 0.9858120679855347 0.4775716960430145, 0.5082404017448425
Loss in 700 steps: 0.985366940498352 0.47788509726524353, 0.5074818730354309
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9711527228355408 0.47573262453079224, 0.49542009830474854
Loss in 100 steps: 0.9695082902908325 0.4682583510875702, 0.5012499094009399
Loss in 200 steps: 0.9932504892349243 0.4902001619338989, 0.5030503273010254
Loss in 300 steps: 0.9625895023345947 0.4659506380558014, 0.4966389238834381
Loss in 400 steps: 0.9614218473434448 0.46626678109169006, 0.49515506625175476
Loss in 500 steps: 0.9768930077552795 0.47635969519615173, 0.500533401966095
Loss in 600 steps: 0.9684845209121704 0.4604204297065735, 0.5080640316009521
Loss in 700 steps: 0.9857513308525085 0.4785972535610199, 0.5071539878845215
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9684712886810303 0.47405850887298584, 0.4944128096103668
Loss in 100 steps: 0.9765158891677856 0.4744912385940552, 0.5020247101783752
Loss in 200 steps: 1.0006071329116821 0.49754756689071655, 0.5030595064163208
Loss in 300 steps: 0.9592346549034119 0.46119555830955505, 0.49803903698921204
Loss in 400 steps: 0.9597269892692566 0.46329033374786377, 0.4964366555213928
Loss in 500 steps: 0.9935919046401978 0.4896523952484131, 0.5039395689964294
Loss in 600 steps: 0.9839536547660828 0.4746243357658386, 0.5093292593955994
Loss in 700 steps: 0.9979794025421143 0.4903221130371094, 0.5076572299003601
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9688438773155212 0.47395840287208557, 0.49488550424575806
Loss in 100 steps: 0.9737929105758667 0.4718797504901886, 0.5019131898880005
Loss in 200 steps: 1.0014162063598633 0.49754923582077026, 0.503866970539093
Loss in 300 steps: 0.9638760089874268 0.46336862444877625, 0.5005074143409729
Loss in 400 steps: 0.9665181636810303 0.4693650007247925, 0.497153103351593
Loss in 500 steps: 0.9812120795249939 0.4801962673664093, 0.501015841960907
Loss in 600 steps: 0.9673598408699036 0.4590301215648651, 0.5083297491073608
Loss in 700 steps: 0.9980525970458984 0.49065783619880676, 0.5073946118354797
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9699292182922363 0.4754490256309509, 0.494480162858963
Loss in 100 steps: 0.9804200530052185 0.47966697812080383, 0.5007529854774475
Loss in 200 steps: 0.9932315349578857 0.490886390209198, 0.502345085144043
Loss in 300 steps: 0.95902019739151 0.46046143770217896, 0.49855872988700867
Loss in 400 steps: 0.9668305516242981 0.46968144178390503, 0.49714910984039307
Loss in 500 steps: 0.9765748381614685 0.4762842059135437, 0.5002906322479248
Loss in 600 steps: 0.9676763415336609 0.4593958556652069, 0.5082805156707764
Loss in 700 steps: 0.9891363978385925 0.48127642273902893, 0.5078599452972412
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9746068716049194 0.478651762008667, 0.4959550201892853
Loss in 100 steps: 0.9642821550369263 0.46368691325187683, 0.5005952715873718
Loss in 200 steps: 0.9924729466438293 0.4902913570404053, 0.5021815896034241
Loss in 300 steps: 0.9620864391326904 0.4620249569416046, 0.5000614523887634
Loss in 400 steps: 0.9717658162117004 0.4740138351917267, 0.49775201082229614
Loss in 500 steps: 0.9951714277267456 0.4908733069896698, 0.5042980909347534
Loss in 600 steps: 0.9827621579170227 0.47361746430397034, 0.5091447830200195
Loss in 700 steps: 0.9907593727111816 0.48301592469215393, 0.5077435374259949
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9678168296813965 0.47265470027923584, 0.49516212940216064
Loss in 100 steps: 0.9650699496269226 0.464444637298584, 0.5006253123283386
Loss in 200 steps: 0.9923582077026367 0.490052729845047, 0.5023055076599121
Loss in 300 steps: 0.971397876739502 0.4698735475540161, 0.5015243291854858
Loss in 400 steps: 0.9592864513397217 0.4635794460773468, 0.4957070052623749
Loss in 500 steps: 0.9768486022949219 0.47609439492225647, 0.5007542371749878
Loss in 600 steps: 0.9799219965934753 0.47081536054611206, 0.5091065764427185
Loss in 700 steps: 0.9841012954711914 0.47608014941215515, 0.5080211162567139
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.980588972568512 0.4851096272468567, 0.4954792559146881
Loss in 100 steps: 0.9640986919403076 0.46341750025749207, 0.5006812214851379
Loss in 200 steps: 0.9960161447525024 0.49241968989372253, 0.5035964250564575
Loss in 300 steps: 0.9647404551506042 0.4662024676799774, 0.49853795766830444
Loss in 400 steps: 0.9640659689903259 0.46807578206062317, 0.49599015712738037
Loss in 500 steps: 0.9864622354507446 0.4858909845352173, 0.5005711913108826
Loss in 600 steps: 0.9750316739082336 0.46675440669059753, 0.5082772970199585
Loss in 700 steps: 0.9935501217842102 0.483959436416626, 0.5095908045768738
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9657998085021973 0.470593124628067, 0.4952065944671631
Loss in 100 steps: 0.9751949906349182 0.4725272059440613, 0.5026677846908569
Loss in 200 steps: 0.9871926307678223 0.48657849431037903, 0.5006141066551208
Loss in 300 steps: 0.9766303300857544 0.47546443343162537, 0.5011658668518066
Loss in 400 steps: 0.9664608240127563 0.4698212444782257, 0.4966396391391754
Loss in 500 steps: 0.9896772503852844 0.4845510721206665, 0.5051262378692627
Loss in 600 steps: 0.980351984500885 0.4723031222820282, 0.508048951625824
Loss in 700 steps: 0.9877169728279114 0.47894182801246643, 0.508774995803833
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.967915952205658 0.47440215945243835, 0.4935138523578644
Loss in 100 steps: 0.9783461093902588 0.4763806462287903, 0.5019655227661133
Loss in 200 steps: 0.9997250437736511 0.49571704864501953, 0.5040079355239868
Loss in 300 steps: 0.9738448858261108 0.4723648428916931, 0.5014800429344177
Loss in 400 steps: 0.9566951394081116 0.4618423283100128, 0.49485281109809875
Loss in 500 steps: 0.978840708732605 0.47536501288414, 0.5034757256507874
Loss in 600 steps: 0.9726020693778992 0.4643298089504242, 0.5082722902297974
Loss in 700 steps: 0.9964158535003662 0.48798373341560364, 0.5084320902824402
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9629716277122498 0.4692821800708771, 0.4936893880367279
Loss in 100 steps: 0.9759944081306458 0.4735915958881378, 0.5024028420448303
Loss in 200 steps: 0.9946533441543579 0.49249035120010376, 0.5021629929542542
Loss in 300 steps: 0.9661760330200195 0.46486037969589233, 0.5013156533241272
Loss in 400 steps: 0.9739959836006165 0.4739457666873932, 0.5000501871109009
Loss in 500 steps: 0.9702610969543457 0.47147607803344727, 0.49878501892089844
Loss in 600 steps: 0.972330629825592 0.464895099401474, 0.5074355006217957
Loss in 700 steps: 0.9845325350761414 0.47772887349128723, 0.5068036317825317
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9707421064376831 0.47563520073890686, 0.49510687589645386
Loss in 100 steps: 0.9784599542617798 0.4752447307109833, 0.5032151937484741
Loss in 200 steps: 0.993401825428009 0.4909592866897583, 0.5024425387382507
Loss in 300 steps: 0.9606453776359558 0.4622975289821625, 0.4983479678630829
Loss in 400 steps: 0.9667596817016602 0.4698061943054199, 0.49695345759391785
Loss in 500 steps: 0.9899464845657349 0.4881947636604309, 0.501751720905304
Loss in 600 steps: 0.9857634902000427 0.4757935702800751, 0.5099698901176453
Loss in 700 steps: 0.9815862774848938 0.4742078185081482, 0.5073784589767456
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9725617170333862 0.4764470160007477, 0.49611467123031616
Loss in 100 steps: 0.9779823422431946 0.47598856687545776, 0.5019937753677368
Loss in 200 steps: 0.9935024380683899 0.48980313539505005, 0.5036992430686951
Loss in 300 steps: 0.9730291366577148 0.4719933867454529, 0.501035749912262
Loss in 400 steps: 0.9628141522407532 0.4672803580760956, 0.4955337941646576
Loss in 500 steps: 0.9792596697807312 0.4784719944000244, 0.5007877349853516
Loss in 600 steps: 0.9736303687095642 0.4667789041996002, 0.5068514943122864
Loss in 700 steps: 0.9909191131591797 0.48293036222457886, 0.5079888105392456
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.973115861415863 0.47732847929000854, 0.4957874119281769
Loss in 100 steps: 0.9756025075912476 0.4736495912075043, 0.5019528865814209
Loss in 200 steps: 0.9928819537162781 0.48969125747680664, 0.5031906962394714
Loss in 300 steps: 0.9575671553611755 0.4583629071712494, 0.49920424818992615
Loss in 400 steps: 0.9672812223434448 0.47108036279678345, 0.4962008595466614
Loss in 500 steps: 0.9778122305870056 0.47418344020843506, 0.5036287903785706
Loss in 600 steps: 0.9816080331802368 0.47401925921440125, 0.5075887441635132
Loss in 700 steps: 0.9912769794464111 0.48340553045272827, 0.5078713893890381
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9656994938850403 0.4718167781829834, 0.4938827157020569
Loss in 100 steps: 0.9656190276145935 0.46510881185531616, 0.5005102753639221
Loss in 200 steps: 0.9930477738380432 0.48980551958084106, 0.5032423138618469
Loss in 300 steps: 0.972238302230835 0.4724796712398529, 0.49975863099098206
Loss in 400 steps: 0.9666452407836914 0.46979665756225586, 0.49684861302375793
Loss in 500 steps: 0.9822967052459717 0.47990742325782776, 0.5023892521858215
Loss in 600 steps: 0.9769607186317444 0.46870407462120056, 0.5082566142082214
Loss in 700 steps: 0.9901716709136963 0.48349955677986145, 0.506672203540802
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9670733213424683 0.4730476140975952, 0.4940257668495178
Loss in 100 steps: 0.9714625477790833 0.4711185097694397, 0.5003440976142883
Loss in 200 steps: 1.001650094985962 0.4985249638557434, 0.5031252503395081
Loss in 300 steps: 0.9780718088150024 0.4779191017150879, 0.500152587890625
Loss in 400 steps: 0.9649113416671753 0.46779537200927734, 0.4971160292625427
Loss in 500 steps: 0.9525893926620483 0.4562338888645172, 0.49635550379753113
Loss in 600 steps: 0.9943228363990784 0.48510125279426575, 0.5092215538024902
Loss in 700 steps: 0.9883913993835449 0.48077982664108276, 0.5076116323471069
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9666670560836792 0.47404026985168457, 0.49262672662734985
Loss in 100 steps: 0.9751420021057129 0.4741798937320709, 0.5009621381759644
Loss in 200 steps: 0.9916673302650452 0.48866182565689087, 0.5030054450035095
Loss in 300 steps: 0.9691869616508484 0.4691215753555298, 0.5000654458999634
Loss in 400 steps: 0.9482887387275696 0.4561575949192047, 0.49213114380836487
Loss in 500 steps: 0.9860162734985352 0.4833587408065796, 0.5026575326919556
Loss in 600 steps: 0.9770692586898804 0.468729168176651, 0.508340060710907
Loss in 700 steps: 0.9851600527763367 0.4767017364501953, 0.5084584355354309
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9720326662063599 0.4763733148574829, 0.49565941095352173
Loss in 100 steps: 0.9711472988128662 0.4699196219444275, 0.5012276768684387
Loss in 200 steps: 0.9959151148796082 0.4928993284702301, 0.5030157566070557
Loss in 300 steps: 0.9664125442504883 0.46807625889778137, 0.4983363151550293
Loss in 400 steps: 0.956752359867096 0.46305546164512634, 0.49369683861732483
Loss in 500 steps: 0.978314995765686 0.47836750745773315, 0.4999474287033081
Loss in 600 steps: 0.9713504314422607 0.4643447995185852, 0.5070056319236755
Loss in 700 steps: 0.9859476089477539 0.47836440801620483, 0.5075832009315491
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9664498567581177 0.47176051139831543, 0.49468934535980225
Loss in 100 steps: 0.972281813621521 0.4709669053554535, 0.5013149380683899
Loss in 200 steps: 0.9964726567268372 0.492648720741272, 0.50382399559021
Loss in 300 steps: 0.9628052115440369 0.4650079607963562, 0.49779731035232544
Loss in 400 steps: 0.9477090239524841 0.45340511202812195, 0.49430394172668457
Loss in 500 steps: 0.9868341088294983 0.4853677451610565, 0.5014663338661194
Loss in 600 steps: 0.993671715259552 0.4856332242488861, 0.5080384612083435
Loss in 700 steps: 0.981426477432251 0.47371557354927063, 0.5077109336853027
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.976608157157898 0.4801754653453827, 0.49643266201019287
Loss in 100 steps: 0.9742759466171265 0.47324612736701965, 0.501029908657074
Loss in 200 steps: 0.9971250295639038 0.49364131689071655, 0.5034836530685425
Loss in 300 steps: 0.9627823829650879 0.46528634428977966, 0.4974960684776306
Loss in 400 steps: 0.9607688188552856 0.46573299169540405, 0.4950357675552368
Loss in 500 steps: 0.9872953295707703 0.48307380080223083, 0.5042215585708618
Loss in 600 steps: 0.978385865688324 0.469939649105072, 0.5084461569786072
Loss in 700 steps: 0.9806920886039734 0.47273969650268555, 0.5079524517059326
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9649219512939453 0.4707620143890381, 0.49415987730026245
Loss in 100 steps: 0.9746750593185425 0.47501805424690247, 0.4996570944786072
Loss in 200 steps: 0.9947155117988586 0.4918709695339203, 0.5028446316719055
Loss in 300 steps: 0.9630489349365234 0.46362268924713135, 0.4994262456893921
Loss in 400 steps: 0.9532192349433899 0.45691028237342834, 0.4963088929653168
Loss in 500 steps: 0.975469708442688 0.4762188196182251, 0.4992509186267853
Loss in 600 steps: 0.9879022836685181 0.4793378412723541, 0.5085645318031311
Loss in 700 steps: 0.9847351908683777 0.47689181566238403, 0.5078433156013489
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9699363112449646 0.47498854994773865, 0.4949478805065155
Loss in 100 steps: 0.9816399812698364 0.4802483916282654, 0.5013915300369263
Loss in 200 steps: 1.0035789012908936 0.501224160194397, 0.5023546814918518
Loss in 300 steps: 0.96952885389328 0.4701383113861084, 0.49939054250717163
Loss in 400 steps: 0.9507291316986084 0.4569474160671234, 0.49378177523612976
Loss in 500 steps: 0.990618884563446 0.48488327860832214, 0.5057356953620911
Loss in 600 steps: 0.9751277565956116 0.46772539615631104, 0.5074023604393005
Loss in 700 steps: 0.9824393391609192 0.4752032160758972, 0.5072360634803772
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9656913876533508 0.4707892835140228, 0.49490219354629517
Loss in 100 steps: 0.97441565990448 0.4743596315383911, 0.5000560879707336
Loss in 200 steps: 0.9951592683792114 0.49134930968284607, 0.5038099884986877
Loss in 300 steps: 0.9641503095626831 0.46543237566947937, 0.49871793389320374
Loss in 400 steps: 0.9585803747177124 0.46450233459472656, 0.4940780699253082
Loss in 500 steps: 0.9810318350791931 0.4837173521518707, 0.497314453125
Loss in 600 steps: 0.969570517539978 0.46252667903900146, 0.5070438385009766
Loss in 700 steps: 0.9816750288009644 0.4744078516960144, 0.50726717710495
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9785946607589722 0.48160579800605774, 0.49698883295059204
Loss in 100 steps: 0.9699828624725342 0.4690268635749817, 0.5009559988975525
Loss in 200 steps: 1.0007020235061646 0.49835026264190674, 0.5023517608642578
Loss in 300 steps: 0.9617060422897339 0.46209824085235596, 0.4996078610420227
Loss in 400 steps: 0.9509859681129456 0.4571405351161957, 0.4938454329967499
Loss in 500 steps: 0.9760754704475403 0.4736270308494568, 0.5024484395980835
Loss in 600 steps: 0.9796922206878662 0.4718025326728821, 0.5078896880149841
Loss in 700 steps: 0.9818990230560303 0.4753169119358063, 0.5065820813179016
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9554364085197449 0.46190929412841797, 0.4935270845890045
Loss in 100 steps: 0.977047324180603 0.47528114914894104, 0.5017661452293396
Loss in 200 steps: 1.0025466680526733 0.5002331137657166, 0.5023136734962463
Loss in 300 steps: 0.9764707684516907 0.47598201036453247, 0.5004887580871582
Loss in 400 steps: 0.9642390012741089 0.4682167172431946, 0.4960223138332367
Loss in 500 steps: 0.984234631061554 0.4804914593696594, 0.5037431120872498
Loss in 600 steps: 0.9840797781944275 0.47451213002204895, 0.5095676183700562
Loss in 700 steps: 0.9881225824356079 0.4810631573200226, 0.5070592761039734
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.972257673740387 0.4760613739490509, 0.4961964190006256
Loss in 100 steps: 0.9771855473518372 0.47643470764160156, 0.5007508397102356
Loss in 200 steps: 0.9982622265815735 0.49518027901649475, 0.5030819773674011
Loss in 300 steps: 0.9749623537063599 0.4756646454334259, 0.4992976784706116
Loss in 400 steps: 0.9500870704650879 0.4546831548213959, 0.4954039752483368
Loss in 500 steps: 0.994164228439331 0.4882791042327881, 0.505885124206543
Loss in 600 steps: 0.9823088645935059 0.47514426708221436, 0.5071645379066467
Loss in 700 steps: 0.9888277053833008 0.4806203246116638, 0.5082073211669922
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9683813452720642 0.47414979338645935, 0.49423155188560486
Loss in 100 steps: 0.9803137183189392 0.47869157791137695, 0.5016221404075623
Loss in 200 steps: 0.9962462186813354 0.4938308894634247, 0.5024153590202332
Loss in 300 steps: 0.9715098142623901 0.47231540083885193, 0.4991944134235382
Loss in 400 steps: 0.9563690423965454 0.461870402097702, 0.49449867010116577
Loss in 500 steps: 0.9940217733383179 0.4900287389755249, 0.5039929747581482
Loss in 600 steps: 1.0031278133392334 0.4928494095802307, 0.5102784037590027
Loss in 700 steps: 0.9845440983772278 0.4775238335132599, 0.5070202350616455
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9701492786407471 0.4747040867805481, 0.49544522166252136
Loss in 100 steps: 0.9757931232452393 0.47513771057128906, 0.5006554126739502
Loss in 200 steps: 0.9850161671638489 0.4821033775806427, 0.5029129385948181
Loss in 300 steps: 0.9834811091423035 0.48265203833580017, 0.5008291006088257
Loss in 400 steps: 0.958188533782959 0.4623948037624359, 0.4957937002182007
Loss in 500 steps: 0.9810746312141418 0.47927409410476685, 0.5018006563186646
Loss in 600 steps: 0.9843654036521912 0.4774029850959778, 0.5069624185562134
Loss in 700 steps: 0.9857938289642334 0.4785251319408417, 0.5072686672210693
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9710808992385864 0.4741631746292114, 0.496917724609375
Loss in 100 steps: 0.9741522669792175 0.4739837944507599, 0.5001684427261353
Loss in 200 steps: 0.9930517673492432 0.48997658491134644, 0.503075122833252
Loss in 300 steps: 0.9746031165122986 0.47580885887145996, 0.4987943172454834
Loss in 400 steps: 0.9633470177650452 0.46652358770370483, 0.4968234598636627
Loss in 500 steps: 0.9943428039550781 0.48807621002197266, 0.5062664747238159
Loss in 600 steps: 0.9909915924072266 0.4831613302230835, 0.5078302621841431
Loss in 700 steps: 0.981257975101471 0.47441133856773376, 0.5068467259407043
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9643455743789673 0.4690590500831604, 0.4952864944934845
Loss in 100 steps: 0.9710497260093689 0.4711705148220062, 0.4998791813850403
Loss in 200 steps: 0.9932224154472351 0.4885571002960205, 0.5046653747558594
Loss in 300 steps: 0.9730026721954346 0.4730255603790283, 0.49997708201408386
Loss in 400 steps: 0.9544230103492737 0.4601229727268219, 0.49430006742477417
Loss in 500 steps: 0.977575421333313 0.4791108965873718, 0.49846452474594116
Loss in 600 steps: 0.97736656665802 0.4698803424835205, 0.5074862241744995
Loss in 700 steps: 0.975811779499054 0.46885037422180176, 0.5069613456726074
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9719046354293823 0.47617441415786743, 0.4957301914691925
Loss in 100 steps: 0.962697446346283 0.46350008249282837, 0.4991973340511322
Loss in 200 steps: 1.002991795539856 0.4997698962688446, 0.5032219290733337
Loss in 300 steps: 0.9711358547210693 0.4728632867336273, 0.49827253818511963
Loss in 400 steps: 0.9580880403518677 0.4617316424846649, 0.49635639786720276
Loss in 500 steps: 0.9879477620124817 0.4871552586555481, 0.5007925033569336
Loss in 600 steps: 0.9950132966041565 0.4852892756462097, 0.5097240209579468
Loss in 700 steps: 0.9761291146278381 0.4697791039943695, 0.5063498616218567
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9644939303398132 0.4697873592376709, 0.49470657110214233
Loss in 100 steps: 0.9739120006561279 0.4747617244720459, 0.49915027618408203
Loss in 200 steps: 0.9976370334625244 0.49506276845932007, 0.5025741457939148
Loss in 300 steps: 0.9692378044128418 0.4699312448501587, 0.4993065893650055
Loss in 400 steps: 0.9552426934242249 0.4605042338371277, 0.49473848938941956
Loss in 500 steps: 0.9840176105499268 0.48399436473846436, 0.5000232458114624
Loss in 600 steps: 0.9782805442810059 0.47102880477905273, 0.5072519183158875
Loss in 700 steps: 0.9865137338638306 0.4791337251663208, 0.507379949092865
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9698054790496826 0.4732838571071625, 0.49652159214019775
Loss in 100 steps: 0.9800148606300354 0.478754460811615, 0.50126051902771
Loss in 200 steps: 0.9972009062767029 0.4937150478363037, 0.503485918045044
Loss in 300 steps: 0.972953736782074 0.4731358289718628, 0.4998178482055664
Loss in 400 steps: 0.9634181261062622 0.46775346994400024, 0.49566471576690674
Loss in 500 steps: 0.9894176721572876 0.48628726601600647, 0.5031303763389587
Loss in 600 steps: 0.9939685463905334 0.48466557264328003, 0.5093030333518982
Loss in 700 steps: 0.9790077805519104 0.47198596596717834, 0.5070219039916992
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9632263779640198 0.468566358089447, 0.49465999007225037
Loss in 100 steps: 0.9752805233001709 0.4752429127693176, 0.5000376105308533
Loss in 200 steps: 0.99223792552948 0.48886847496032715, 0.5033694505691528
Loss in 300 steps: 0.9784995317459106 0.47760534286499023, 0.5008941292762756
Loss in 400 steps: 0.9584398865699768 0.46472111344337463, 0.493718683719635
Loss in 500 steps: 0.9793842434883118 0.4811975061893463, 0.4981868267059326
Loss in 600 steps: 0.9883816242218018 0.4803883135318756, 0.5079934000968933
Loss in 700 steps: 0.9930956363677979 0.48477903008461, 0.5083166360855103
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9629674553871155 0.4684695899486542, 0.4944978356361389
Loss in 100 steps: 0.9841262698173523 0.4823499619960785, 0.5017762780189514
Loss in 200 steps: 0.9982614517211914 0.4949997663497925, 0.5032615661621094
Loss in 300 steps: 0.9725291132926941 0.4713110625743866, 0.5012180209159851
Loss in 400 steps: 0.9571022987365723 0.4620855748653412, 0.49501675367355347
Loss in 500 steps: 0.9999275803565979 0.49677813053131104, 0.5031494498252869
Loss in 600 steps: 0.9848064184188843 0.4764746427536011, 0.5083317160606384
Loss in 700 steps: 0.9838542938232422 0.4771997928619385, 0.5066545605659485
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9700479507446289 0.4744013547897339, 0.49564656615257263
Loss in 100 steps: 0.9683716297149658 0.468167781829834, 0.5002038478851318
Loss in 200 steps: 0.9937092661857605 0.491689532995224, 0.5020197033882141
Loss in 300 steps: 0.9801120758056641 0.4791730046272278, 0.5009390711784363
Loss in 400 steps: 0.956847071647644 0.46121081709861755, 0.49563634395599365
Loss in 500 steps: 0.9773276448249817 0.4771084487438202, 0.5002192854881287
Loss in 600 steps: 0.9831053614616394 0.4752638041973114, 0.5078415274620056
Loss in 700 steps: 0.9873828887939453 0.4814443290233612, 0.5059385895729065
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9721559882164001 0.47494715452194214, 0.49720895290374756
Loss in 100 steps: 0.9724110960960388 0.47125834226608276, 0.5011528134346008
Loss in 200 steps: 0.9971165657043457 0.4934122860431671, 0.503704309463501
Loss in 300 steps: 0.974496603012085 0.47451770305633545, 0.49997881054878235
Loss in 400 steps: 0.9654484987258911 0.46948033571243286, 0.49596816301345825
Loss in 500 steps: 0.9900329113006592 0.4867541790008545, 0.5032786726951599
Loss in 600 steps: 0.9897258281707764 0.4815444350242615, 0.5081814527511597
Loss in 700 steps: 0.9777066707611084 0.4703100025653839, 0.5073966979980469
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9688005447387695 0.47266316413879395, 0.49613744020462036
Loss in 100 steps: 0.9722456932067871 0.4718376100063324, 0.5004080533981323
Loss in 200 steps: 0.9861847162246704 0.4831451177597046, 0.5030395984649658
Loss in 300 steps: 0.974900484085083 0.475763201713562, 0.4991372525691986
Loss in 400 steps: 0.9536678194999695 0.45866233110427856, 0.49500545859336853
Loss in 500 steps: 0.9924495220184326 0.48888200521469116, 0.5035675168037415
Loss in 600 steps: 0.9856367707252502 0.4783344864845276, 0.5073022842407227
Loss in 700 steps: 0.9901654720306396 0.4829632639884949, 0.5072022676467896
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9792545437812805 0.48236092925071716, 0.49689358472824097
Loss in 100 steps: 0.9843530654907227 0.4828789234161377, 0.5014740824699402
Loss in 200 steps: 0.9875231981277466 0.4830489754676819, 0.5044741630554199
Loss in 300 steps: 0.971388041973114 0.4733637869358063, 0.49802419543266296
Loss in 400 steps: 0.9620543122291565 0.46500569581985474, 0.49704861640930176
Loss in 500 steps: 0.9782108664512634 0.4773932695388794, 0.5008175373077393
Loss in 600 steps: 0.9752675890922546 0.46743619441986084, 0.5078313946723938
Loss in 700 steps: 0.9887940883636475 0.48023343086242676, 0.5085607171058655
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9691210389137268 0.47279468178749084, 0.4963262975215912
Loss in 100 steps: 0.9704766273498535 0.469533234834671, 0.5009434223175049
Loss in 200 steps: 0.9942824244499207 0.49045512080192566, 0.5038273334503174
Loss in 300 steps: 0.9753918051719666 0.4769488275051117, 0.49844300746917725
Loss in 400 steps: 0.9632495641708374 0.46651867032051086, 0.49673089385032654
Loss in 500 steps: 0.9660158753395081 0.4699588418006897, 0.49605706334114075
Loss in 600 steps: 0.9873252511024475 0.4797949492931366, 0.5075302720069885
Loss in 700 steps: 0.9857368469238281 0.478525847196579, 0.507210910320282
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9670674800872803 0.472074955701828, 0.49499255418777466
Loss in 100 steps: 0.9755931496620178 0.4750833809375763, 0.5005098581314087
Loss in 200 steps: 0.9855296015739441 0.4816611707210541, 0.5038684010505676
Loss in 300 steps: 0.9681955575942993 0.46782463788986206, 0.5003709197044373
Loss in 400 steps: 0.96371990442276 0.4666520059108734, 0.4970678985118866
Loss in 500 steps: 0.9970899820327759 0.49318283796310425, 0.5039071440696716
Loss in 600 steps: 0.9847337007522583 0.4767550826072693, 0.5079785585403442
Loss in 700 steps: 0.9848076701164246 0.47873952984809875, 0.5060680508613586
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9771325588226318 0.4799255132675171, 0.49720701575279236
Loss in 100 steps: 0.9732263684272766 0.4744318127632141, 0.4987945556640625
Loss in 200 steps: 0.9984923005104065 0.4945172667503357, 0.503974974155426
Loss in 300 steps: 0.972934901714325 0.47424522042274475, 0.498689740896225
Loss in 400 steps: 0.9566792249679565 0.4616398811340332, 0.4950394034385681
Loss in 500 steps: 0.96182781457901 0.46604830026626587, 0.49577951431274414
Loss in 600 steps: 0.9964235424995422 0.48876047134399414, 0.5076630711555481
Loss in 700 steps: 0.9811001420021057 0.4746658205986023, 0.5064342617988586
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9667505621910095 0.47039559483528137, 0.49635499715805054
Loss in 100 steps: 0.9697971940040588 0.4695679545402527, 0.5002291798591614
Loss in 200 steps: 0.9884553551673889 0.48600682616233826, 0.502448558807373
Loss in 300 steps: 0.965889573097229 0.4698815941810608, 0.496008038520813
Loss in 400 steps: 0.9585092663764954 0.46209245920181274, 0.4964168667793274
Loss in 500 steps: 0.9799214601516724 0.47762611508369446, 0.5022952556610107
Loss in 600 steps: 0.9941640496253967 0.4857596158981323, 0.5084044337272644
Loss in 700 steps: 0.9789565801620483 0.473509281873703, 0.5054472088813782
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9669598340988159 0.47108879685401917, 0.49587100744247437
Loss in 100 steps: 0.9780575633049011 0.47713717818260193, 0.5009204745292664
Loss in 200 steps: 0.988467812538147 0.4855669438838959, 0.5029008388519287
Loss in 300 steps: 0.972955584526062 0.4755341410636902, 0.49742141366004944
Loss in 400 steps: 0.9637154340744019 0.4665948450565338, 0.49712055921554565
Loss in 500 steps: 0.9723607301712036 0.47231966257095337, 0.5000410676002502
Loss in 600 steps: 0.9885362982749939 0.4807400703430176, 0.5077962279319763
Loss in 700 steps: 0.9844997525215149 0.4782634973526001, 0.5062361359596252
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9704441428184509 0.47504696249961853, 0.4953971803188324
Loss in 100 steps: 0.9660381078720093 0.4663771092891693, 0.4996609687805176
Loss in 200 steps: 0.9924784302711487 0.49049705266952515, 0.5019813179969788
Loss in 300 steps: 0.9634778499603271 0.4658624231815338, 0.4976154565811157
Loss in 400 steps: 0.965935468673706 0.4709087908267975, 0.4950266182422638
Loss in 500 steps: 0.9795915484428406 0.4804161787033081, 0.4991753399372101
Loss in 600 steps: 0.9730912446975708 0.46650955080986023, 0.5065816640853882
Loss in 700 steps: 0.9828605055809021 0.4761320650577545, 0.50672847032547
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9674580693244934 0.47155478596687317, 0.49590328335762024
Loss in 100 steps: 0.9686899185180664 0.4695943593978882, 0.4990955591201782
Loss in 200 steps: 0.9946251511573792 0.4920048117637634, 0.5026203393936157
Loss in 300 steps: 0.958905041217804 0.4601854681968689, 0.49871954321861267
Loss in 400 steps: 0.962897777557373 0.46730801463127136, 0.49558982253074646
Loss in 500 steps: 0.9719760417938232 0.47514212131500244, 0.4968339204788208
Loss in 600 steps: 0.9894695281982422 0.4816643297672272, 0.5078051686286926
Loss in 700 steps: 0.9844223856925964 0.4782176613807678, 0.5062046051025391
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9650800824165344 0.4713492691516876, 0.4937307834625244
Loss in 100 steps: 0.9744401574134827 0.4734952747821808, 0.5009448528289795
Loss in 200 steps: 0.9934203028678894 0.49045825004577637, 0.5029619932174683
Loss in 300 steps: 0.9681033492088318 0.470974862575531, 0.49712851643562317
Loss in 400 steps: 0.9641545414924622 0.4683963656425476, 0.49575820565223694
Loss in 500 steps: 0.9724202752113342 0.47375234961509705, 0.49866795539855957
Loss in 600 steps: 0.9842797517776489 0.4751121401786804, 0.5091676115989685
Loss in 700 steps: 0.9900104403495789 0.48254233598709106, 0.5074681043624878
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9625006318092346 0.4660874307155609, 0.49641329050064087
Loss in 100 steps: 0.9746885895729065 0.473446249961853, 0.5012423396110535
Loss in 200 steps: 0.9855715036392212 0.48271307349205017, 0.5028584003448486
Loss in 300 steps: 0.9698448181152344 0.4722670614719391, 0.4975777864456177
Loss in 400 steps: 0.9669390320777893 0.4704345166683197, 0.49650445580482483
Loss in 500 steps: 0.9797338843345642 0.4786115288734436, 0.5011223554611206
Loss in 600 steps: 0.9875849485397339 0.48092103004455566, 0.5066637992858887
Loss in 700 steps: 0.9787575602531433 0.4720071256160736, 0.5067504048347473
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9712950587272644 0.4756145179271698, 0.4956806004047394
Loss in 100 steps: 0.9699450135231018 0.4700583517551422, 0.49988672137260437
Loss in 200 steps: 0.9971182346343994 0.4939245879650116, 0.5031936764717102
Loss in 300 steps: 0.9718022346496582 0.47461551427841187, 0.4971867501735687
Loss in 400 steps: 0.9626672267913818 0.4666743874549866, 0.49599283933639526
Loss in 500 steps: 0.9832649230957031 0.4817509651184082, 0.5015139579772949
Loss in 600 steps: 0.9851385951042175 0.4778600037097931, 0.5072786211967468
Loss in 700 steps: 0.9837831258773804 0.4767560064792633, 0.5070272088050842
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9775282144546509 0.48211100697517395, 0.49541720747947693
Loss in 100 steps: 0.9632093906402588 0.46437838673591614, 0.49883100390434265
Loss in 200 steps: 1.0014504194259644 0.4982812702655792, 0.5031691193580627
Loss in 300 steps: 0.9698432087898254 0.4722236096858978, 0.49761950969696045
Loss in 400 steps: 0.9520948529243469 0.4585263729095459, 0.49356845021247864
Loss in 500 steps: 0.9783622026443481 0.4776762127876282, 0.5006860494613647
Loss in 600 steps: 0.9808438420295715 0.47416529059410095, 0.5066785216331482
Loss in 700 steps: 0.9791558980941772 0.47284168004989624, 0.5063142776489258
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9803179502487183 0.4838033616542816, 0.4965146481990814
Loss in 100 steps: 0.9698870778083801 0.46975767612457275, 0.5001294016838074
Loss in 200 steps: 1.0023778676986694 0.49866360425949097, 0.5037142634391785
Loss in 300 steps: 0.9651606678962708 0.46683740615844727, 0.4983232915401459
Loss in 400 steps: 0.9707574248313904 0.47510606050491333, 0.49565136432647705
Loss in 500 steps: 0.94725102186203 0.45465168356895447, 0.4925992488861084
Loss in 600 steps: 0.9874576330184937 0.48020535707473755, 0.5072523355484009
Loss in 700 steps: 0.9893677830696106 0.4813028872013092, 0.5080649256706238
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.972174882888794 0.4767671227455139, 0.49540767073631287
Loss in 100 steps: 0.9800840020179749 0.47938403487205505, 0.500700056552887
Loss in 200 steps: 1.0024203062057495 0.49934589862823486, 0.5030744075775146
Loss in 300 steps: 0.964484691619873 0.4661725163459778, 0.49831220507621765
Loss in 400 steps: 0.9585801959037781 0.465226948261261, 0.4933532476425171
Loss in 500 steps: 0.9504072666168213 0.4597213864326477, 0.4906858503818512
Loss in 600 steps: 0.9825073480606079 0.47564730048179626, 0.506860077381134
Loss in 700 steps: 0.9880521893501282 0.4807424545288086, 0.5073097348213196
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9763782620429993 0.48096776008605957, 0.4954104423522949
Loss in 100 steps: 0.9708935618400574 0.4709869623184204, 0.4999065399169922
Loss in 200 steps: 0.9911393523216248 0.48867321014404297, 0.5024662017822266
Loss in 300 steps: 0.9734429121017456 0.4750722646713257, 0.4983707070350647
Loss in 400 steps: 0.964274525642395 0.46812519431114197, 0.49614930152893066
Loss in 500 steps: 0.9746026396751404 0.4782543480396271, 0.4963483214378357
Loss in 600 steps: 0.986971914768219 0.4796483516693115, 0.5073236227035522
Loss in 700 steps: 0.9834145307540894 0.47573035955429077, 0.5076841711997986
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9819721579551697 0.48417168855667114, 0.49780040979385376
Loss in 100 steps: 0.9681069254875183 0.4678855538368225, 0.5002213716506958
Loss in 200 steps: 1.0013177394866943 0.4978673756122589, 0.503450334072113
Loss in 300 steps: 0.9592941403388977 0.46265605092048645, 0.49663805961608887
Loss in 400 steps: 0.9782259464263916 0.480673611164093, 0.49755239486694336
Loss in 500 steps: 0.9489465951919556 0.4571467339992523, 0.49179989099502563
Loss in 600 steps: 0.9941353797912598 0.48655420541763306, 0.5075811743736267
Loss in 700 steps: 0.9715412855148315 0.46596500277519226, 0.5055762529373169
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9841588139533997 0.48685401678085327, 0.4973047971725464
Loss in 100 steps: 0.9721760749816895 0.4727360010147095, 0.49944013357162476
Loss in 200 steps: 0.9868779182434082 0.4841712415218353, 0.5027066469192505
Loss in 300 steps: 0.9651482105255127 0.46810322999954224, 0.49704498052597046
Loss in 400 steps: 0.963843822479248 0.4681665301322937, 0.4956773519515991
Loss in 500 steps: 0.9672555923461914 0.47109586000442505, 0.4961596727371216
Loss in 600 steps: 0.9968979358673096 0.4885566532611847, 0.5083412528038025
Loss in 700 steps: 0.9893001317977905 0.4829200208187103, 0.5063800811767578
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9723726511001587 0.47723957896232605, 0.49513301253318787
Loss in 100 steps: 0.971427321434021 0.4722842872142792, 0.4991430938243866
Loss in 200 steps: 0.9937355518341064 0.4926777482032776, 0.5010578632354736
Loss in 300 steps: 0.968206524848938 0.46934524178504944, 0.49886125326156616
Loss in 400 steps: 0.9605912566184998 0.46367838978767395, 0.49691295623779297
Loss in 500 steps: 0.959696888923645 0.46377798914909363, 0.49591898918151855
Loss in 600 steps: 0.9804849624633789 0.4730426073074341, 0.5074424147605896
Loss in 700 steps: 0.9863243699073792 0.47890186309814453, 0.5074225068092346
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9750099182128906 0.47905677556991577, 0.49595317244529724
Loss in 100 steps: 0.9758092761039734 0.4755248725414276, 0.5002844333648682
Loss in 200 steps: 0.9899418354034424 0.4878278374671936, 0.502113938331604
Loss in 300 steps: 0.9594970345497131 0.4615790545940399, 0.4979179799556732
Loss in 400 steps: 0.9657562971115112 0.4701160788536072, 0.49564021825790405
Loss in 500 steps: 0.9649344682693481 0.4685184955596924, 0.49641603231430054
Loss in 600 steps: 0.9818890690803528 0.47410696744918823, 0.5077821612358093
Loss in 700 steps: 0.9716717600822449 0.46615418791770935, 0.5055176019668579
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.979401707649231 0.4817710816860199, 0.49763059616088867
Loss in 100 steps: 0.9746147394180298 0.4749896824359894, 0.4996250569820404
Loss in 200 steps: 0.9833167195320129 0.4803639352321625, 0.5029527544975281
Loss in 300 steps: 0.9654775857925415 0.4692663848400116, 0.4962111711502075
Loss in 400 steps: 0.9612825512886047 0.4650450050830841, 0.49623751640319824
Loss in 500 steps: 0.9508156776428223 0.4602733254432678, 0.49054238200187683
Loss in 600 steps: 0.9959097504615784 0.4874226748943329, 0.5084870457649231
Loss in 700 steps: 0.9802522659301758 0.47421079874038696, 0.5060413479804993
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9706716537475586 0.4739094078540802, 0.496762216091156
Loss in 100 steps: 0.976606547832489 0.47790002822875977, 0.49870654940605164
Loss in 200 steps: 0.9766727685928345 0.4742986559867859, 0.5023741722106934
Loss in 300 steps: 0.9754849672317505 0.47797662019729614, 0.4975082576274872
Loss in 400 steps: 0.9600319266319275 0.46411120891571045, 0.49592065811157227
Loss in 500 steps: 0.950985312461853 0.4557291269302368, 0.495256245136261
Loss in 600 steps: 0.9779991507530212 0.4708138406276703, 0.5071852803230286
Loss in 700 steps: 0.9876841902732849 0.48139992356300354, 0.5062842965126038
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9728215932846069 0.4757538139820099, 0.49706771969795227
Loss in 100 steps: 0.9779340028762817 0.4768019914627075, 0.5011320114135742
Loss in 200 steps: 0.9758140444755554 0.4749946892261505, 0.5008194446563721
Loss in 300 steps: 0.9782546758651733 0.4801187813282013, 0.49813592433929443
Loss in 400 steps: 0.9657472968101501 0.4701407849788666, 0.49560657143592834
Loss in 500 steps: 0.9691700339317322 0.47239598631858826, 0.49677401781082153
Loss in 600 steps: 0.9858567118644714 0.4792262613773346, 0.5066304206848145
Loss in 700 steps: 0.9742727875709534 0.46792343258857727, 0.5063494443893433
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.978564977645874 0.4799415171146393, 0.49862346053123474
Loss in 100 steps: 0.9724671244621277 0.47246798872947693, 0.49999916553497314
Loss in 200 steps: 0.9817926287651062 0.480216920375824, 0.5015757083892822
Loss in 300 steps: 0.9714628458023071 0.47336918115615845, 0.4980935752391815
Loss in 400 steps: 0.9652665257453918 0.47025686502456665, 0.4950096309185028
Loss in 500 steps: 0.9522944688796997 0.46078214049339294, 0.49151232838630676
Loss in 600 steps: 0.9873143434524536 0.4790547490119934, 0.5082595348358154
Loss in 700 steps: 0.9827278256416321 0.4762527644634247, 0.506475031375885
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9727181792259216 0.47639918327331543, 0.4963190257549286
Loss in 100 steps: 0.9795871376991272 0.47941866517066956, 0.5001684427261353
Loss in 200 steps: 0.9714979529380798 0.4700879454612732, 0.5014100074768066
Loss in 300 steps: 0.9616150856018066 0.46509674191474915, 0.4965183436870575
Loss in 400 steps: 0.9653711318969727 0.47079721093177795, 0.4945738911628723
Loss in 500 steps: 0.9582309722900391 0.46521103382110596, 0.4930199682712555
Loss in 600 steps: 0.9886288046836853 0.4809602200984955, 0.5076685547828674
Loss in 700 steps: 0.9851348400115967 0.477670818567276, 0.5074639320373535
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9729820489883423 0.47640398144721985, 0.49657803773880005
Loss in 100 steps: 0.9674340486526489 0.4685470461845398, 0.49888697266578674
Loss in 200 steps: 0.974466860294342 0.4712506830692291, 0.5032162666320801
Loss in 300 steps: 0.9654905796051025 0.46876803040504456, 0.4967225193977356
Loss in 400 steps: 0.9661498665809631 0.4715901017189026, 0.49455979466438293
Loss in 500 steps: 0.9581416845321655 0.46651026606559753, 0.4916313588619232
Loss in 600 steps: 0.9865167140960693 0.48008570075035095, 0.5064310431480408
Loss in 700 steps: 0.9799264073371887 0.4730272591114044, 0.5068991780281067
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9627722501754761 0.46820613741874695, 0.4945661425590515
Loss in 100 steps: 0.9755302667617798 0.47593989968299866, 0.49959033727645874
Loss in 200 steps: 0.9739165902137756 0.4729733169078827, 0.5009432435035706
Loss in 300 steps: 0.9632099866867065 0.46766990423202515, 0.49553999304771423
Loss in 400 steps: 0.9732227921485901 0.47673821449279785, 0.4964846074581146
Loss in 500 steps: 0.9485739469528198 0.45761236548423767, 0.49096155166625977
Loss in 600 steps: 0.9841387271881104 0.4761132001876831, 0.508025586605072
Loss in 700 steps: 0.9781651496887207 0.47272083163261414, 0.505444347858429
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.978975772857666 0.48193761706352234, 0.4970381557941437
Loss in 100 steps: 0.9730379581451416 0.47294315695762634, 0.5000948309898376
Loss in 200 steps: 0.9783543348312378 0.47712966799736023, 0.5012246966362
Loss in 300 steps: 0.9694420099258423 0.47160032391548157, 0.4978416860103607
Loss in 400 steps: 0.9653226733207703 0.4705979526042938, 0.4947247803211212
Loss in 500 steps: 0.9665177464485168 0.4710267186164856, 0.49549105763435364
Loss in 600 steps: 0.9933300018310547 0.48657673597335815, 0.5067532062530518
Loss in 700 steps: 0.9791229963302612 0.47232383489608765, 0.5067991018295288
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9673987627029419 0.4716671109199524, 0.4957317113876343
Loss in 100 steps: 0.976463794708252 0.47799474000930786, 0.49846914410591125
Loss in 200 steps: 0.9717531204223633 0.4711475074291229, 0.500605583190918
Loss in 300 steps: 0.9660312533378601 0.469832181930542, 0.4961991608142853
Loss in 400 steps: 0.975135862827301 0.4786509573459625, 0.4964848756790161
Loss in 500 steps: 0.970306396484375 0.4758297801017761, 0.4944766163825989
Loss in 600 steps: 0.9908430576324463 0.482995867729187, 0.5078471899032593
Loss in 700 steps: 0.9793327450752258 0.47310757637023926, 0.5062251687049866
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9631701707839966 0.4689562916755676, 0.49421387910842896
Loss in 100 steps: 0.9765782952308655 0.4785926640033722, 0.4979856610298157
Loss in 200 steps: 0.979558527469635 0.47754302620887756, 0.5020155906677246
Loss in 300 steps: 0.9635553956031799 0.4664223790168762, 0.49713295698165894
Loss in 400 steps: 0.9604844450950623 0.46508991718292236, 0.4953945577144623
Loss in 500 steps: 0.9638634920120239 0.46952444314956665, 0.4943389594554901
Loss in 600 steps: 0.9919363260269165 0.48651695251464844, 0.5054193139076233
Loss in 700 steps: 0.9791970252990723 0.4724906384944916, 0.5067064762115479
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.962638258934021 0.4670454263687134, 0.49559286236763
Loss in 100 steps: 0.9773209691047668 0.4768843352794647, 0.5004366636276245
Loss in 200 steps: 0.9745352864265442 0.47292453050613403, 0.5016107559204102
Loss in 300 steps: 0.9715667963027954 0.47306644916534424, 0.49850037693977356
Loss in 400 steps: 0.9641426801681519 0.469521701335907, 0.4946209490299225
Loss in 500 steps: 0.9524739384651184 0.4588255286216736, 0.4936484694480896
Loss in 600 steps: 0.9841823577880859 0.47744500637054443, 0.5067373514175415
Loss in 700 steps: 0.9710496664047241 0.46472984552383423, 0.5063197612762451
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9726513624191284 0.47497978806495667, 0.49767160415649414
Loss in 100 steps: 0.9777867197990417 0.4792270362377167, 0.4985596537590027
Loss in 200 steps: 0.965640127658844 0.4641082286834717, 0.5015319585800171
Loss in 300 steps: 0.9680724143981934 0.46951720118522644, 0.4985552430152893
Loss in 400 steps: 0.9635384678840637 0.46769624948501587, 0.4958421289920807
Loss in 500 steps: 0.9599212408065796 0.4639993906021118, 0.4959217607975006
Loss in 600 steps: 0.987979531288147 0.4796275198459625, 0.5083520412445068
Loss in 700 steps: 0.9807361960411072 0.47368818521499634, 0.5070480108261108
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9795560240745544 0.4830111265182495, 0.49654483795166016
Loss in 100 steps: 0.9657366275787354 0.46835532784461975, 0.49738121032714844
Loss in 200 steps: 0.9762330651283264 0.4739096164703369, 0.5023234486579895
Loss in 300 steps: 0.9637153744697571 0.46511536836624146, 0.498600035905838
Loss in 400 steps: 0.960945725440979 0.4672408401966095, 0.4937049448490143
Loss in 500 steps: 0.9550150036811829 0.46285131573677063, 0.492163747549057
Loss in 600 steps: 0.9795343279838562 0.47326478362083435, 0.5062693357467651
Loss in 700 steps: 0.9756214022636414 0.46897709369659424, 0.5066443085670471
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9687978029251099 0.4738472104072571, 0.49495062232017517
Loss in 100 steps: 0.9661073088645935 0.46854814887046814, 0.49755918979644775
Loss in 200 steps: 0.9728853106498718 0.47115129232406616, 0.5017339587211609
Loss in 300 steps: 0.9629310965538025 0.4652716815471649, 0.4976593554019928
Loss in 400 steps: 0.9545133113861084 0.46177297830581665, 0.49274036288261414
Loss in 500 steps: 0.9547292590141296 0.4633710980415344, 0.4913581907749176
Loss in 600 steps: 0.9839862585067749 0.47674763202667236, 0.5072386264801025
Loss in 700 steps: 0.9875056743621826 0.48019471764564514, 0.5073109269142151
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9742050170898438 0.47751089930534363, 0.49669408798217773
Loss in 100 steps: 0.9716646075248718 0.4724447727203369, 0.4992198944091797
Loss in 200 steps: 0.9739469885826111 0.4727668762207031, 0.5011800527572632
Loss in 300 steps: 0.9681709408760071 0.4702734649181366, 0.4978974461555481
Loss in 400 steps: 0.9576603174209595 0.4608646333217621, 0.4967957139015198
Loss in 500 steps: 0.9667994976043701 0.4727785587310791, 0.49402087926864624
Loss in 600 steps: 0.9924573302268982 0.4857393205165863, 0.5067180395126343
Loss in 700 steps: 0.991499662399292 0.48450377583503723, 0.5069959759712219
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9755131602287292 0.4791155457496643, 0.49639758467674255
Loss in 100 steps: 0.9623931646347046 0.46531301736831665, 0.49708011746406555
Loss in 200 steps: 0.9846784472465515 0.48254549503326416, 0.5021329522132874
Loss in 300 steps: 0.9708073735237122 0.4714431166648865, 0.4993642270565033
Loss in 400 steps: 0.9621385335922241 0.4668528437614441, 0.49528563022613525
Loss in 500 steps: 0.9624239206314087 0.46810972690582275, 0.49431416392326355
Loss in 600 steps: 0.9943222403526306 0.48645856976509094, 0.5078636407852173
Loss in 700 steps: 0.9814011454582214 0.4746617376804352, 0.5067394375801086
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9694117903709412 0.4726177752017975, 0.4967939853668213
Loss in 100 steps: 0.9707990884780884 0.47318392992019653, 0.49761518836021423
Loss in 200 steps: 0.9761645197868347 0.475143700838089, 0.5010208487510681
Loss in 300 steps: 0.9651203751564026 0.4665924906730652, 0.498527854681015
Loss in 400 steps: 0.9495635628700256 0.45456287264823914, 0.4950006604194641
Loss in 500 steps: 0.9684609174728394 0.47500741481781006, 0.4934535026550293
Loss in 600 steps: 0.9913886189460754 0.4840444028377533, 0.5073441863059998
Loss in 700 steps: 0.9851890802383423 0.47917139530181885, 0.5060177445411682
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.961151659488678 0.46721237897872925, 0.49393925070762634
Loss in 100 steps: 0.9740030765533447 0.4759137034416199, 0.49808940291404724
Loss in 200 steps: 0.971689760684967 0.47063374519348145, 0.5010559558868408
Loss in 300 steps: 0.9613412022590637 0.4645006060600281, 0.49684062600135803
Loss in 400 steps: 0.954716145992279 0.4597833454608917, 0.49493280053138733
Loss in 500 steps: 0.9673954248428345 0.47190234065055847, 0.4954931139945984
Loss in 600 steps: 0.9832068085670471 0.4772583246231079, 0.5059484839439392
Loss in 700 steps: 0.9803130626678467 0.4725986123085022, 0.5077145099639893
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9744251370429993 0.47907009720802307, 0.49535512924194336
Loss in 100 steps: 0.9749364256858826 0.47663551568984985, 0.4983009696006775
Loss in 200 steps: 0.9714788794517517 0.47086748480796814, 0.5006113648414612
Loss in 300 steps: 0.960547924041748 0.4635419249534607, 0.49700599908828735
Loss in 400 steps: 0.9602587819099426 0.4643973708152771, 0.4958614706993103
Loss in 500 steps: 0.9667406678199768 0.47128137946128845, 0.49545928835868835
Loss in 600 steps: 0.9821385145187378 0.47673025727272034, 0.5054082274436951
Loss in 700 steps: 0.9782754182815552 0.4707462787628174, 0.5075291395187378
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9636542797088623 0.4691850543022156, 0.4944692552089691
Loss in 100 steps: 0.9684685468673706 0.47041547298431396, 0.49805307388305664
Loss in 200 steps: 0.9669370651245117 0.4659622013568878, 0.5009748339653015
Loss in 300 steps: 0.9708343148231506 0.47218945622444153, 0.4986448884010315
Loss in 400 steps: 0.9600748419761658 0.4656180143356323, 0.49445682764053345
Loss in 500 steps: 0.9666518568992615 0.4740489721298218, 0.4926028549671173
Loss in 600 steps: 0.9781445264816284 0.4712405502796173, 0.5069040060043335
Loss in 700 steps: 0.9837205410003662 0.47747719287872314, 0.5062433481216431
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9670966863632202 0.47096845507621765, 0.49612823128700256
Loss in 100 steps: 0.9801046848297119 0.481612890958786, 0.49849173426628113
Loss in 200 steps: 0.9672375321388245 0.46597713232040405, 0.5012604594230652
Loss in 300 steps: 0.958279013633728 0.46145278215408325, 0.4968262016773224
Loss in 400 steps: 0.9569980502128601 0.4611721634864807, 0.4958259165287018
Loss in 500 steps: 0.9674329161643982 0.47217705845832825, 0.4952557682991028
Loss in 600 steps: 0.9872068762779236 0.48059406876564026, 0.5066128373146057
Loss in 700 steps: 0.9904119372367859 0.48323068022727966, 0.5071812868118286
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9639167785644531 0.4695332646369934, 0.49438345432281494
Loss in 100 steps: 0.9755544662475586 0.4769095182418823, 0.49864494800567627
Loss in 200 steps: 0.9699944257736206 0.46898573637008667, 0.5010086894035339
Loss in 300 steps: 0.9635148644447327 0.46423760056495667, 0.4992772936820984
Loss in 400 steps: 0.9673861265182495 0.472965806722641, 0.4944204092025757
Loss in 500 steps: 0.9711781740188599 0.47447147965431213, 0.4967067837715149
Loss in 600 steps: 0.9948074817657471 0.4877881407737732, 0.5070194005966187
Loss in 700 steps: 0.9885709881782532 0.48160138726234436, 0.5069695711135864
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9716585278511047 0.47510573267936707, 0.49655285477638245
Loss in 100 steps: 0.9765545725822449 0.47707951068878174, 0.49947506189346313
Loss in 200 steps: 0.9772947430610657 0.47744351625442505, 0.4998512268066406
Loss in 300 steps: 0.9661880135536194 0.4671592116355896, 0.4990287721157074
Loss in 400 steps: 0.9566348791122437 0.4624977111816406, 0.494137167930603
Loss in 500 steps: 0.9809027910232544 0.48167717456817627, 0.49922552704811096
Loss in 600 steps: 0.9776360988616943 0.47274863719940186, 0.5048875212669373
Loss in 700 steps: 0.9781510233879089 0.47205638885498047, 0.5060945749282837
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9582726955413818 0.46362563967704773, 0.4946470558643341
Loss in 100 steps: 0.9851021766662598 0.4852786064147949, 0.49982360005378723
Loss in 200 steps: 0.9708976149559021 0.4699993133544922, 0.5008983612060547
Loss in 300 steps: 0.9707765579223633 0.4733317494392395, 0.49744486808776855
Loss in 400 steps: 0.9516529440879822 0.4602240324020386, 0.4914288818836212
Loss in 500 steps: 0.9543178081512451 0.46333909034729004, 0.4909787178039551
Loss in 600 steps: 1.0009684562683105 0.4934544265270233, 0.5075140595436096
Loss in 700 steps: 0.9828126430511475 0.47542253136634827, 0.5073899626731873
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9654991626739502 0.47015655040740967, 0.4953426122665405
Loss in 100 steps: 0.9651445150375366 0.46741220355033875, 0.49773234128952026
Loss in 200 steps: 0.9761049151420593 0.4759196937084198, 0.5001853108406067
Loss in 300 steps: 0.9664720296859741 0.46743351221084595, 0.4990384876728058
Loss in 400 steps: 0.9606376886367798 0.46676963567733765, 0.49386799335479736
Loss in 500 steps: 0.9727951884269714 0.477043092250824, 0.49575215578079224
Loss in 600 steps: 0.9831107258796692 0.4774710536003113, 0.5056397318840027
Loss in 700 steps: 0.9922643303871155 0.48513054847717285, 0.5071339011192322
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9755526781082153 0.4803754985332489, 0.49517714977264404
Loss in 100 steps: 0.9718698859214783 0.47412362694740295, 0.4977463185787201
Loss in 200 steps: 0.9695940613746643 0.46884411573410034, 0.5007498264312744
Loss in 300 steps: 0.9622104167938232 0.46573954820632935, 0.49647077918052673
Loss in 400 steps: 0.9621498584747314 0.4676627516746521, 0.4944871962070465
Loss in 500 steps: 0.9716630578041077 0.4738416373729706, 0.4978214204311371
Loss in 600 steps: 0.9695099592208862 0.4649354815483093, 0.5045744180679321
Loss in 700 steps: 0.9867725372314453 0.4798564314842224, 0.5069161057472229
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9551398754119873 0.4598185122013092, 0.4953213930130005
Loss in 100 steps: 0.9772512912750244 0.47954297065734863, 0.497708261013031
Loss in 200 steps: 0.9760975241661072 0.4744723439216614, 0.5016251802444458
Loss in 300 steps: 0.967968225479126 0.46901264786720276, 0.49895551800727844
Loss in 400 steps: 0.9687232971191406 0.4721873104572296, 0.496535986661911
Loss in 500 steps: 0.9704147577285767 0.4762914478778839, 0.494123250246048
Loss in 600 steps: 0.9862397313117981 0.4796299636363983, 0.5066097974777222
Loss in 700 steps: 0.9898671507835388 0.48227542638778687, 0.5075916051864624
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9628468155860901 0.46824946999549866, 0.4945973753929138
Loss in 100 steps: 0.9761659502983093 0.47802963852882385, 0.4981363117694855
Loss in 200 steps: 0.9754538536071777 0.47370484471321106, 0.5017490386962891
Loss in 300 steps: 0.9710368514060974 0.473152220249176, 0.4978846311569214
Loss in 400 steps: 0.9630276560783386 0.4690283238887787, 0.49399930238723755
Loss in 500 steps: 0.9855250716209412 0.48534679412841797, 0.500178337097168
Loss in 600 steps: 0.9803372025489807 0.47538456320762634, 0.5049526691436768
Loss in 700 steps: 0.9873510003089905 0.4793771803379059, 0.5079737901687622
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9705784916877747 0.4740036129951477, 0.49657484889030457
Loss in 100 steps: 0.9842821955680847 0.48629796504974365, 0.49798426032066345
Loss in 200 steps: 0.9656679630279541 0.4644484221935272, 0.5012195706367493
Loss in 300 steps: 0.9696440100669861 0.47112205624580383, 0.49852195382118225
Loss in 400 steps: 0.9491115212440491 0.4569210410118103, 0.49219048023223877
Loss in 500 steps: 0.9638006091117859 0.47148627042770386, 0.49231430888175964
Loss in 600 steps: 0.9916544556617737 0.48558875918388367, 0.5060657262802124
Loss in 700 steps: 0.9873210787773132 0.4798353612422943, 0.5074856281280518
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.975489616394043 0.4778032898902893, 0.49768635630607605
Loss in 100 steps: 0.9723540544509888 0.4745763838291168, 0.49777764081954956
Loss in 200 steps: 0.9725337028503418 0.4709087014198303, 0.5016250014305115
Loss in 300 steps: 0.9657818078994751 0.4678749740123749, 0.497906893491745
Loss in 400 steps: 0.9561757445335388 0.4640432894229889, 0.4921324849128723
Loss in 500 steps: 0.9746028780937195 0.4760536849498749, 0.498549222946167
Loss in 600 steps: 0.9825560450553894 0.47807005047798157, 0.504486083984375
Loss in 700 steps: 0.9883087277412415 0.4806501269340515, 0.5076585412025452
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9633315205574036 0.46797215938568115, 0.4953594207763672
Loss in 100 steps: 0.9762536287307739 0.47806596755981445, 0.49818772077560425
Loss in 200 steps: 0.966213583946228 0.4646911919116974, 0.501522421836853
Loss in 300 steps: 0.9650285243988037 0.4671609699726105, 0.497867614030838
Loss in 400 steps: 0.9523751139640808 0.45968562364578247, 0.49268949031829834
Loss in 500 steps: 0.9765802621841431 0.4770529568195343, 0.49952733516693115
Loss in 600 steps: 0.9840376973152161 0.47894543409347534, 0.5050922632217407
Loss in 700 steps: 0.9803029894828796 0.4739364981651306, 0.506366491317749
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9653584361076355 0.47113049030303955, 0.49422797560691833
Loss in 100 steps: 0.9750990271568298 0.4779451787471771, 0.4971538186073303
Loss in 200 steps: 0.9838756322860718 0.48211705684661865, 0.5017586350440979
Loss in 300 steps: 0.965026319026947 0.46885165572166443, 0.4961746633052826
Loss in 400 steps: 0.9547056555747986 0.46240857243537903, 0.49229708313941956
Loss in 500 steps: 0.9914871454238892 0.49173739552497864, 0.4997497797012329
Loss in 600 steps: 0.9948959946632385 0.48764678835868835, 0.5072492361068726
Loss in 700 steps: 0.9938748478889465 0.487159788608551, 0.5067150592803955
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9783113598823547 0.48372969031333923, 0.4945816397666931
Loss in 100 steps: 0.9652251601219177 0.4663671553134918, 0.4988580346107483
Loss in 200 steps: 0.9746940732002258 0.47325676679611206, 0.5014373064041138
Loss in 300 steps: 0.9747236371040344 0.4769451320171356, 0.4977785050868988
Loss in 400 steps: 0.9668141603469849 0.47289738059043884, 0.4939168095588684
Loss in 500 steps: 0.9889228940010071 0.48999664187431335, 0.4989262521266937
Loss in 600 steps: 0.9899328351020813 0.48327988386154175, 0.5066529512405396
Loss in 700 steps: 0.9957334995269775 0.48803430795669556, 0.507699191570282
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9613401889801025 0.4664539098739624, 0.49488621950149536
Loss in 100 steps: 0.9738768935203552 0.4765157103538513, 0.49736112356185913
Loss in 200 steps: 0.9811227917671204 0.4810160994529724, 0.5001067519187927
Loss in 300 steps: 0.971164345741272 0.47172579169273376, 0.499438613653183
Loss in 400 steps: 0.9603897929191589 0.46611011028289795, 0.49427974224090576
Loss in 500 steps: 0.9741654992103577 0.47946083545684814, 0.49470457434654236
Loss in 600 steps: 0.9787740111351013 0.4743942320346832, 0.5043796896934509
Loss in 700 steps: 0.9832686185836792 0.4768638014793396, 0.5064048171043396
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9698557257652283 0.473360151052475, 0.49649548530578613
Loss in 100 steps: 0.9822102189064026 0.48282504081726074, 0.49938511848449707
Loss in 200 steps: 0.9764066934585571 0.47464972734451294, 0.5017569065093994
Loss in 300 steps: 0.9757770895957947 0.47831976413726807, 0.49745726585388184
Loss in 400 steps: 0.959063708782196 0.46727174520492554, 0.4917919933795929
Loss in 500 steps: 0.9984201788902283 0.49611273407936096, 0.5023075342178345
Loss in 600 steps: 0.9971088767051697 0.49071285128593445, 0.5063960552215576
Loss in 700 steps: 0.9906374216079712 0.4844236373901367, 0.5062137246131897
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9694482088088989 0.4724532663822174, 0.4969949424266815
Loss in 100 steps: 0.9702239036560059 0.4723764657974243, 0.49784746766090393
Loss in 200 steps: 0.9711808562278748 0.4690944254398346, 0.5020864605903625
Loss in 300 steps: 0.9698127508163452 0.4726793169975281, 0.4971334636211395
Loss in 400 steps: 0.9483850598335266 0.4555937647819519, 0.4927912950515747
Loss in 500 steps: 0.9612992405891418 0.46737340092658997, 0.4939258396625519
Loss in 600 steps: 0.9910666942596436 0.48398226499557495, 0.5070844888687134
Loss in 700 steps: 0.9918097853660583 0.48467737436294556, 0.507132351398468
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9735085368156433 0.4773654341697693, 0.49614307284355164
Loss in 100 steps: 0.9756734371185303 0.4778714179992676, 0.4978020191192627
Loss in 200 steps: 0.982037365436554 0.4801459014415741, 0.5018914937973022
Loss in 300 steps: 0.9589553475379944 0.4623393714427948, 0.4966158866882324
Loss in 400 steps: 0.9565926194190979 0.4626149833202362, 0.4939776659011841
Loss in 500 steps: 0.9689536094665527 0.47450143098831177, 0.49445220828056335
Loss in 600 steps: 0.9872319102287292 0.48091089725494385, 0.5063209533691406
Loss in 700 steps: 0.9939209222793579 0.4875833988189697, 0.5063374638557434
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9677928686141968 0.47263607382774353, 0.49515682458877563
Loss in 100 steps: 0.9819463491439819 0.48348307609558105, 0.4984632730484009
Loss in 200 steps: 0.9788488149642944 0.47818443179130554, 0.5006644129753113
Loss in 300 steps: 0.9740670919418335 0.47522634267807007, 0.4988408386707306
Loss in 400 steps: 0.956325888633728 0.463207870721817, 0.49311795830726624
Loss in 500 steps: 0.9894430637359619 0.4886620342731476, 0.5007809996604919
Loss in 600 steps: 0.9924332499504089 0.4858910143375397, 0.5065422654151917
Loss in 700 steps: 0.9838263392448425 0.4763953685760498, 0.5074309706687927
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9725873470306396 0.4756265878677368, 0.49696072936058044
Loss in 100 steps: 0.9794294834136963 0.4814528524875641, 0.4979766607284546
Loss in 200 steps: 0.9738964438438416 0.471641480922699, 0.5022549033164978
Loss in 300 steps: 0.9670241475105286 0.4707562327384949, 0.4962679445743561
Loss in 400 steps: 0.959969699382782 0.46596309542655945, 0.4940066635608673
Loss in 500 steps: 0.9803381562232971 0.48260098695755005, 0.49773719906806946
Loss in 600 steps: 0.9808031916618347 0.47562241554260254, 0.5051807761192322
Loss in 700 steps: 0.9883988499641418 0.48165345191955566, 0.5067453384399414
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9656352996826172 0.46887290477752686, 0.4967624247074127
Loss in 100 steps: 0.9843525886535645 0.48565566539764404, 0.4986969232559204
Loss in 200 steps: 0.9671609997749329 0.46562549471855164, 0.5015355944633484
Loss in 300 steps: 0.9731592535972595 0.474869042634964, 0.49829018115997314
Loss in 400 steps: 0.9513102173805237 0.4588968753814697, 0.49241337180137634
Loss in 500 steps: 0.9728754758834839 0.4759403467178345, 0.496935099363327
Loss in 600 steps: 0.9889569282531738 0.4819260239601135, 0.5070309042930603
Loss in 700 steps: 0.9854767322540283 0.4789094924926758, 0.5065671801567078
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9691897034645081 0.47223979234695435, 0.4969499111175537
Loss in 100 steps: 0.9782517552375793 0.47911733388900757, 0.4991343915462494
Loss in 200 steps: 0.9803848266601562 0.47882211208343506, 0.5015626549720764
Loss in 300 steps: 0.9706858396530151 0.47237133979797363, 0.49831438064575195
Loss in 400 steps: 0.9501232504844666 0.4567878842353821, 0.4933353364467621
Loss in 500 steps: 0.9942648410797119 0.49206823110580444, 0.5021966099739075
Loss in 600 steps: 0.9824442267417908 0.47690409421920776, 0.505540132522583
Loss in 700 steps: 0.9846806526184082 0.47775107622146606, 0.5069296360015869
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9666730165481567 0.46970704197883606, 0.4969659745693207
Loss in 100 steps: 0.9784345626831055 0.4801579415798187, 0.4982765316963196
Loss in 200 steps: 0.9701003432273865 0.46859219670295715, 0.5015081167221069
Loss in 300 steps: 0.9734288454055786 0.47627705335617065, 0.49715179204940796
Loss in 400 steps: 0.9527686238288879 0.4597001075744629, 0.49306854605674744
Loss in 500 steps: 0.9797585606575012 0.481285035610199, 0.49847346544265747
Loss in 600 steps: 0.9910791516304016 0.48470449447631836, 0.5063745975494385
Loss in 700 steps: 0.9881420731544495 0.48191842436790466, 0.5062236785888672
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9618649482727051 0.46547359228134155, 0.4963913857936859
Loss in 100 steps: 0.981184720993042 0.4822937846183777, 0.49889087677001953
Loss in 200 steps: 0.9745499491691589 0.47391337156295776, 0.5006365776062012
Loss in 300 steps: 0.9695974588394165 0.4708312749862671, 0.49876609444618225
Loss in 400 steps: 0.9550807476043701 0.46193796396255493, 0.4931427240371704
Loss in 500 steps: 0.9857236742973328 0.48534637689590454, 0.500377357006073
Loss in 600 steps: 0.9905079007148743 0.4842786192893982, 0.5062292814254761
Loss in 700 steps: 0.9817603230476379 0.47642406821250916, 0.5053362846374512
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9703629016876221 0.47269731760025024, 0.49766552448272705
Loss in 100 steps: 0.9682770371437073 0.4692586064338684, 0.49901843070983887
Loss in 200 steps: 0.9807531833648682 0.47815218567848206, 0.5026010274887085
Loss in 300 steps: 0.973329484462738 0.4745105504989624, 0.498818963766098
Loss in 400 steps: 0.9449888467788696 0.4524378180503845, 0.4925510585308075
Loss in 500 steps: 0.9872729182243347 0.486482173204422, 0.5007907152175903
Loss in 600 steps: 0.9902518391609192 0.48312535881996155, 0.50712651014328
Loss in 700 steps: 0.9859614968299866 0.4796214997768402, 0.506339967250824
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9631009101867676 0.46707865595817566, 0.4960222840309143
Loss in 100 steps: 0.9699316024780273 0.47243550419807434, 0.4974961280822754
Loss in 200 steps: 0.9747024774551392 0.4725930690765381, 0.5021093487739563
Loss in 300 steps: 0.9725673198699951 0.47371870279312134, 0.49884864687919617
Loss in 400 steps: 0.9400253891944885 0.4505453109741211, 0.48948004841804504
Loss in 500 steps: 0.9730349779129028 0.4769555628299713, 0.4960794448852539
Loss in 600 steps: 0.9841259717941284 0.4776771068572998, 0.5064488649368286
Loss in 700 steps: 0.9915991425514221 0.4857528507709503, 0.505846381187439
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9699670672416687 0.4743022620677948, 0.4956648051738739
Loss in 100 steps: 0.9758997559547424 0.47933027148246765, 0.49656951427459717
Loss in 200 steps: 0.973544180393219 0.4717841148376465, 0.5017601847648621
Loss in 300 steps: 0.976714015007019 0.4779644012451172, 0.49874964356422424
Loss in 400 steps: 0.9515255689620972 0.46034175157546997, 0.491183876991272
Loss in 500 steps: 0.9713220596313477 0.475370854139328, 0.4959511458873749
Loss in 600 steps: 0.9948881268501282 0.4875759780406952, 0.5073121786117554
Loss in 700 steps: 0.9914621114730835 0.4855639636516571, 0.5058982372283936
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9602221846580505 0.46280258893966675, 0.4974195659160614
Loss in 100 steps: 0.9716055393218994 0.4754602015018463, 0.4961453378200531
Loss in 200 steps: 0.9800412058830261 0.4780546724796295, 0.501986563205719
Loss in 300 steps: 0.966277003288269 0.4683728516101837, 0.4979041516780853
Loss in 400 steps: 0.9555507898330688 0.4630119502544403, 0.4925388693809509
Loss in 500 steps: 0.9637072086334229 0.46949079632759094, 0.49421635270118713
Loss in 600 steps: 0.9851603507995605 0.4788203835487366, 0.5063399076461792
Loss in 700 steps: 0.9892175793647766 0.48298925161361694, 0.5062283277511597
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9619535803794861 0.4667380452156067, 0.4952155351638794
Loss in 100 steps: 0.9777159690856934 0.47958388924598694, 0.4981321692466736
Loss in 200 steps: 0.9730513691902161 0.47151637077331543, 0.5015349984169006
Loss in 300 steps: 0.9714962840080261 0.4739120900630951, 0.4975842535495758
Loss in 400 steps: 0.9508007168769836 0.4594504237174988, 0.49135029315948486
Loss in 500 steps: 0.9872756004333496 0.48491349816322327, 0.5023620128631592
Loss in 600 steps: 0.970923900604248 0.46591877937316895, 0.5050050020217896
Loss in 700 steps: 0.9955666065216064 0.4881652295589447, 0.5074013471603394
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9659441709518433 0.4693411886692047, 0.49660301208496094
Loss in 100 steps: 0.9674385190010071 0.4702329933643341, 0.49720558524131775
Loss in 200 steps: 0.9627618193626404 0.4611561596393585, 0.5016056299209595
Loss in 300 steps: 0.9736104011535645 0.4756239056587219, 0.49798664450645447
Loss in 400 steps: 0.9437363743782043 0.45208799839019775, 0.4916483759880066
Loss in 500 steps: 0.9717313051223755 0.4766288995742798, 0.49510231614112854
Loss in 600 steps: 0.9774802327156067 0.4719029366970062, 0.5055772662162781
Loss in 700 steps: 0.9854289293289185 0.4793310761451721, 0.5060978531837463
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9732111692428589 0.47739478945732117, 0.4958164095878601
Loss in 100 steps: 0.9694154858589172 0.47273439168930054, 0.4966811239719391
Loss in 200 steps: 0.9762985110282898 0.4751339852809906, 0.5011645555496216
Loss in 300 steps: 0.9723536372184753 0.4736701548099518, 0.49868354201316833
Loss in 400 steps: 0.9695714712142944 0.4744090735912323, 0.4951624274253845
Loss in 500 steps: 0.9771922826766968 0.48147520422935486, 0.4957170784473419
Loss in 600 steps: 0.9876744151115417 0.4810389578342438, 0.5066354870796204
Loss in 700 steps: 0.9861325621604919 0.479537695646286, 0.5065948367118835
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.965687096118927 0.4695659577846527, 0.4961211681365967
Loss in 100 steps: 0.9765414595603943 0.47819605469703674, 0.49834543466567993
Loss in 200 steps: 0.9775598049163818 0.476498544216156, 0.501061201095581
Loss in 300 steps: 0.9664027094841003 0.46719837188720703, 0.4992043077945709
Loss in 400 steps: 0.9472784996032715 0.45758408308029175, 0.48969441652297974
Loss in 500 steps: 0.9657996296882629 0.4712198078632355, 0.49457985162734985
Loss in 600 steps: 0.9739254117012024 0.46860408782958984, 0.5053213238716125
Loss in 700 steps: 0.978919267654419 0.47217172384262085, 0.5067475438117981
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9732947945594788 0.4760471284389496, 0.49724769592285156
Loss in 100 steps: 0.967413604259491 0.47015103697776794, 0.4972625970840454
Loss in 200 steps: 0.9789658784866333 0.475458025932312, 0.5035078525543213
Loss in 300 steps: 0.9692744612693787 0.4716693162918091, 0.4976051449775696
Loss in 400 steps: 0.9511598944664001 0.4591342806816101, 0.4920256435871124
Loss in 500 steps: 0.9653379917144775 0.4699002802371979, 0.4954376816749573
Loss in 600 steps: 0.9768630266189575 0.47184890508651733, 0.505014181137085
Loss in 700 steps: 0.9843141436576843 0.4785204827785492, 0.505793571472168
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9681708216667175 0.47219154238700867, 0.49597927927970886
Loss in 100 steps: 0.9762289524078369 0.4785049557685852, 0.4977239668369293
Loss in 200 steps: 0.973368763923645 0.471426784992218, 0.5019418597221375
Loss in 300 steps: 0.9701527953147888 0.4710552394390106, 0.4990975260734558
Loss in 400 steps: 0.9526567459106445 0.46090537309646606, 0.49175137281417847
Loss in 500 steps: 0.9831197261810303 0.4823256731033325, 0.5007940530776978
Loss in 600 steps: 0.9712932705879211 0.46638864278793335, 0.5049046277999878
Loss in 700 steps: 0.981758177280426 0.476027250289917, 0.5057309865951538
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9686518907546997 0.4707532227039337, 0.4978986978530884
Loss in 100 steps: 0.9726529121398926 0.4747239947319031, 0.4979289770126343
Loss in 200 steps: 0.9720661640167236 0.4703264832496643, 0.5017396211624146
Loss in 300 steps: 0.976055383682251 0.47831225395202637, 0.4977430999279022
Loss in 400 steps: 0.9473150968551636 0.4546581208705902, 0.49265703558921814
Loss in 500 steps: 0.9558520317077637 0.4611470401287079, 0.4947050213813782
Loss in 600 steps: 0.9874529242515564 0.47990936040878296, 0.5075435042381287
Loss in 700 steps: 0.9738492369651794 0.46814465522766113, 0.5057046413421631
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9732908606529236 0.47586578130722046, 0.4974251091480255
Loss in 100 steps: 0.9776715040206909 0.4801306128501892, 0.49754083156585693
Loss in 200 steps: 0.966628909111023 0.4649541974067688, 0.5016745924949646
Loss in 300 steps: 0.9730520844459534 0.47475603222846985, 0.4982960522174835
Loss in 400 steps: 0.9421966671943665 0.45250147581100464, 0.4896951913833618
Loss in 500 steps: 0.9710734486579895 0.4719716012477875, 0.49910181760787964
Loss in 600 steps: 0.9854791760444641 0.47912323474884033, 0.5063559412956238
Loss in 700 steps: 0.9779748916625977 0.4725456237792969, 0.5054292678833008
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9751696586608887 0.47719067335128784, 0.49797895550727844
Loss in 100 steps: 0.964494526386261 0.4677746295928955, 0.4967198669910431
Loss in 200 steps: 0.9704748392105103 0.4697422683238983, 0.5007324814796448
Loss in 300 steps: 0.9715906977653503 0.47228366136550903, 0.4993070662021637
Loss in 400 steps: 0.9372034072875977 0.44877925515174866, 0.488424152135849
Loss in 500 steps: 0.9741477966308594 0.4802086353302002, 0.49393919110298157
Loss in 600 steps: 0.9895007014274597 0.4835183322429657, 0.5059822797775269
Loss in 700 steps: 0.9768853783607483 0.4706178605556488, 0.5062674880027771
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9747985005378723 0.4785514175891876, 0.4962470531463623
Loss in 100 steps: 0.9847533702850342 0.4864442050457001, 0.49830925464630127
Loss in 200 steps: 0.970918595790863 0.46848443150520325, 0.5024341344833374
Loss in 300 steps: 0.9765258431434631 0.4782232940196991, 0.4983024597167969
Loss in 400 steps: 0.9508941173553467 0.46107518672943115, 0.4898189306259155
Loss in 500 steps: 0.9550705552101135 0.46339744329452515, 0.4916730225086212
Loss in 600 steps: 0.9753445386886597 0.4695082902908325, 0.5058362483978271
Loss in 700 steps: 0.9857675433158875 0.47926944494247437, 0.5064979791641235
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9726114273071289 0.47564011812210083, 0.4969713091850281
Loss in 100 steps: 0.9842519760131836 0.4861311614513397, 0.49812084436416626
Loss in 200 steps: 0.9749361276626587 0.4730110466480255, 0.5019250512123108
Loss in 300 steps: 0.9681171774864197 0.47087323665618896, 0.4972439408302307
Loss in 400 steps: 0.9535036683082581 0.46240028738975525, 0.4911033809185028
Loss in 500 steps: 0.9836897253990173 0.4829522371292114, 0.5007374882698059
Loss in 600 steps: 0.9633344411849976 0.45775800943374634, 0.5055764317512512
Loss in 700 steps: 0.9759652614593506 0.4704301059246063, 0.5055350661277771
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9645201563835144 0.4689551293849945, 0.4955650269985199
Loss in 100 steps: 0.9880387187004089 0.4890310764312744, 0.49900758266448975
Loss in 200 steps: 0.9690629839897156 0.4666973352432251, 0.5023656487464905
Loss in 300 steps: 0.9729040265083313 0.4749584496021271, 0.4979455769062042
Loss in 400 steps: 0.9350126385688782 0.4459986090660095, 0.48901399970054626
Loss in 500 steps: 0.9611724615097046 0.46391376852989197, 0.49725860357284546
Loss in 600 steps: 0.9778870344161987 0.47041091322898865, 0.5074760913848877
Loss in 700 steps: 0.9727116227149963 0.46653831005096436, 0.5061732530593872
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9720205068588257 0.47516492009162903, 0.49685555696487427
Loss in 100 steps: 0.9777608513832092 0.47932058572769165, 0.4984402656555176
Loss in 200 steps: 0.9641352891921997 0.4616394340991974, 0.5024958848953247
Loss in 300 steps: 0.9573361277580261 0.4599655568599701, 0.4973706007003784
Loss in 400 steps: 0.955023467540741 0.4643772840499878, 0.49064621329307556
Loss in 500 steps: 0.9739145636558533 0.4768107831478119, 0.49710386991500854
Loss in 600 steps: 0.9643632173538208 0.4580315053462982, 0.506331741809845
Loss in 700 steps: 0.9796729683876038 0.4735032021999359, 0.5061696767807007
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9643925428390503 0.4679904282093048, 0.49640214443206787
Loss in 100 steps: 0.9685285091400146 0.4710984230041504, 0.49743011593818665
Loss in 200 steps: 0.9803906083106995 0.4783746302127838, 0.5020158886909485
Loss in 300 steps: 0.9606468081474304 0.46367526054382324, 0.49697157740592957
Loss in 400 steps: 0.9498456716537476 0.46107369661331177, 0.4887719750404358
Loss in 500 steps: 0.9581934213638306 0.466587632894516, 0.4916057586669922
Loss in 600 steps: 0.9888545870780945 0.4808782935142517, 0.5079762935638428
Loss in 700 steps: 0.9884167909622192 0.4810427725315094, 0.5073741674423218
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9665135145187378 0.47196197509765625, 0.49455147981643677
Loss in 100 steps: 0.9795055389404297 0.4810944199562073, 0.49841105937957764
Loss in 200 steps: 0.9823461771011353 0.4803248345851898, 0.502021312713623
Loss in 300 steps: 0.9736881852149963 0.47620901465415955, 0.497479110956192
Loss in 400 steps: 0.9541017413139343 0.4624951183795929, 0.4916066527366638
Loss in 500 steps: 0.9622328281402588 0.46625807881355286, 0.4959748685359955
Loss in 600 steps: 0.9969372749328613 0.48803725838661194, 0.5089000463485718
Loss in 700 steps: 0.9804339408874512 0.47489622235298157, 0.5055376887321472
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9687484502792358 0.4716107249259949, 0.49713772535324097
Loss in 100 steps: 0.9838259220123291 0.4852026700973511, 0.49862319231033325
Loss in 200 steps: 0.9877075552940369 0.48537781834602356, 0.5023297667503357
Loss in 300 steps: 0.9614981412887573 0.4653458893299103, 0.49615225195884705
Loss in 400 steps: 0.9505736827850342 0.45776864886283875, 0.4928050935268402
Loss in 500 steps: 0.9388570189476013 0.45081833004951477, 0.48803865909576416
Loss in 600 steps: 0.9912856221199036 0.4842170178890228, 0.5070686936378479
Loss in 700 steps: 0.9916948676109314 0.48512500524520874, 0.5065698623657227
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=100, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output100', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9589563012123108 0.46347782015800476, 0.49547842144966125
Loss in 100 steps: 0.9815137386322021 0.4816431403160095, 0.499870628118515
Loss in 200 steps: 0.9644548892974854 0.4622974991798401, 0.5021573901176453
Loss in 300 steps: 0.9634244441986084 0.4659411311149597, 0.4974832832813263
Loss in 400 steps: 0.954556405544281 0.4644719660282135, 0.4900844097137451
Loss in 500 steps: 0.9750853180885315 0.4762597978115082, 0.4988255202770233
Loss in 600 steps: 0.9899384379386902 0.48299872875213623, 0.5069396495819092
Loss in 700 steps: 0.9903621077537537 0.48435813188552856, 0.5060038566589355
