Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=5, log_step=100, lr=0.0025, min_count=25, output='coca.txt.raw.token.train-output', text='coca.txt.raw.token.train', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=1e-11, window_size=5, years=30)
Loss in 0 steps: 1.3862943649291992 0.6931471824645996, 0.6931471824645996
Loss in 100 steps: 1.1165024042129517 0.5004011392593384, 0.6161013245582581
Loss in 200 steps: 1.0821088552474976 0.5342904925346375, 0.5478184223175049
Loss in 300 steps: 1.0479114055633545 0.5108311176300049, 0.5370804071426392
Loss in 400 steps: 1.0620051622390747 0.5277355909347534, 0.5342696905136108
Loss in 500 steps: 1.0233937501907349 0.4955589175224304, 0.5278348922729492
Loss in 600 steps: 1.0107964277267456 0.487214595079422, 0.5235818028450012
Loss in 700 steps: 1.0376149415969849 0.5128480792045593, 0.5247669219970703
Loss in 800 steps: 1.003687858581543 0.4841668903827667, 0.5195208787918091
Loss in 900 steps: 1.0095984935760498 0.49166813492774963, 0.5179303288459778
Loss in 1000 steps: 0.9809398055076599 0.4695659577846527, 0.51137375831604
Loss in 1100 steps: 0.9998435974121094 0.4863506853580475, 0.5134929418563843
Loss in 1200 steps: 0.9956965446472168 0.4850762188434601, 0.5106202960014343
Loss in 1300 steps: 0.9748868942260742 0.46656620502471924, 0.5083206295967102
Loss in 1400 steps: 0.9703491926193237 0.4626304507255554, 0.5077188611030579
Loss in 1500 steps: 0.9925013184547424 0.48732683062553406, 0.505174458026886
Loss in 1600 steps: 0.9847656488418579 0.4780639410018921, 0.5067017674446106
Loss in 1700 steps: 0.9836233258247375 0.47880157828330994, 0.50482177734375
Loss in 1800 steps: 0.9963476657867432 0.4919278621673584, 0.5044198036193848
Loss in 1900 steps: 0.9490658044815063 0.44978323578834534, 0.499282568693161
Loss in 2000 steps: 0.992390513420105 0.4905678629875183, 0.5018226504325867
Loss in 2100 steps: 0.9731459021568298 0.47367119789123535, 0.4994747042655945
Loss in 2200 steps: 0.9825015664100647 0.4820170998573303, 0.5004844665527344
Loss in  test0: 0.9599853000157184 
Loss in  test1: 0.9605221419830938 
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=5, log_step=100, lr=0.0025, min_count=25, output='coca.txt.raw.token.train-output', text='coca.txt.raw.token.train', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=1e-11, window_size=5, years=30)
Loss in 0 steps: 0.9478170275688171 0.4516560733318329, 0.49616101384162903
Loss in 100 steps: 0.9519433975219727 0.45468807220458984, 0.49725544452667236
Loss in 200 steps: 0.9604750871658325 0.4654960036277771, 0.4949791729450226
Loss in 300 steps: 0.948520302772522 0.4555320143699646, 0.49298834800720215
Loss in 400 steps: 0.9679887294769287 0.4735068380832672, 0.4944819211959839
Loss in 500 steps: 0.9419281482696533 0.4493449926376343, 0.49258315563201904
Loss in 600 steps: 0.9445198774337769 0.45104339718818665, 0.493476539850235
Loss in 700 steps: 0.9639185070991516 0.47031527757644653, 0.4936032295227051
Loss in 800 steps: 0.9434256553649902 0.4535861015319824, 0.4898395836353302
Loss in 900 steps: 0.9520941972732544 0.45994460582733154, 0.4921496510505676
Loss in 1000 steps: 0.9439881443977356 0.4529033303260803, 0.4910848140716553
Loss in 1100 steps: 0.9522797465324402 0.4600786864757538, 0.49220114946365356
Loss in 1200 steps: 0.9533590078353882 0.462798535823822, 0.4905605614185333
Loss in 1300 steps: 0.9479501843452454 0.4578707218170166, 0.49007949233055115
Loss in 1400 steps: 0.943250298500061 0.4507504105567932, 0.4924998879432678
Loss in 1500 steps: 0.957016110420227 0.46637284755706787, 0.49064329266548157
Loss in 1600 steps: 0.955306887626648 0.4660782217979431, 0.48922866582870483
Loss in 1700 steps: 0.9715347290039062 0.47971367835998535, 0.49182096123695374
Loss in 1800 steps: 0.971921443939209 0.48085978627204895, 0.49106156826019287
Loss in 1900 steps: 0.9213816523551941 0.43543002009391785, 0.485951691865921
Loss in 2000 steps: 0.9560784697532654 0.4653593599796295, 0.49071916937828064
Loss in 2100 steps: 0.9460233449935913 0.45821893215179443, 0.48780444264411926
Loss in 2200 steps: 0.9666029214859009 0.47573140263557434, 0.49087151885032654
Loss in  test0: 0.9434430670374385 
Loss in  test1: 0.9443614343231413 
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=5, log_step=100, lr=0.0025, min_count=25, output='coca.txt.raw.token.train-output', text='coca.txt.raw.token.train', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=1e-11, window_size=5, years=30)
Loss in 0 steps: 0.928749680519104 0.4426031708717346, 0.4861465096473694
Loss in 100 steps: 0.9292842149734497 0.4425358474254608, 0.4867483377456665
Loss in 200 steps: 0.9421358108520508 0.4553834795951843, 0.48675230145454407
Loss in 300 steps: 0.9381510615348816 0.45140746235847473, 0.4867437183856964
Loss in 400 steps: 0.9600499272346497 0.4739407002925873, 0.48610919713974
Loss in 500 steps: 0.936336100101471 0.450002521276474, 0.48633354902267456
Loss in 600 steps: 0.9271832704544067 0.44004276394844055, 0.4871405065059662
Loss in 700 steps: 0.9626818299293518 0.4741295278072357, 0.48855236172676086
Loss in 800 steps: 0.9367934465408325 0.4504724442958832, 0.4863210916519165
Loss in 900 steps: 0.9388353824615479 0.45477911829948425, 0.4840562641620636
Loss in 1000 steps: 0.9349138140678406 0.4498761296272278, 0.4850376546382904
Loss in 1100 steps: 0.9449617862701416 0.45946699380874634, 0.48549479246139526
Loss in 1200 steps: 0.9304128885269165 0.44508326053619385, 0.4853295683860779
Loss in 1300 steps: 0.9373075366020203 0.45300477743148804, 0.48430272936820984
Loss in 1400 steps: 0.9207937717437744 0.4367865324020386, 0.4840072691440582
Loss in 1500 steps: 0.9470000863075256 0.46257761120796204, 0.4844225347042084
Loss in 1600 steps: 0.9435129761695862 0.4604187309741974, 0.4830942153930664
Loss in 1700 steps: 0.954884946346283 0.4680164158344269, 0.4868684411048889
Loss in 1800 steps: 0.94757080078125 0.462603896856308, 0.484966903924942
Loss in 1900 steps: 0.916405439376831 0.4350006580352783, 0.4814048111438751
Loss in 2000 steps: 0.9570011496543884 0.47068384289741516, 0.48631730675697327
Loss in 2100 steps: 0.9460471868515015 0.4611922800540924, 0.48485496640205383
Loss in 2200 steps: 0.9311789274215698 0.44722771644592285, 0.483951210975647
Loss in  test0: 0.9382816511114367 
Loss in  test1: 0.9388499261821408 
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=5, log_step=100, lr=0.0025, min_count=25, output='coca.txt.raw.token.train-output', text='coca.txt.raw.token.train', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=1e-11, window_size=5, years=30)
Loss in 0 steps: 0.9171359539031982 0.43436095118522644, 0.48277491331100464
Loss in 100 steps: 0.9182794690132141 0.4355544149875641, 0.48272505402565
Loss in 200 steps: 0.9156854152679443 0.4344848692417145, 0.48120051622390747
Loss in 300 steps: 0.9263229966163635 0.4435292184352875, 0.48279380798339844
Loss in 400 steps: 0.9562806487083435 0.47291356325149536, 0.48336708545684814
Loss in 500 steps: 0.9251844882965088 0.44306856393814087, 0.4821159541606903
Loss in 600 steps: 0.9179614186286926 0.43603962659835815, 0.4819217622280121
Loss in 700 steps: 0.9455226063728333 0.46094512939453125, 0.4845775067806244
Loss in 800 steps: 0.9183267951011658 0.4376107156276703, 0.48071610927581787
Loss in 900 steps: 0.9374889135360718 0.4558291733264923, 0.4816597104072571
Loss in 1000 steps: 0.9276984930038452 0.44521987438201904, 0.48247864842414856
Loss in 1100 steps: 0.9363986253738403 0.45220711827278137, 0.4841914772987366
Loss in 1200 steps: 0.9268834590911865 0.4448678195476532, 0.4820156395435333
Loss in 1300 steps: 0.9367592930793762 0.4543537497520447, 0.4824056029319763
Loss in 1400 steps: 0.9301964044570923 0.44595247507095337, 0.4842439293861389
Loss in 1500 steps: 0.9362919330596924 0.45603230595588684, 0.48025962710380554
Loss in 1600 steps: 0.9546861052513123 0.472402960062027, 0.48228323459625244
Loss in 1700 steps: 0.9509727358818054 0.4668200612068176, 0.4841527044773102
Loss in 1800 steps: 0.9382080435752869 0.4554251730442047, 0.4827828109264374
Loss in 1900 steps: 0.9109354019165039 0.4314943850040436, 0.4794410169124603
Loss in 2000 steps: 0.948387086391449 0.4638254940509796, 0.4845614731311798
Loss in 2100 steps: 0.9313520193099976 0.45002609491348267, 0.4813258647918701
Loss in 2200 steps: 0.9369802474975586 0.4543234705924988, 0.4826568067073822
Loss in  test0: 0.9359205213494104 
Loss in  test1: 0.9368299184303523 
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=5, log_step=100, lr=0.0025, min_count=25, output='coca.txt.raw.token.train-output', text='coca.txt.raw.token.train', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=1e-11, window_size=5, years=30)
Loss in 0 steps: 0.9142934083938599 0.43296098709106445, 0.4813324213027954
Loss in 100 steps: 0.9191449880599976 0.4385474920272827, 0.48059749603271484
Loss in 200 steps: 0.9243288040161133 0.4430762231349945, 0.4812525510787964
Loss in 300 steps: 0.9312906265258789 0.447774738073349, 0.4835158884525299
Loss in 400 steps: 0.94912189245224 0.4673323929309845, 0.4817894697189331
Loss in 500 steps: 0.9374338388442993 0.4540829360485077, 0.48335081338882446
Loss in 600 steps: 0.9069057106971741 0.4272855520248413, 0.479620099067688
Loss in 700 steps: 0.9532971382141113 0.4694751501083374, 0.48382195830345154
Loss in 800 steps: 0.9259500503540039 0.4458332657814026, 0.48011675477027893
Loss in 900 steps: 0.9374338388442993 0.45693403482437134, 0.48049983382225037
Loss in 1000 steps: 0.9248136281967163 0.442989319562912, 0.48182424902915955
Loss in 1100 steps: 0.9482377767562866 0.4645233154296875, 0.4837144613265991
Loss in 1200 steps: 0.9252145290374756 0.44403722882270813, 0.48117735981941223
Loss in 1300 steps: 0.9274625778198242 0.4474300444126129, 0.4800325930118561
Loss in 1400 steps: 0.9138967990875244 0.4326033890247345, 0.48129338026046753
Loss in 1500 steps: 0.9383302330970764 0.4585522711277008, 0.47977790236473083
Loss in 1600 steps: 0.9394237995147705 0.4604490101337433, 0.4789747893810272
Loss in 1700 steps: 0.9425187110900879 0.4605681002140045, 0.48195067048072815
Loss in 1800 steps: 0.945343554019928 0.4620269536972046, 0.4833166003227234
Loss in 1900 steps: 0.9042342305183411 0.4280235767364502, 0.47621068358421326
Loss in 2000 steps: 0.9506475329399109 0.4674951434135437, 0.4831523895263672
Loss in 2100 steps: 0.9292097687721252 0.44777917861938477, 0.48143070936203003
Loss in 2200 steps: 0.9352138638496399 0.45243555307388306, 0.4827783405780792
Loss in  test0: 0.9355833866972898 
Loss in  test1: 0.9361599968943605 
