Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=5, log_step=100, lr=0.0025, min_count=25, output='nyt_yao_tiny.txt.norm.train-output', text='nyt_yao_tiny.txt.norm.train', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=1e-11, window_size=5, years=30)
Loss in 0 steps: 1.3862943649291992 0.6931471824645996, 0.6931471824645996
Loss in 100 steps: 1.2057220935821533 0.5524927377700806, 0.6532293558120728
Loss in 200 steps: 1.1676496267318726 0.5729698538780212, 0.5946798324584961
Loss in 300 steps: 1.1386996507644653 0.5537063479423523, 0.5849934220314026
Loss in 400 steps: 1.1349012851715088 0.5583983063697815, 0.5765030384063721
Loss in 500 steps: 1.1309045553207397 0.557979166507721, 0.572925329208374
Loss in 600 steps: 1.0989432334899902 0.533100962638855, 0.56584233045578
Loss in  test0: 1.104934336759993 
Loss in  test1: 1.102062027024202 
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=5, log_step=100, lr=0.0025, min_count=25, output='nyt_yao_tiny.txt.norm.train-output', text='nyt_yao_tiny.txt.norm.train', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=1e-11, window_size=5, years=30)
Loss in 0 steps: 1.1130812168121338 0.5496594309806824, 0.5634216666221619
Loss in 100 steps: 1.0867019891738892 0.5282627940177917, 0.5584391355514526
Loss in 200 steps: 1.089677095413208 0.5341424942016602, 0.5555346012115479
Loss in 300 steps: 1.0698257684707642 0.5188733339309692, 0.5509524941444397
Loss in 400 steps: 1.082651138305664 0.5331180691719055, 0.5495331287384033
Loss in 500 steps: 1.0858303308486938 0.53667151927948, 0.5491587519645691
Loss in 600 steps: 1.054309368133545 0.5105444192886353, 0.5437650084495544
Loss in  test0: 1.075270850177205 
Loss in  test1: 1.0720359052357689 
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=5, log_step=100, lr=0.0025, min_count=25, output='nyt_yao_tiny.txt.norm.train-output', text='nyt_yao_tiny.txt.norm.train', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=1e-11, window_size=5, years=30)
Loss in 0 steps: 1.0758507251739502 0.5302113890647888, 0.5456393361091614
Loss in 100 steps: 1.0468995571136475 0.5057579278945923, 0.5411416292190552
Loss in 200 steps: 1.061769723892212 0.5207355618476868, 0.5410340428352356
Loss in 300 steps: 1.0428507328033447 0.5037649869918823, 0.5390858054161072
Loss in 400 steps: 1.0658259391784668 0.5243686437606812, 0.5414573550224304
Loss in 500 steps: 1.0741348266601562 0.5345641374588013, 0.539570689201355
Loss in 600 steps: 1.0495831966400146 0.5128470063209534, 0.5367361307144165
Loss in  test0: 1.0641030353872782 
Loss in  test1: 1.0599259240182533 
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=5, log_step=100, lr=0.0025, min_count=25, output='nyt_yao_tiny.txt.norm.train-output', text='nyt_yao_tiny.txt.norm.train', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=1e-11, window_size=5, years=30)
Loss in 0 steps: 1.0731784105300903 0.5330619812011719, 0.5401164293289185
Loss in 100 steps: 1.0302366018295288 0.49446776509284973, 0.5357688069343567
Loss in 200 steps: 1.053338646888733 0.5178645253181458, 0.5354741215705872
Loss in 300 steps: 1.0333365201950073 0.49929773807525635, 0.5340387225151062
Loss in 400 steps: 1.0587931871414185 0.5215973258018494, 0.5371958613395691
Loss in 500 steps: 1.0557447671890259 0.5205785036087036, 0.535166323184967
Loss in 600 steps: 1.0363446474075317 0.5032385587692261, 0.5331061482429504
Loss in  test0: 1.0599328478177388 
Loss in  test1: 1.0562360228383942 
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=5, log_step=100, lr=0.0025, min_count=25, output='nyt_yao_tiny.txt.norm.train-output', text='nyt_yao_tiny.txt.norm.train', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=1e-11, window_size=5, years=30)
Loss in 0 steps: 1.05251145362854 0.5162612199783325, 0.5362502336502075
Loss in 100 steps: 1.031303882598877 0.498736172914505, 0.5325677990913391
Loss in 200 steps: 1.0416178703308105 0.5088566541671753, 0.5327613353729248
Loss in 300 steps: 1.0281319618225098 0.495195597410202, 0.5329363346099854
Loss in 400 steps: 1.0392121076583862 0.5035862922668457, 0.5356257557868958
Loss in 500 steps: 1.0557670593261719 0.5218543410301208, 0.5339127779006958
Loss in 600 steps: 1.0399339199066162 0.5079438090324402, 0.5319902300834656
Loss in  test0: 1.0596616769420262 
Loss in  test1: 1.0559202820882885 
