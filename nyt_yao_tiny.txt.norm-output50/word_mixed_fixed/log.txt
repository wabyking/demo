Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.3862944841384888 0.6931472420692444, 0.6931472420692444
Loss in 100 steps: 1.2520455121994019 0.5662316679954529, 0.6858139038085938
Loss in 200 steps: 1.200793743133545 0.5662098526954651, 0.6345839500427246
Loss in 300 steps: 1.1405717134475708 0.5410948395729065, 0.5994769334793091
Loss in 400 steps: 1.1390297412872314 0.5539337396621704, 0.5850958824157715
Loss in 500 steps: 1.1577032804489136 0.5675187110900879, 0.5901846289634705
Loss in 600 steps: 1.135465145111084 0.5499609708786011, 0.5855041146278381
Loss in 700 steps: 1.120490550994873 0.5458915829658508, 0.574599027633667
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.1434036493301392 0.5673967599868774, 0.5760069489479065
Loss in 100 steps: 1.0963555574417114 0.5268459320068359, 0.5695096254348755
Loss in 200 steps: 1.1208012104034424 0.5485007762908936, 0.5723004937171936
Loss in 300 steps: 1.0823142528533936 0.5204232335090637, 0.5618910789489746
Loss in 400 steps: 1.0867778062820435 0.532038688659668, 0.5547391176223755
Loss in 500 steps: 1.1147594451904297 0.5504311323165894, 0.5643283128738403
Loss in 600 steps: 1.0883126258850098 0.5254268646240234, 0.5628857016563416
Loss in 700 steps: 1.0951415300369263 0.5380515456199646, 0.5570901036262512
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0872783660888672 0.5372023582458496, 0.5500759482383728
Loss in 100 steps: 1.06247878074646 0.513856828212738, 0.5486219525337219
Loss in 200 steps: 1.0895285606384277 0.5258000493049622, 0.5637285709381104
Loss in 300 steps: 1.0628732442855835 0.5154569149017334, 0.5474162101745605
Loss in 400 steps: 1.0603100061416626 0.5211639404296875, 0.5391461253166199
Loss in 500 steps: 1.0935639142990112 0.5410118103027344, 0.5525521636009216
Loss in 600 steps: 1.069983720779419 0.5163044929504395, 0.5536792874336243
Loss in 700 steps: 1.0696731805801392 0.522679328918457, 0.5469939708709717
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0596576929092407 0.5279345512390137, 0.5317232012748718
Loss in 100 steps: 1.0507630109786987 0.51236891746521, 0.5383940935134888
Loss in 200 steps: 1.0746122598648071 0.517831563949585, 0.5567806959152222
Loss in 300 steps: 1.0409579277038574 0.5015182495117188, 0.5394396185874939
Loss in 400 steps: 1.0527961254119873 0.5218220949172974, 0.5309740304946899
Loss in 500 steps: 1.0734299421310425 0.5301616787910461, 0.5432682633399963
Loss in 600 steps: 1.0493698120117188 0.5037223696708679, 0.5456475019454956
Loss in 700 steps: 1.0598721504211426 0.5191213488578796, 0.5407507419586182
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0521211624145508 0.5278146266937256, 0.5243065357208252
Loss in 100 steps: 1.0330981016159058 0.5028776526451111, 0.5302204489707947
Loss in 200 steps: 1.052783727645874 0.5038598775863647, 0.5489240288734436
Loss in 300 steps: 1.0284464359283447 0.4962822198867798, 0.5321642160415649
Loss in 400 steps: 1.034329891204834 0.5096608400344849, 0.5246690511703491
Loss in 500 steps: 1.0528967380523682 0.5176268219947815, 0.5352699160575867
Loss in 600 steps: 1.0415147542953491 0.49970370531082153, 0.5418109893798828
Loss in 700 steps: 1.0405404567718506 0.5040909051895142, 0.5364495515823364
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0351929664611816 0.5187793374061584, 0.5164136290550232
Loss in 100 steps: 1.027550220489502 0.501535952091217, 0.5260143280029297
Loss in 200 steps: 1.0520352125167847 0.5075043439865112, 0.5445309281349182
Loss in 300 steps: 1.0249096155166626 0.4952636659145355, 0.5296459197998047
Loss in 400 steps: 1.0264605283737183 0.505801796913147, 0.5206587910652161
Loss in 500 steps: 1.0386593341827393 0.5078009963035583, 0.5308583974838257
Loss in 600 steps: 1.029758095741272 0.4929560422897339, 0.5368021130561829
Loss in 700 steps: 1.0385382175445557 0.5057151317596436, 0.5328230261802673
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.03025221824646 0.5162013173103333, 0.5140509009361267
Loss in 100 steps: 1.012425422668457 0.49175548553466797, 0.5206700563430786
Loss in 200 steps: 1.0395268201828003 0.4977756440639496, 0.5417512059211731
Loss in 300 steps: 1.022278904914856 0.4948323965072632, 0.527446448802948
Loss in 400 steps: 1.0232266187667847 0.5040541291236877, 0.5191724896430969
Loss in 500 steps: 1.0420198440551758 0.5146825909614563, 0.5273372530937195
Loss in 600 steps: 1.0366793870925903 0.5029128193855286, 0.5337665677070618
Loss in 700 steps: 1.0229651927947998 0.4935551583766937, 0.5294100642204285
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0128929615020752 0.5028166770935059, 0.5100762844085693
Loss in 100 steps: 1.0067692995071411 0.4880822002887726, 0.5186871886253357
Loss in 200 steps: 1.0362459421157837 0.49921315908432007, 0.5370327234268188
Loss in 300 steps: 1.0064371824264526 0.48301565647125244, 0.523421585559845
Loss in 400 steps: 1.0103563070297241 0.4962672293186188, 0.5140891075134277
Loss in 500 steps: 1.0537134408950806 0.5265290141105652, 0.5271844267845154
Loss in 600 steps: 1.0097700357437134 0.48054268956184387, 0.5292273163795471
Loss in 700 steps: 1.0286026000976562 0.4994100332260132, 0.5291925072669983
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0209815502166748 0.5116302371025085, 0.5093513131141663
Loss in 100 steps: 1.0050950050354004 0.4896846413612366, 0.5154104232788086
Loss in 200 steps: 1.0287208557128906 0.49600231647491455, 0.5327184200286865
Loss in 300 steps: 1.013651728630066 0.4906584918498993, 0.5229932069778442
Loss in 400 steps: 1.0084733963012695 0.49535125494003296, 0.5131221413612366
Loss in 500 steps: 1.0517947673797607 0.5263158082962036, 0.5254789590835571
Loss in 600 steps: 1.0174509286880493 0.4885312020778656, 0.5289198160171509
Loss in 700 steps: 1.0281381607055664 0.5009467005729675, 0.5271913409233093
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0016802549362183 0.49519166350364685, 0.5064886808395386
Loss in 100 steps: 1.0036444664001465 0.4880964756011963, 0.5155479311943054
Loss in 200 steps: 1.0194745063781738 0.49083074927330017, 0.528643786907196
Loss in 300 steps: 1.0121937990188599 0.49027180671691895, 0.5219219326972961
Loss in 400 steps: 1.0080169439315796 0.4943750500679016, 0.513641893863678
Loss in 500 steps: 1.0427641868591309 0.5207589268684387, 0.5220053195953369
Loss in 600 steps: 1.0296519994735718 0.5026540160179138, 0.5269978642463684
Loss in 700 steps: 1.0275229215621948 0.5012391805648804, 0.5262837409973145
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0017226934432983 0.49541735649108887, 0.5063052773475647
Loss in 100 steps: 0.9948128461837769 0.4820590019226074, 0.5127538442611694
Loss in 200 steps: 1.023820400238037 0.4962538778781891, 0.5275664925575256
Loss in 300 steps: 1.0026569366455078 0.4823018014431, 0.5203551650047302
Loss in 400 steps: 1.0043920278549194 0.4935820698738098, 0.5108100771903992
Loss in 500 steps: 1.0444400310516357 0.5244132280349731, 0.5200268626213074
Loss in 600 steps: 1.0179356336593628 0.49141937494277954, 0.526516318321228
Loss in 700 steps: 1.016242504119873 0.49164050817489624, 0.5246019959449768
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9946722388267517 0.49156323075294495, 0.5031090378761292
Loss in 100 steps: 0.9920023083686829 0.4802708029747009, 0.5117315053939819
Loss in 200 steps: 1.0175575017929077 0.49314042925834656, 0.5244171619415283
Loss in 300 steps: 0.9995604157447815 0.48161864280700684, 0.5179417729377747
Loss in 400 steps: 1.0018694400787354 0.4916045069694519, 0.5102649331092834
Loss in 500 steps: 1.0350468158721924 0.5164511799812317, 0.5185956358909607
Loss in 600 steps: 1.009191870689392 0.48528578877449036, 0.5239059925079346
Loss in 700 steps: 1.0114433765411377 0.4888327121734619, 0.5226106643676758
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0099343061447144 0.5024216175079346, 0.5075127482414246
Loss in 100 steps: 1.0034945011138916 0.49083182215690613, 0.5126625299453735
Loss in 200 steps: 1.030271291732788 0.5059636235237122, 0.5243077278137207
Loss in 300 steps: 0.9908407926559448 0.4746168255805969, 0.5162239670753479
Loss in 400 steps: 0.995544970035553 0.48722967505455017, 0.5083152651786804
Loss in 500 steps: 1.0407248735427856 0.5224916934967041, 0.5182331204414368
Loss in 600 steps: 1.000827670097351 0.47954776883125305, 0.5212799310684204
Loss in 700 steps: 1.017966628074646 0.4954541027545929, 0.5225124359130859
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.999311625957489 0.49465447664260864, 0.5046571493148804
Loss in 100 steps: 0.9892026782035828 0.47819983959198, 0.5110028386116028
Loss in 200 steps: 1.019395351409912 0.4977821111679077, 0.5216132402420044
Loss in 300 steps: 0.9894800782203674 0.47421354055404663, 0.515266478061676
Loss in 400 steps: 0.9915443658828735 0.4839998781681061, 0.5075445175170898
Loss in 500 steps: 1.0278029441833496 0.5105429887771606, 0.517259955406189
Loss in 600 steps: 1.0034677982330322 0.4824804365634918, 0.520987331867218
Loss in 700 steps: 1.0088251829147339 0.4870125651359558, 0.5218127369880676
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0037521123886108 0.49999141693115234, 0.5037607550621033
Loss in 100 steps: 0.98444664478302 0.4750041663646698, 0.5094424486160278
Loss in 200 steps: 1.01382315158844 0.4946115016937256, 0.5192116498947144
Loss in 300 steps: 0.9872714877128601 0.4723075032234192, 0.5149639844894409
Loss in 400 steps: 0.9939972758293152 0.4871405363082886, 0.5068566203117371
Loss in 500 steps: 1.0250039100646973 0.5089526176452637, 0.5160512924194336
Loss in 600 steps: 1.012251853942871 0.49077048897743225, 0.5214812755584717
Loss in 700 steps: 1.0070873498916626 0.48610422015190125, 0.5209832191467285
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9959158897399902 0.4913271367549896, 0.5045887231826782
Loss in 100 steps: 0.9930179715156555 0.4836433231830597, 0.5093746185302734
Loss in 200 steps: 1.0057086944580078 0.48903465270996094, 0.5166739821434021
Loss in 300 steps: 0.9892714023590088 0.4768282175064087, 0.5124431252479553
Loss in 400 steps: 0.9887690544128418 0.48281872272491455, 0.505950391292572
Loss in 500 steps: 1.0199482440948486 0.5060993432998657, 0.5138489007949829
Loss in 600 steps: 0.9936678409576416 0.4742676615715027, 0.5194001197814941
Loss in 700 steps: 1.010422706604004 0.49072587490081787, 0.519696831703186
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9910944104194641 0.48711565136909485, 0.5039787888526917
Loss in 100 steps: 0.9876872301101685 0.4785270690917969, 0.5091601610183716
Loss in 200 steps: 1.0122711658477783 0.49619969725608826, 0.5160714387893677
Loss in 300 steps: 0.985386848449707 0.47278669476509094, 0.5126001238822937
Loss in 400 steps: 0.9895808100700378 0.4828486144542694, 0.506732165813446
Loss in 500 steps: 1.0315749645233154 0.5163158178329468, 0.5152591466903687
Loss in 600 steps: 0.9967499375343323 0.47887083888053894, 0.517879068851471
Loss in 700 steps: 1.004679560661316 0.48579925298690796, 0.518880307674408
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 1.0007050037384033 0.4970586895942688, 0.5036463737487793
Loss in 100 steps: 0.9872984886169434 0.47842684388160706, 0.5088715553283691
Loss in 200 steps: 1.0117946863174438 0.4965522289276123, 0.5152425169944763
Loss in 300 steps: 1.001640796661377 0.48670506477355957, 0.5149357914924622
Loss in 400 steps: 0.9875395894050598 0.4806872010231018, 0.5068523287773132
Loss in 500 steps: 1.030346155166626 0.516331672668457, 0.514014482498169
Loss in 600 steps: 0.9995948672294617 0.4819113314151764, 0.5176835656166077
Loss in 700 steps: 0.9981996417045593 0.48015880584716797, 0.5180408358573914
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9795423150062561 0.4794316291809082, 0.5001107454299927
Loss in 100 steps: 0.9903451204299927 0.4810885488986969, 0.5092566013336182
Loss in 200 steps: 1.0052663087844849 0.49155864119529724, 0.51370769739151
Loss in 300 steps: 0.9929701685905457 0.480990469455719, 0.5119796395301819
Loss in 400 steps: 0.9831468462944031 0.4792954623699188, 0.5038513541221619
Loss in 500 steps: 1.029045820236206 0.5145048499107361, 0.51454097032547
Loss in 600 steps: 1.0033694505691528 0.48526105284690857, 0.5181083679199219
Loss in 700 steps: 1.005514144897461 0.48791441321372986, 0.5175998210906982
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9901057481765747 0.48471274971961975, 0.5053929686546326
Loss in 100 steps: 0.9904493093490601 0.48132312297821045, 0.5091261863708496
Loss in 200 steps: 1.0009639263153076 0.488701194524765, 0.5122627019882202
Loss in 300 steps: 0.9830538034439087 0.47253796458244324, 0.5105158686637878
Loss in 400 steps: 0.9785125851631165 0.4773218035697937, 0.5011907815933228
Loss in 500 steps: 1.0201382637023926 0.508236825466156, 0.5119014382362366
Loss in 600 steps: 0.9966353178024292 0.4802100360393524, 0.5164252519607544
Loss in 700 steps: 0.9995830655097961 0.48224055767059326, 0.5173425078392029
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9787895679473877 0.4770744740962982, 0.5017150044441223
Loss in 100 steps: 0.9771919250488281 0.46957144141197205, 0.5076204538345337
Loss in 200 steps: 1.0165443420410156 0.5030221343040466, 0.513522207736969
Loss in 300 steps: 0.9963987469673157 0.48570510745048523, 0.5106936097145081
Loss in 400 steps: 0.9839975833892822 0.48027172684669495, 0.5037259459495544
Loss in 500 steps: 1.0266375541687012 0.5121325850486755, 0.5145049691200256
Loss in 600 steps: 0.9900074601173401 0.4746037423610687, 0.515403687953949
Loss in 700 steps: 0.9933337569236755 0.4760449230670929, 0.5172889232635498
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9842550158500671 0.48394253849983215, 0.5003125071525574
Loss in 100 steps: 0.9801480770111084 0.47304514050483704, 0.5071029663085938
Loss in 200 steps: 1.0028904676437378 0.4916486144065857, 0.5112418532371521
Loss in 300 steps: 0.9883280992507935 0.47800713777542114, 0.5103210210800171
Loss in 400 steps: 0.9851455688476562 0.48093998432159424, 0.504205584526062
Loss in 500 steps: 1.0174520015716553 0.5049635171890259, 0.5124884843826294
Loss in 600 steps: 1.0004581212997437 0.4837580621242523, 0.516700029373169
Loss in 700 steps: 0.9916599988937378 0.47621864080429077, 0.515441358089447
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.989561140537262 0.48709943890571594, 0.5024616718292236
Loss in 100 steps: 0.9811593294143677 0.473343163728714, 0.5078161358833313
Loss in 200 steps: 1.0016348361968994 0.49205341935157776, 0.5095813870429993
Loss in 300 steps: 0.9898049831390381 0.4800141155719757, 0.50979083776474
Loss in 400 steps: 0.9767611622810364 0.4733957350254059, 0.5033653974533081
Loss in 500 steps: 1.0309245586395264 0.5169038772583008, 0.5140207409858704
Loss in 600 steps: 0.9941585659980774 0.47847622632980347, 0.5156822800636292
Loss in 700 steps: 1.0007026195526123 0.48531416058540344, 0.5153884291648865
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9720044136047363 0.47182169556617737, 0.5001827478408813
Loss in 100 steps: 0.9793517589569092 0.47245049476623535, 0.5069012641906738
Loss in 200 steps: 1.0048258304595947 0.4939018189907074, 0.5109241008758545
Loss in 300 steps: 0.9892747402191162 0.47934895753860474, 0.5099257826805115
Loss in 400 steps: 0.9764567613601685 0.47595471143722534, 0.5005019903182983
Loss in 500 steps: 1.022786021232605 0.5088894367218018, 0.513896644115448
Loss in 600 steps: 0.9872908592224121 0.47279489040374756, 0.514495849609375
Loss in 700 steps: 0.9916305541992188 0.4768036901950836, 0.5148268342018127
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9781439900398254 0.47938185930252075, 0.4987621605396271
Loss in 100 steps: 0.980217695236206 0.4734269976615906, 0.5067906975746155
Loss in 200 steps: 1.0055330991744995 0.495484322309494, 0.5100487470626831
Loss in 300 steps: 0.9903073906898499 0.48101699352264404, 0.5092903971672058
Loss in 400 steps: 0.9733036160469055 0.4703492522239685, 0.5029543042182922
Loss in 500 steps: 1.0092151165008545 0.4984748363494873, 0.5107402801513672
Loss in 600 steps: 0.9891228675842285 0.4746468961238861, 0.5144760608673096
Loss in 700 steps: 0.9835110306739807 0.4701601564884186, 0.5133508443832397
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9765974879264832 0.4759674370288849, 0.5006300806999207
Loss in 100 steps: 0.9805318117141724 0.4726145267486572, 0.5079172253608704
Loss in 200 steps: 1.0016552209854126 0.4933617413043976, 0.5082935690879822
Loss in 300 steps: 0.9795116186141968 0.47264933586120605, 0.5068622827529907
Loss in 400 steps: 0.9878199100494385 0.48317819833755493, 0.504641592502594
Loss in 500 steps: 0.9890334010124207 0.4816225469112396, 0.5074108839035034
Loss in 600 steps: 0.9854630827903748 0.47172024846076965, 0.5137428045272827
Loss in 700 steps: 1.0000442266464233 0.48845183849334717, 0.5115923881530762
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9832244515419006 0.4805435538291931, 0.5026809573173523
Loss in 100 steps: 0.9825149178504944 0.47560086846351624, 0.5069141387939453
Loss in 200 steps: 0.9979222416877747 0.4898652732372284, 0.5080569982528687
Loss in 300 steps: 0.9853447079658508 0.4767305850982666, 0.5086140036582947
Loss in 400 steps: 0.9782042503356934 0.4758087396621704, 0.502395510673523
Loss in 500 steps: 1.0031640529632568 0.49211621284484863, 0.511047899723053
Loss in 600 steps: 0.9882365465164185 0.47343164682388306, 0.5148048996925354
Loss in 700 steps: 0.9815801978111267 0.4692121148109436, 0.5123680233955383
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9787099361419678 0.47834935784339905, 0.5003606081008911
Loss in 100 steps: 0.9820014238357544 0.474567174911499, 0.5074343085289001
Loss in 200 steps: 0.9987438321113586 0.4903688430786133, 0.5083750486373901
Loss in 300 steps: 0.9919811487197876 0.48284149169921875, 0.5091395974159241
Loss in 400 steps: 0.9762647747993469 0.47559547424316406, 0.5006693005561829
Loss in 500 steps: 1.0049258470535278 0.49643945693969727, 0.5084865093231201
Loss in 600 steps: 0.9936215877532959 0.4802970290184021, 0.5133246183395386
Loss in 700 steps: 0.9893439412117004 0.4779527187347412, 0.5113912224769592
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9778580665588379 0.4774477183818817, 0.5004103779792786
Loss in 100 steps: 0.9731175899505615 0.46682512760162354, 0.506292462348938
Loss in 200 steps: 1.001564860343933 0.4947674870491028, 0.5067974328994751
Loss in 300 steps: 0.9896160960197449 0.48305544257164, 0.5065606832504272
Loss in 400 steps: 0.9788606762886047 0.474949449300766, 0.5039111971855164
Loss in 500 steps: 0.9967125058174133 0.48721909523010254, 0.5094934105873108
Loss in 600 steps: 0.9819799065589905 0.46809619665145874, 0.513883650302887
Loss in 700 steps: 0.985840380191803 0.4742152988910675, 0.5116251111030579
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9762292504310608 0.47686493396759033, 0.49936434626579285
Loss in 100 steps: 0.979001522064209 0.4744901955127716, 0.5045113563537598
Loss in 200 steps: 0.9992666244506836 0.49270573258399963, 0.5065609216690063
Loss in 300 steps: 0.9852859377861023 0.4792501926422119, 0.5060358047485352
Loss in 400 steps: 0.9734442830085754 0.47115203738212585, 0.5022922158241272
Loss in 500 steps: 1.0023243427276611 0.4900641441345215, 0.5122601389884949
Loss in 600 steps: 0.9962341785430908 0.48299747705459595, 0.5132365822792053
Loss in 700 steps: 0.9955935478210449 0.48384910821914673, 0.511744499206543
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9664809107780457 0.4670136868953705, 0.4994671642780304
Loss in 100 steps: 0.9847609996795654 0.47849783301353455, 0.506263256072998
Loss in 200 steps: 0.9979650974273682 0.4899952709674835, 0.5079697966575623
Loss in 300 steps: 0.9919700026512146 0.48563888669013977, 0.5063310265541077
Loss in 400 steps: 0.9711617231369019 0.4689234495162964, 0.5022381544113159
Loss in 500 steps: 0.9983280897140503 0.4887065589427948, 0.5096216201782227
Loss in 600 steps: 0.9770768284797668 0.46553468704223633, 0.5115420818328857
Loss in 700 steps: 0.9945510029792786 0.48042505979537964, 0.5141259431838989
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9747609496116638 0.475395143032074, 0.49936583638191223
Loss in 100 steps: 0.9795127511024475 0.4733258783817291, 0.5061867833137512
Loss in 200 steps: 0.9906752109527588 0.4841748774051666, 0.5065004229545593
Loss in 300 steps: 0.9702973365783691 0.4653298854827881, 0.5049675107002258
Loss in 400 steps: 0.9720456600189209 0.47166624665260315, 0.5003794431686401
Loss in 500 steps: 0.9999597668647766 0.48865124583244324, 0.5113085508346558
Loss in 600 steps: 0.9807397127151489 0.4676456153392792, 0.5130941271781921
Loss in 700 steps: 0.9903006553649902 0.4780285656452179, 0.5122720003128052
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9860226511955261 0.48698386549949646, 0.49903881549835205
Loss in 100 steps: 0.9850725531578064 0.479777067899704, 0.5052955150604248
Loss in 200 steps: 0.999171257019043 0.49191156029701233, 0.5072596669197083
Loss in 300 steps: 0.9837959408760071 0.47704729437828064, 0.5067485570907593
Loss in 400 steps: 0.9806787371635437 0.47794222831726074, 0.5027365684509277
Loss in 500 steps: 1.0101807117462158 0.49975475668907166, 0.5104259252548218
Loss in 600 steps: 0.9909799695014954 0.4782646894454956, 0.5127151608467102
Loss in 700 steps: 0.9896154999732971 0.47835680842399597, 0.5112587809562683
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.975825846195221 0.47720611095428467, 0.4986196756362915
Loss in 100 steps: 0.9788371324539185 0.4732193350791931, 0.5056177973747253
Loss in 200 steps: 1.002865195274353 0.4961467385292053, 0.5067185759544373
Loss in 300 steps: 0.9888888001441956 0.4827101528644562, 0.5061786770820618
Loss in 400 steps: 0.972732663154602 0.47240757942199707, 0.500325083732605
Loss in 500 steps: 1.00467848777771 0.49428293108940125, 0.5103957056999207
Loss in 600 steps: 0.9880831837654114 0.4745900630950928, 0.5134931802749634
Loss in 700 steps: 0.9891398549079895 0.4777008593082428, 0.5114389657974243
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9859299063682556 0.48431575298309326, 0.5016140937805176
Loss in 100 steps: 0.9722586870193481 0.4677554965019226, 0.5045032501220703
Loss in 200 steps: 0.9991410374641418 0.4933517575263977, 0.5057892203330994
Loss in 300 steps: 0.9863001704216003 0.478604793548584, 0.5076953768730164
Loss in 400 steps: 0.9768857955932617 0.4766870141029358, 0.5001987814903259
Loss in 500 steps: 0.9968668818473816 0.4885854721069336, 0.508281409740448
Loss in 600 steps: 0.9760589599609375 0.4637441635131836, 0.5123148560523987
Loss in 700 steps: 0.9963751435279846 0.4844810366630554, 0.511894166469574
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9774619340896606 0.4770248830318451, 0.5004369616508484
Loss in 100 steps: 0.9731283783912659 0.4678686857223511, 0.5052597522735596
Loss in 200 steps: 1.0000425577163696 0.4936964213848114, 0.5063460469245911
Loss in 300 steps: 0.9806655645370483 0.47661101818084717, 0.5040544867515564
Loss in 400 steps: 0.984106183052063 0.48269039392471313, 0.5014158487319946
Loss in 500 steps: 0.9867412447929382 0.4781711995601654, 0.5085700154304504
Loss in 600 steps: 0.9877566695213318 0.4745831787586212, 0.513173520565033
Loss in 700 steps: 0.9840778112411499 0.4742383062839508, 0.5098395347595215
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9713291525840759 0.47111454606056213, 0.5002145767211914
Loss in 100 steps: 0.9715186357498169 0.46611517667770386, 0.5054035186767578
Loss in 200 steps: 0.9898619055747986 0.4842284619808197, 0.5056333541870117
Loss in 300 steps: 0.9826171398162842 0.47749197483062744, 0.5051251649856567
Loss in 400 steps: 0.9715113043785095 0.4731495976448059, 0.4983617961406708
Loss in 500 steps: 0.9967417120933533 0.48871272802352905, 0.5080289840698242
Loss in 600 steps: 0.9883851408958435 0.47513312101364136, 0.5132520198822021
Loss in 700 steps: 0.9939649105072021 0.4814043343067169, 0.5125606060028076
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9770681858062744 0.4775249660015106, 0.4995431900024414
Loss in 100 steps: 0.9687615633010864 0.46377864480018616, 0.5049829483032227
Loss in 200 steps: 0.994368851184845 0.487851619720459, 0.5065172910690308
Loss in 300 steps: 0.9858241081237793 0.48030510544776917, 0.5055189728736877
Loss in 400 steps: 0.9733183979988098 0.47287294268608093, 0.5004454851150513
Loss in 500 steps: 0.9826341867446899 0.47555649280548096, 0.5070778131484985
Loss in 600 steps: 0.9897788166999817 0.4769247770309448, 0.5128540396690369
Loss in 700 steps: 0.9900203347206116 0.4778931736946106, 0.5121272802352905
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9703768491744995 0.4713323414325714, 0.4990445673465729
Loss in 100 steps: 0.9765941500663757 0.47139760851860046, 0.5051964521408081
Loss in 200 steps: 0.9871211647987366 0.4821353852748871, 0.5049856901168823
Loss in 300 steps: 0.977172315120697 0.4739571511745453, 0.5032152533531189
Loss in 400 steps: 0.980796754360199 0.48048287630081177, 0.5003138780593872
Loss in 500 steps: 0.9876604080200195 0.48367542028427124, 0.5039849877357483
Loss in 600 steps: 0.9704193472862244 0.45973706245422363, 0.510682225227356
Loss in 700 steps: 0.9873141646385193 0.4771859049797058, 0.5101282596588135
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9770268201828003 0.4768843650817871, 0.5001424551010132
Loss in 100 steps: 0.9733635187149048 0.4686514139175415, 0.5047121047973633
Loss in 200 steps: 1.0014235973358154 0.496237188577652, 0.5051863789558411
Loss in 300 steps: 0.9793464541435242 0.4758455753326416, 0.5035009384155273
Loss in 400 steps: 0.979827880859375 0.4791395664215088, 0.5006882548332214
Loss in 500 steps: 0.9783653616905212 0.4742920994758606, 0.5040732622146606
Loss in 600 steps: 0.9742717742919922 0.4622783660888672, 0.5119933485984802
Loss in 700 steps: 1.0028462409973145 0.4920790493488312, 0.5107672810554504
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.977920413017273 0.4791328012943268, 0.49878764152526855
Loss in 100 steps: 0.9683179259300232 0.46591660380363464, 0.5024012923240662
Loss in 200 steps: 0.9884586334228516 0.48384392261505127, 0.5046147108078003
Loss in 300 steps: 0.9766172766685486 0.47315138578414917, 0.5034659504890442
Loss in 400 steps: 0.976888120174408 0.47682738304138184, 0.5000607371330261
Loss in 500 steps: 0.999630331993103 0.4903200566768646, 0.509310245513916
Loss in 600 steps: 0.9850249290466309 0.4733113646507263, 0.5117135047912598
Loss in 700 steps: 0.985496461391449 0.47502291202545166, 0.5104734897613525
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9679555892944336 0.4700307250022888, 0.4979248642921448
Loss in 100 steps: 0.9692032337188721 0.46382951736450195, 0.5053736567497253
Loss in 200 steps: 0.988844633102417 0.48455244302749634, 0.5042921900749207
Loss in 300 steps: 0.9811988472938538 0.4766750931739807, 0.504523754119873
Loss in 400 steps: 0.9695466160774231 0.4706977605819702, 0.4988488256931305
Loss in 500 steps: 1.0043041706085205 0.49509766697883606, 0.5092064142227173
Loss in 600 steps: 0.9849572777748108 0.4739690124988556, 0.5109882950782776
Loss in 700 steps: 0.9885330200195312 0.4781419634819031, 0.5103910565376282
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9733635187149048 0.4752654433250427, 0.4980980455875397
Loss in 100 steps: 0.9752931594848633 0.47145920991897583, 0.5038338899612427
Loss in 200 steps: 0.9997991919517517 0.49441367387771606, 0.5053855776786804
Loss in 300 steps: 0.9773784279823303 0.4719233810901642, 0.5054550766944885
Loss in 400 steps: 0.9765152931213379 0.4765486419200897, 0.49996665120124817
Loss in 500 steps: 0.9874364137649536 0.48012569546699524, 0.5073107481002808
Loss in 600 steps: 0.984982430934906 0.4724538028240204, 0.5125285983085632
Loss in 700 steps: 0.9861071705818176 0.47548091411590576, 0.5106262564659119
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9769377708435059 0.47929948568344116, 0.4976382553577423
Loss in 100 steps: 0.9752521514892578 0.47117167711257935, 0.5040804147720337
Loss in 200 steps: 0.9934340119361877 0.4889880418777466, 0.5044459104537964
Loss in 300 steps: 0.9790756106376648 0.4746023118495941, 0.5044732689857483
Loss in 400 steps: 0.970851719379425 0.47145339846611023, 0.49939823150634766
Loss in 500 steps: 0.9884772300720215 0.47959208488464355, 0.5088851451873779
Loss in 600 steps: 0.9889683127403259 0.4769733250141144, 0.5119950175285339
Loss in 700 steps: 0.9858270287513733 0.476215660572052, 0.5096113085746765
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9591699242591858 0.4629449248313904, 0.4962250292301178
Loss in 100 steps: 0.9756450653076172 0.4712456166744232, 0.5043994784355164
Loss in 200 steps: 0.9927932024002075 0.48702123761177063, 0.5057719945907593
Loss in 300 steps: 0.9796837568283081 0.4751341938972473, 0.5045495629310608
Loss in 400 steps: 0.9722036719322205 0.4728993773460388, 0.4993043839931488
Loss in 500 steps: 0.9940905570983887 0.48601895570755005, 0.5080716609954834
Loss in 600 steps: 0.9740221500396729 0.4628019630908966, 0.5112201571464539
Loss in 700 steps: 0.9812777638435364 0.4709402024745941, 0.5103376507759094
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9566815495491028 0.4620583951473236, 0.49462318420410156
Loss in 100 steps: 0.9707531929016113 0.4677141010761261, 0.5030390620231628
Loss in 200 steps: 0.9855710864067078 0.4821355938911438, 0.5034355521202087
Loss in 300 steps: 0.9806461334228516 0.47742918133735657, 0.5032170414924622
Loss in 400 steps: 0.9758901000022888 0.4764363765716553, 0.49945372343063354
Loss in 500 steps: 0.9794965386390686 0.4726918637752533, 0.5068046450614929
Loss in 600 steps: 0.9777947068214417 0.46819600462913513, 0.5095986723899841
Loss in 700 steps: 0.9917328953742981 0.481487900018692, 0.5102449059486389
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9758933186531067 0.4773661196231842, 0.4985271394252777
Loss in 100 steps: 0.9675831198692322 0.46452367305755615, 0.5030593872070312
Loss in 200 steps: 0.9782307744026184 0.47651928663253784, 0.5017115473747253
Loss in 300 steps: 0.9663463234901428 0.4638189971446991, 0.5025272965431213
Loss in 400 steps: 0.9664530158042908 0.46840840578079224, 0.4980446696281433
Loss in 500 steps: 0.9735285639762878 0.46767717599868774, 0.5058513879776001
Loss in 600 steps: 0.9748209714889526 0.4648793935775757, 0.509941577911377
Loss in 700 steps: 0.9753577709197998 0.4665816128253937, 0.5087761878967285
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9602963328361511 0.46452996134757996, 0.49576637148857117
Loss in 100 steps: 0.9765492677688599 0.47256097197532654, 0.5039883255958557
Loss in 200 steps: 0.9843378663063049 0.4810742139816284, 0.5032637119293213
Loss in 300 steps: 0.9776909351348877 0.4736771285533905, 0.5040137767791748
Loss in 400 steps: 0.9697893857955933 0.47084638476371765, 0.4989430010318756
Loss in 500 steps: 0.9873376488685608 0.4818771481513977, 0.5054604411125183
Loss in 600 steps: 0.996021568775177 0.48295673727989197, 0.5130649209022522
Loss in 700 steps: 0.9950538873672485 0.4851123094558716, 0.5099415183067322
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9600761532783508 0.4661371111869812, 0.4939391016960144
Loss in 100 steps: 0.9705074429512024 0.467159241437912, 0.5033481121063232
Loss in 200 steps: 0.9925258159637451 0.4877546727657318, 0.5047711730003357
Loss in 300 steps: 0.9719319343566895 0.46902430057525635, 0.5029075741767883
Loss in 400 steps: 0.9719874858856201 0.4733943045139313, 0.4985932409763336
Loss in 500 steps: 0.9851925373077393 0.48258426785469055, 0.5026082396507263
Loss in 600 steps: 0.9845869541168213 0.4725896716117859, 0.5119972825050354
Loss in 700 steps: 0.9939852952957153 0.48485150933265686, 0.5091336965560913
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9670287370681763 0.46858465671539307, 0.4984440803527832
Loss in 100 steps: 0.9647942185401917 0.4618145823478699, 0.5029796957969666
Loss in 200 steps: 0.9844284057617188 0.48168429732322693, 0.5027440786361694
Loss in 300 steps: 0.9840831160545349 0.47905197739601135, 0.5050310492515564
Loss in 400 steps: 0.9754920601844788 0.4759836196899414, 0.49950847029685974
Loss in 500 steps: 0.9874853491783142 0.48026058077812195, 0.5072247982025146
Loss in 600 steps: 0.987442135810852 0.4760817587375641, 0.5113603472709656
Loss in 700 steps: 0.9866616725921631 0.4767580032348633, 0.5099036693572998
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9627075791358948 0.46652892231941223, 0.49617862701416016
Loss in 100 steps: 0.9690583944320679 0.4653656482696533, 0.5036927461624146
Loss in 200 steps: 0.9881301522254944 0.485211580991745, 0.502918541431427
Loss in 300 steps: 0.9704623222351074 0.4681921601295471, 0.5022701621055603
Loss in 400 steps: 0.9701895117759705 0.47176340222358704, 0.4984261095523834
Loss in 500 steps: 0.9882237315177917 0.4812134802341461, 0.5070103406906128
Loss in 600 steps: 0.9940226078033447 0.4820871949195862, 0.5119355320930481
Loss in 700 steps: 0.990411639213562 0.48059478402137756, 0.5098167061805725
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.976714015007019 0.4800715446472168, 0.49664244055747986
Loss in 100 steps: 0.9683486223220825 0.4636956453323364, 0.5046529769897461
Loss in 200 steps: 0.9844634532928467 0.48125216364860535, 0.5032113194465637
Loss in 300 steps: 0.969413161277771 0.466657817363739, 0.5027554035186768
Loss in 400 steps: 0.9673771262168884 0.47041571140289307, 0.496961385011673
Loss in 500 steps: 0.9874416589736938 0.48122045397758484, 0.5062211155891418
Loss in 600 steps: 0.9808576107025146 0.47101905941963196, 0.5098385214805603
Loss in 700 steps: 0.9891304969787598 0.47895297408103943, 0.510177493095398
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9714857339859009 0.47440022230148315, 0.4970855116844177
Loss in 100 steps: 0.9796401858329773 0.4769785404205322, 0.5026616454124451
Loss in 200 steps: 0.9885089993476868 0.4845302999019623, 0.5039786696434021
Loss in 300 steps: 0.9679208397865295 0.4661591053009033, 0.501761794090271
Loss in 400 steps: 0.9736488461494446 0.4760631024837494, 0.4975856840610504
Loss in 500 steps: 0.9830752015113831 0.4789091646671295, 0.5041660666465759
Loss in 600 steps: 0.9743800163269043 0.46450209617614746, 0.5098779201507568
Loss in 700 steps: 0.9801077246665955 0.4706973731517792, 0.5094103813171387
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9706635475158691 0.47503140568733215, 0.495632141828537
Loss in 100 steps: 0.9720205068588257 0.46950459480285645, 0.5025159120559692
Loss in 200 steps: 0.9941998720169067 0.49004003405570984, 0.5041598677635193
Loss in 300 steps: 0.964349091053009 0.4634883999824524, 0.5008606910705566
Loss in 400 steps: 0.9807029962539673 0.4813970625400543, 0.49930593371391296
Loss in 500 steps: 0.9764595031738281 0.47517523169517517, 0.5012842416763306
Loss in 600 steps: 0.9814234972000122 0.47113990783691406, 0.5102834701538086
Loss in 700 steps: 0.9934067726135254 0.4826267659664154, 0.5107800364494324
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9686123132705688 0.47271355986595154, 0.4958987832069397
Loss in 100 steps: 0.975648820400238 0.47181639075279236, 0.5038323998451233
Loss in 200 steps: 0.9938632249832153 0.489361435174942, 0.5045018196105957
Loss in 300 steps: 0.9550608992576599 0.4566151797771454, 0.4984457194805145
Loss in 400 steps: 0.9673031568527222 0.4703424274921417, 0.4969607889652252
Loss in 500 steps: 0.9878526926040649 0.48150959610939026, 0.5063430666923523
Loss in 600 steps: 0.9945115447044373 0.4830209016799927, 0.5114905834197998
Loss in 700 steps: 0.9948263168334961 0.48379528522491455, 0.5110310912132263
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9780372381210327 0.4796750247478485, 0.49836215376853943
Loss in 100 steps: 0.9710952639579773 0.46735385060310364, 0.503741443157196
Loss in 200 steps: 0.9987694025039673 0.4942883849143982, 0.5044810771942139
Loss in 300 steps: 0.9630244374275208 0.462405264377594, 0.5006191730499268
Loss in 400 steps: 0.9726607203483582 0.474547803401947, 0.49811291694641113
Loss in 500 steps: 0.984855592250824 0.4813571870326996, 0.503498375415802
Loss in 600 steps: 0.9860514998435974 0.47517067193984985, 0.5108808279037476
Loss in 700 steps: 0.9950811862945557 0.48505496978759766, 0.510026216506958
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9699845910072327 0.4743298590183258, 0.4956547021865845
Loss in 100 steps: 0.9679669737815857 0.46484464406967163, 0.5031224489212036
Loss in 200 steps: 0.9965806603431702 0.4927278757095337, 0.5038528442382812
Loss in 300 steps: 0.9800931215286255 0.4778289198875427, 0.502264142036438
Loss in 400 steps: 0.9704707860946655 0.4706498980522156, 0.49982088804244995
Loss in 500 steps: 0.9874188899993896 0.4822378158569336, 0.505181074142456
Loss in 600 steps: 0.9814785122871399 0.4710965156555176, 0.5103819966316223
Loss in 700 steps: 0.9777830839157104 0.4683578908443451, 0.5094252228736877
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9721781611442566 0.4755077362060547, 0.4966704547405243
Loss in 100 steps: 0.9721907377243042 0.4684770703315735, 0.5037136673927307
Loss in 200 steps: 0.9871759414672852 0.4845512807369232, 0.5026246905326843
Loss in 300 steps: 0.9591658711433411 0.45934128761291504, 0.49982452392578125
Loss in 400 steps: 0.9673453569412231 0.46892356872558594, 0.49842187762260437
Loss in 500 steps: 0.9832785725593567 0.4775015115737915, 0.5057769417762756
Loss in 600 steps: 0.9819467067718506 0.4714263379573822, 0.510520339012146
Loss in 700 steps: 0.9715872406959534 0.46275588870048523, 0.5088314414024353
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9682086110115051 0.4704912304878235, 0.49771741032600403
Loss in 100 steps: 0.9670279622077942 0.46411946415901184, 0.50290846824646
Loss in 200 steps: 0.9996995329856873 0.494830459356308, 0.5048690438270569
Loss in 300 steps: 0.9751453995704651 0.47399699687957764, 0.501148521900177
Loss in 400 steps: 0.9699357748031616 0.4711408317089081, 0.4987949728965759
Loss in 500 steps: 0.981648325920105 0.4775412082672119, 0.5041071176528931
Loss in 600 steps: 0.9843082427978516 0.4738565683364868, 0.5104517340660095
Loss in 700 steps: 0.9913294911384583 0.48240166902542114, 0.5089278817176819
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9727402925491333 0.47522953152656555, 0.49751076102256775
Loss in 100 steps: 0.9659746289253235 0.4635573625564575, 0.502417266368866
Loss in 200 steps: 0.9878788590431213 0.4847276508808136, 0.5031512379646301
Loss in 300 steps: 0.9623269438743591 0.46239858865737915, 0.49992841482162476
Loss in 400 steps: 0.9665581583976746 0.46965402364730835, 0.4969041347503662
Loss in 500 steps: 1.0000475645065308 0.4922046661376953, 0.5078428387641907
Loss in 600 steps: 0.9802813529968262 0.47046715021133423, 0.5098141431808472
Loss in 700 steps: 0.9929560422897339 0.4826286733150482, 0.5103273987770081
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9607840180397034 0.4638822078704834, 0.4969017505645752
Loss in 100 steps: 0.9760258793830872 0.4730202257633209, 0.5030056834220886
Loss in 200 steps: 0.982795000076294 0.479717493057251, 0.503077507019043
Loss in 300 steps: 0.9644753932952881 0.4650155305862427, 0.4994598627090454
Loss in 400 steps: 0.9783738851547241 0.4785309433937073, 0.49984294176101685
Loss in 500 steps: 0.9820731282234192 0.4787565767765045, 0.5033165216445923
Loss in 600 steps: 0.9839379787445068 0.4737492501735687, 0.510188639163971
Loss in 700 steps: 0.98792964220047 0.4786413609981537, 0.5092882513999939
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9640341997146606 0.46955665946006775, 0.4944775402545929
Loss in 100 steps: 0.9663378596305847 0.46406474709510803, 0.5022730827331543
Loss in 200 steps: 0.9930917620658875 0.48976996541023254, 0.5033218264579773
Loss in 300 steps: 0.9744845628738403 0.4729437530040741, 0.5015408396720886
Loss in 400 steps: 0.9706469178199768 0.47307512164115906, 0.49757182598114014
Loss in 500 steps: 1.0001976490020752 0.49138468503952026, 0.5088129639625549
Loss in 600 steps: 0.9811062812805176 0.4698401093482971, 0.5112661719322205
Loss in 700 steps: 0.9948433637619019 0.48515841364860535, 0.5096849799156189
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9671274423599243 0.4714985191822052, 0.49562886357307434
Loss in 100 steps: 0.9708756804466248 0.46744346618652344, 0.5034321546554565
Loss in 200 steps: 0.988718569278717 0.48503783345222473, 0.5036807656288147
Loss in 300 steps: 0.9734471440315247 0.4708790183067322, 0.5025681257247925
Loss in 400 steps: 0.9820056557655334 0.48171210289001465, 0.5002935528755188
Loss in 500 steps: 0.9882427453994751 0.48296982049942017, 0.5052729845046997
Loss in 600 steps: 0.9837449789047241 0.4733862578868866, 0.5103586912155151
Loss in 700 steps: 0.990143895149231 0.4799007773399353, 0.5102430582046509
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9572997093200684 0.46394529938697815, 0.4933544397354126
Loss in 100 steps: 0.9637920260429382 0.46163779497146606, 0.5021541714668274
Loss in 200 steps: 0.9946046471595764 0.491609126329422, 0.5029955506324768
Loss in 300 steps: 0.9670737385749817 0.46789154410362244, 0.49918219447135925
Loss in 400 steps: 0.9762142896652222 0.4787351191043854, 0.4974792003631592
Loss in 500 steps: 0.9977858662605286 0.4910026788711548, 0.5067831873893738
Loss in 600 steps: 0.9857747554779053 0.4758283793926239, 0.509946346282959
Loss in 700 steps: 0.9864802360534668 0.47711947560310364, 0.5093607306480408
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9658529162406921 0.4727201461791992, 0.4931327998638153
Loss in 100 steps: 0.9614282846450806 0.45825353264808655, 0.5031746029853821
Loss in 200 steps: 1.0008190870285034 0.497314989566803, 0.5035040974617004
Loss in 300 steps: 0.9573177099227905 0.4574298858642578, 0.4998878836631775
Loss in 400 steps: 0.9630215764045715 0.4658677577972412, 0.4971538484096527
Loss in 500 steps: 0.984037458896637 0.4798496663570404, 0.5041877627372742
Loss in 600 steps: 0.9826066493988037 0.47154271602630615, 0.5110639333724976
Loss in 700 steps: 0.9940269589424133 0.48374664783477783, 0.5102803111076355
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9663906693458557 0.47083136439323425, 0.4955592751502991
Loss in 100 steps: 0.9662441611289978 0.46326133608818054, 0.5029828548431396
Loss in 200 steps: 0.9897794127464294 0.4867618680000305, 0.5030175447463989
Loss in 300 steps: 0.9795190095901489 0.47707074880599976, 0.5024482607841492
Loss in 400 steps: 0.9676886796951294 0.4701123535633087, 0.4975762367248535
Loss in 500 steps: 0.9830992817878723 0.4776436686515808, 0.5054556131362915
Loss in 600 steps: 0.9812072515487671 0.47176992893218994, 0.5094373226165771
Loss in 700 steps: 0.9869579076766968 0.4771254062652588, 0.5098325610160828
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9689168930053711 0.47271159291267395, 0.49620530009269714
Loss in 100 steps: 0.9636834263801575 0.46161791682243347, 0.5020655393600464
Loss in 200 steps: 0.9908320903778076 0.48690417408943176, 0.5039279460906982
Loss in 300 steps: 0.968834638595581 0.46834373474121094, 0.5004909038543701
Loss in 400 steps: 0.9665810465812683 0.4700944423675537, 0.4964865446090698
Loss in 500 steps: 0.996856153011322 0.49224182963371277, 0.5046143531799316
Loss in 600 steps: 0.9751922488212585 0.4657319188117981, 0.5094603300094604
Loss in 700 steps: 0.987112283706665 0.4782372713088989, 0.5088748931884766
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9703433513641357 0.4748019874095917, 0.4955413341522217
Loss in 100 steps: 0.9689877033233643 0.46635690331459045, 0.502630889415741
Loss in 200 steps: 0.9845045208930969 0.48165565729141235, 0.5028488039970398
Loss in 300 steps: 0.9627816081047058 0.46343979239463806, 0.49934178590774536
Loss in 400 steps: 0.9707814455032349 0.47232380509376526, 0.498457670211792
Loss in 500 steps: 0.9887709617614746 0.48672452569007874, 0.5020464658737183
Loss in 600 steps: 0.969559371471405 0.46083372831344604, 0.5087257027626038
Loss in 700 steps: 0.989622950553894 0.48003894090652466, 0.5095839500427246
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9661625027656555 0.4710988700389862, 0.49506357312202454
Loss in 100 steps: 0.9772940278053284 0.474252849817276, 0.5030413269996643
Loss in 200 steps: 0.9914249181747437 0.4874427318572998, 0.5039821863174438
Loss in 300 steps: 0.9606170058250427 0.460806280374527, 0.4998107850551605
Loss in 400 steps: 0.9740070104598999 0.47648829221725464, 0.4975186884403229
Loss in 500 steps: 0.9969504475593567 0.4903354048728943, 0.5066151022911072
Loss in 600 steps: 0.9775400757789612 0.4697572588920593, 0.5077828168869019
Loss in 700 steps: 1.000482201576233 0.4911092221736908, 0.5093729496002197
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9720461368560791 0.4756164252758026, 0.4964297115802765
Loss in 100 steps: 0.9674828052520752 0.4655085504055023, 0.50197434425354
Loss in 200 steps: 0.9856950044631958 0.4839525520801544, 0.501742422580719
Loss in 300 steps: 0.9639343619346619 0.46431005001068115, 0.4996243119239807
Loss in 400 steps: 0.9786319732666016 0.47956663370132446, 0.4990653395652771
Loss in 500 steps: 0.9867643713951111 0.48087137937545776, 0.5058928728103638
Loss in 600 steps: 0.9842680096626282 0.476131409406662, 0.5081365704536438
Loss in 700 steps: 0.9996363520622253 0.4912228584289551, 0.5084134340286255
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9834752082824707 0.4869576096534729, 0.4965176284313202
Loss in 100 steps: 0.9705636501312256 0.4683305621147156, 0.50223308801651
Loss in 200 steps: 0.9925905466079712 0.4892577528953552, 0.503332793712616
Loss in 300 steps: 0.9724509119987488 0.47186362743377686, 0.5005872845649719
Loss in 400 steps: 0.9806843400001526 0.4792673885822296, 0.5014168620109558
Loss in 500 steps: 0.9724218845367432 0.47126340866088867, 0.5011585354804993
Loss in 600 steps: 0.9796623587608337 0.47009435296058655, 0.5095679759979248
Loss in 700 steps: 0.9901276230812073 0.48171302676200867, 0.508414626121521
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9724205732345581 0.4765745997428894, 0.49584588408470154
Loss in 100 steps: 0.9721029996871948 0.4696677029132843, 0.5024352669715881
Loss in 200 steps: 0.9938556551933289 0.488807737827301, 0.5050479769706726
Loss in 300 steps: 0.960527777671814 0.4602471590042114, 0.5002806186676025
Loss in 400 steps: 0.9712693095207214 0.4723787009716034, 0.4988906979560852
Loss in 500 steps: 0.9988298416137695 0.4916549623012543, 0.5071749091148376
Loss in 600 steps: 0.9813679456710815 0.4723023772239685, 0.5090655088424683
Loss in 700 steps: 0.9853162169456482 0.47602516412734985, 0.5092910528182983
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9797730445861816 0.4820634126663208, 0.497709721326828
Loss in 100 steps: 0.976097047328949 0.47324150800704956, 0.5028555989265442
Loss in 200 steps: 0.9859848022460938 0.48324304819107056, 0.5027417540550232
Loss in 300 steps: 0.9695411324501038 0.4689265489578247, 0.500614583492279
Loss in 400 steps: 0.972543478012085 0.4737948477268219, 0.49874868988990784
Loss in 500 steps: 0.9898741841316223 0.4837527275085449, 0.5061215162277222
Loss in 600 steps: 0.9708054065704346 0.4642478823661804, 0.5065575242042542
Loss in 700 steps: 0.9940069913864136 0.48475882411003113, 0.5092480778694153
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9736824035644531 0.4769369065761566, 0.4967454969882965
Loss in 100 steps: 0.9736647009849548 0.47054022550582886, 0.503124475479126
Loss in 200 steps: 0.9854422211647034 0.4823782444000244, 0.503063976764679
Loss in 300 steps: 0.9727376103401184 0.47040852904319763, 0.5023292303085327
Loss in 400 steps: 0.9618460536003113 0.4655647873878479, 0.4962812066078186
Loss in 500 steps: 0.9861584901809692 0.48249709606170654, 0.5036613941192627
Loss in 600 steps: 0.9860462546348572 0.47734174132347107, 0.5087044835090637
Loss in 700 steps: 0.9805419445037842 0.4721376895904541, 0.5084041953086853
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9688844680786133 0.4745621681213379, 0.4943222999572754
Loss in 100 steps: 0.9730038046836853 0.47160208225250244, 0.5014017224311829
Loss in 200 steps: 0.9918728470802307 0.4883984923362732, 0.5034744143486023
Loss in 300 steps: 0.9623188376426697 0.4643711745738983, 0.49794772267341614
Loss in 400 steps: 0.9614442586898804 0.4656527638435364, 0.495791494846344
Loss in 500 steps: 0.9846722483634949 0.4808509647846222, 0.5038212537765503
Loss in 600 steps: 0.9757260680198669 0.46750035881996155, 0.5082257390022278
Loss in 700 steps: 0.9926339387893677 0.48408812284469604, 0.5085458755493164
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9639723300933838 0.4711916446685791, 0.4927806258201599
Loss in 100 steps: 0.9731677174568176 0.47043517231941223, 0.502732515335083
Loss in 200 steps: 0.9937966465950012 0.49010518193244934, 0.5036914944648743
Loss in 300 steps: 0.9571061134338379 0.4587595760822296, 0.4983465373516083
Loss in 400 steps: 0.9677464365959167 0.4713023006916046, 0.49644413590431213
Loss in 500 steps: 0.9748992919921875 0.4738398492336273, 0.5010594725608826
Loss in 600 steps: 0.9674892425537109 0.4599584937095642, 0.5075307488441467
Loss in 700 steps: 0.9968465566635132 0.48738420009613037, 0.5094624161720276
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.976193904876709 0.4794071912765503, 0.4967866539955139
Loss in 100 steps: 0.9737645983695984 0.47042346000671387, 0.503341019153595
Loss in 200 steps: 0.9919039011001587 0.4883705973625183, 0.5035333037376404
Loss in 300 steps: 0.9785279035568237 0.47669997811317444, 0.5018278956413269
Loss in 400 steps: 0.9648787975311279 0.4679679572582245, 0.49691084027290344
Loss in 500 steps: 0.9896963238716125 0.48666033148765564, 0.5030360221862793
Loss in 600 steps: 0.9749129414558411 0.46546730399131775, 0.5094456076622009
Loss in 700 steps: 0.9907453060150146 0.48144903779029846, 0.5092962384223938
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9653236865997314 0.47071895003318787, 0.49460479617118835
Loss in 100 steps: 0.9658331871032715 0.46333301067352295, 0.5025002360343933
Loss in 200 steps: 0.9859712719917297 0.4819696247577667, 0.5040016770362854
Loss in 300 steps: 0.9564968347549438 0.45728975534439087, 0.49920713901519775
Loss in 400 steps: 0.9764699935913086 0.47848212718963623, 0.49798789620399475
Loss in 500 steps: 0.9745854139328003 0.471223920583725, 0.5033615231513977
Loss in 600 steps: 0.9771363139152527 0.4695312976837158, 0.5076050162315369
Loss in 700 steps: 0.9952113628387451 0.4873000979423523, 0.5079113245010376
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9720692038536072 0.47711697220802307, 0.4949522316455841
Loss in 100 steps: 0.9579640626907349 0.45680704712867737, 0.5011570453643799
Loss in 200 steps: 0.9973498582839966 0.4939464330673218, 0.5034034252166748
Loss in 300 steps: 0.9634608626365662 0.4643755257129669, 0.49908533692359924
Loss in 400 steps: 0.9577042460441589 0.46255627274513245, 0.4951480031013489
Loss in 500 steps: 0.9816651344299316 0.47700223326683044, 0.5046629309654236
Loss in 600 steps: 0.9831611514091492 0.4752247929573059, 0.5079363584518433
Loss in 700 steps: 0.9978159666061401 0.4899093508720398, 0.5079066753387451
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9691919684410095 0.47527188062667847, 0.49392011761665344
Loss in 100 steps: 0.9757072925567627 0.4734935164451599, 0.5022138357162476
Loss in 200 steps: 0.9935101270675659 0.49012255668640137, 0.5033875703811646
Loss in 300 steps: 0.9741215705871582 0.4729350805282593, 0.5011864304542542
Loss in 400 steps: 0.9658030271530151 0.469624787569046, 0.49617817997932434
Loss in 500 steps: 0.9849647879600525 0.483354389667511, 0.501610517501831
Loss in 600 steps: 0.9855325818061829 0.4782770276069641, 0.507255494594574
Loss in 700 steps: 0.9860838651657104 0.47847649455070496, 0.5076073408126831
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9709067344665527 0.4756622016429901, 0.49524450302124023
Loss in 100 steps: 0.968950092792511 0.46834224462509155, 0.5006078481674194
Loss in 200 steps: 0.9931552410125732 0.4896484911441803, 0.5035068392753601
Loss in 300 steps: 0.9629960060119629 0.4658747613430023, 0.4971212148666382
Loss in 400 steps: 0.961156964302063 0.46648845076560974, 0.49466851353645325
Loss in 500 steps: 0.9768149256706238 0.4756587743759155, 0.5011561512947083
Loss in 600 steps: 0.9678786396980286 0.46101564168930054, 0.5068630576133728
Loss in 700 steps: 0.9854131937026978 0.4780982434749603, 0.5073149800300598
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9678519368171692 0.4738672077655792, 0.49398472905158997
Loss in 100 steps: 0.9761404395103455 0.47477641701698303, 0.5013640522956848
Loss in 200 steps: 1.0004850625991821 0.49697813391685486, 0.5035070180892944
Loss in 300 steps: 0.9594902396202087 0.46078455448150635, 0.4987056255340576
Loss in 400 steps: 0.9590851068496704 0.46343040466308594, 0.4956546127796173
Loss in 500 steps: 0.9931914806365967 0.48856550455093384, 0.5046259164810181
Loss in 600 steps: 0.983518660068512 0.47530537843704224, 0.508213222026825
Loss in 700 steps: 0.9979621767997742 0.49019289016723633, 0.5077692866325378
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9685805439949036 0.47391048073768616, 0.494670033454895
Loss in 100 steps: 0.9737310409545898 0.4725661873817444, 0.5011648535728455
Loss in 200 steps: 1.0010817050933838 0.49678224325180054, 0.5042994618415833
Loss in 300 steps: 0.9640291929244995 0.46305447816848755, 0.5009747743606567
Loss in 400 steps: 0.9669767022132874 0.4703228175640106, 0.4966539144515991
Loss in 500 steps: 0.9813203811645508 0.4795513451099396, 0.5017690062522888
Loss in 600 steps: 0.9671701788902283 0.45981523394584656, 0.5073549747467041
Loss in 700 steps: 0.9980367422103882 0.4904313087463379, 0.5076054334640503
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9694865942001343 0.4754326045513153, 0.4940539598464966
Loss in 100 steps: 0.9804619550704956 0.48016515374183655, 0.5002968311309814
Loss in 200 steps: 0.9933978915214539 0.49059295654296875, 0.5028049349784851
Loss in 300 steps: 0.959488034248352 0.46027249097824097, 0.49921560287475586
Loss in 400 steps: 0.9670641422271729 0.4701663851737976, 0.49689781665802
Loss in 500 steps: 0.9769916534423828 0.4760023355484009, 0.5009893178939819
Loss in 600 steps: 0.9673283696174622 0.4601624011993408, 0.5071659684181213
Loss in 700 steps: 0.9889706373214722 0.4810355007648468, 0.5079349875450134
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9747723340988159 0.4791417121887207, 0.4956305921077728
Loss in 100 steps: 0.9641241431236267 0.46441757678985596, 0.499706506729126
Loss in 200 steps: 0.9925186038017273 0.4897906184196472, 0.5027279853820801
Loss in 300 steps: 0.9625988006591797 0.46185943484306335, 0.500739336013794
Loss in 400 steps: 0.9717476963996887 0.47434160113334656, 0.49740612506866455
Loss in 500 steps: 0.9951162338256836 0.4897696077823639, 0.5053466558456421
Loss in 600 steps: 0.982189953327179 0.4742231070995331, 0.5079668760299683
Loss in 700 steps: 0.9911600351333618 0.4830840826034546, 0.508076012134552
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9675742387771606 0.47282710671424866, 0.494747132062912
Loss in 100 steps: 0.9647607207298279 0.46479088068008423, 0.49996981024742126
Loss in 200 steps: 0.9920618534088135 0.4895414412021637, 0.5025203824043274
Loss in 300 steps: 0.9714866280555725 0.4693632423877716, 0.5021233558654785
Loss in 400 steps: 0.959196150302887 0.46399635076522827, 0.4951998293399811
Loss in 500 steps: 0.9764925241470337 0.4750917851924896, 0.5014006495475769
Loss in 600 steps: 0.9792423844337463 0.47125834226608276, 0.5079840421676636
Loss in 700 steps: 0.983900785446167 0.4757792055606842, 0.50812166929245
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9805029034614563 0.48534420132637024, 0.4951587915420532
Loss in 100 steps: 0.9637874960899353 0.4639393091201782, 0.4998481571674347
Loss in 200 steps: 0.9958977103233337 0.49169594049453735, 0.5042017698287964
Loss in 300 steps: 0.9651825428009033 0.4661034047603607, 0.4990791976451874
Loss in 400 steps: 0.964367687702179 0.46884819865226746, 0.4955194890499115
Loss in 500 steps: 0.9867744445800781 0.48553842306137085, 0.5012360215187073
Loss in 600 steps: 0.9746508002281189 0.46753743290901184, 0.5071132779121399
Loss in 700 steps: 0.9935928583145142 0.48370712995529175, 0.5098857879638672
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9656558036804199 0.47076141834259033, 0.4948943257331848
Loss in 100 steps: 0.9752404689788818 0.47335195541381836, 0.5018885135650635
Loss in 200 steps: 0.986899197101593 0.4858945906162262, 0.501004695892334
Loss in 300 steps: 0.9771105647087097 0.47547999024391174, 0.5016306042671204
Loss in 400 steps: 0.9663525819778442 0.47035184502601624, 0.49600064754486084
Loss in 500 steps: 0.9896588325500488 0.4836028218269348, 0.506056010723114
Loss in 600 steps: 0.9795692563056946 0.47262635827064514, 0.506942868232727
Loss in 700 steps: 0.9874730110168457 0.4785430133342743, 0.5089300274848938
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9678433537483215 0.47457653284072876, 0.4932668209075928
Loss in 100 steps: 0.978325366973877 0.47681939601898193, 0.501505970954895
Loss in 200 steps: 0.9993753433227539 0.49482375383377075, 0.5045515894889832
Loss in 300 steps: 0.9741247296333313 0.4721074104309082, 0.5020173192024231
Loss in 400 steps: 0.9573379158973694 0.46280041337013245, 0.49453750252723694
Loss in 500 steps: 0.9787020683288574 0.4741329848766327, 0.5045689940452576
Loss in 600 steps: 0.9716109037399292 0.4647762179374695, 0.5068346858024597
Loss in 700 steps: 0.996112048625946 0.4875907897949219, 0.5085212588310242
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9630040526390076 0.46955636143684387, 0.4934476613998413
Loss in 100 steps: 0.9760715365409851 0.47415891289711, 0.5019126534461975
Loss in 200 steps: 0.9940771460533142 0.49161937832832336, 0.5024576783180237
Loss in 300 steps: 0.9663218259811401 0.46433699131011963, 0.5019848346710205
Loss in 400 steps: 0.9741286635398865 0.47457143664360046, 0.4995571970939636
Loss in 500 steps: 0.9701969027519226 0.47068995237350464, 0.4995069205760956
Loss in 600 steps: 0.9717640280723572 0.4656120836734772, 0.5061519145965576
Loss in 700 steps: 0.9841646552085876 0.477052241563797, 0.5071123242378235
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9710000157356262 0.4759303629398346, 0.495069682598114
Loss in 100 steps: 0.9783676266670227 0.47585058212280273, 0.50251704454422
Loss in 200 steps: 0.9935368895530701 0.49043262004852295, 0.5031043291091919
Loss in 300 steps: 0.9609242677688599 0.46186563372612, 0.4990585446357727
Loss in 400 steps: 0.9666611552238464 0.4703141152858734, 0.49634695053100586
Loss in 500 steps: 0.9898456335067749 0.48720523715019226, 0.502640426158905
Loss in 600 steps: 0.9851775765419006 0.4763350784778595, 0.5088424682617188
Loss in 700 steps: 0.9817874431610107 0.47402334213256836, 0.5077641606330872
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9724918603897095 0.47655460238456726, 0.4959372580051422
Loss in 100 steps: 0.977294921875 0.4759054183959961, 0.5013895630836487
Loss in 200 steps: 0.9931768178939819 0.48913800716400146, 0.5040388107299805
Loss in 300 steps: 0.9731611609458923 0.4718281328678131, 0.5013329982757568
Loss in 400 steps: 0.9631432890892029 0.4682503640651703, 0.49489298462867737
Loss in 500 steps: 0.9791719317436218 0.47763189673423767, 0.5015400052070618
Loss in 600 steps: 0.972836434841156 0.4671105146408081, 0.5057259798049927
Loss in 700 steps: 0.9906232357025146 0.48227787017822266, 0.508345365524292
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.973127543926239 0.47747600078582764, 0.49565160274505615
Loss in 100 steps: 0.9751303195953369 0.4738798439502716, 0.5012505054473877
Loss in 200 steps: 0.9924803972244263 0.4887259304523468, 0.5037545561790466
Loss in 300 steps: 0.9573001861572266 0.45765623450279236, 0.499644011259079
Loss in 400 steps: 0.967385470867157 0.4717373549938202, 0.4956481456756592
Loss in 500 steps: 0.9773886203765869 0.4728815257549286, 0.5045071244239807
Loss in 600 steps: 0.9807922840118408 0.47443827986717224, 0.5063539743423462
Loss in 700 steps: 0.9917700290679932 0.4835473597049713, 0.5082226991653442
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9657564759254456 0.47187310457229614, 0.4938834011554718
Loss in 100 steps: 0.9654244184494019 0.4656462073326111, 0.49977824091911316
Loss in 200 steps: 0.9932673573493958 0.48943743109703064, 0.5038298964500427
Loss in 300 steps: 0.9722592830657959 0.4721631109714508, 0.5000961422920227
Loss in 400 steps: 0.9668909311294556 0.4705459475517273, 0.4963448941707611
Loss in 500 steps: 0.9819570183753967 0.47878214716911316, 0.5031748414039612
Loss in 600 steps: 0.9760916233062744 0.46924272179603577, 0.506848931312561
Loss in 700 steps: 0.9901353716850281 0.4829907715320587, 0.507144570350647
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9664438962936401 0.4726580083370209, 0.49378588795661926
Loss in 100 steps: 0.9709456562995911 0.47159743309020996, 0.4993482232093811
Loss in 200 steps: 1.001643180847168 0.4979901909828186, 0.5036528706550598
Loss in 300 steps: 0.9779529571533203 0.4773847162723541, 0.5005682110786438
Loss in 400 steps: 0.965352475643158 0.46868157386779785, 0.4966709315776825
Loss in 500 steps: 0.9528642296791077 0.4559665322303772, 0.4968976080417633
Loss in 600 steps: 0.9938371777534485 0.4855508804321289, 0.5082862973213196
Loss in 700 steps: 0.9886957406997681 0.4808382987976074, 0.507857620716095
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9666248559951782 0.4741440713405609, 0.4924808144569397
Loss in 100 steps: 0.9752292633056641 0.4749794900417328, 0.5002498030662537
Loss in 200 steps: 0.9914920330047607 0.4880017936229706, 0.5034902691841125
Loss in 300 steps: 0.9691684246063232 0.46862080693244934, 0.5005476474761963
Loss in 400 steps: 0.9485089182853699 0.4568973779678345, 0.4916115999221802
Loss in 500 steps: 0.9858484268188477 0.48228156566619873, 0.5035669803619385
Loss in 600 steps: 0.9761375188827515 0.4690204858779907, 0.5071170926094055
Loss in 700 steps: 0.9853672981262207 0.476505309343338, 0.5088619589805603
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9718673825263977 0.4765196442604065, 0.4953477382659912
Loss in 100 steps: 0.9710036516189575 0.4705204367637634, 0.5004832744598389
Loss in 200 steps: 0.9958975911140442 0.4923836588859558, 0.5035139322280884
Loss in 300 steps: 0.9664288759231567 0.4674949645996094, 0.498933881521225
Loss in 400 steps: 0.9568608403205872 0.463604211807251, 0.4932566285133362
Loss in 500 steps: 0.9787329435348511 0.4778437912464142, 0.5008891820907593
Loss in 600 steps: 0.9709272384643555 0.46514442563056946, 0.5057828426361084
Loss in 700 steps: 0.9859886169433594 0.4780992567539215, 0.5078893303871155
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9664068222045898 0.4717017710208893, 0.49470508098602295
Loss in 100 steps: 0.9720932841300964 0.4714222848415375, 0.5006709694862366
Loss in 200 steps: 0.9960782527923584 0.491634339094162, 0.504443883895874
Loss in 300 steps: 0.9630028009414673 0.4646837115287781, 0.498319149017334
Loss in 400 steps: 0.9479328989982605 0.4540911912918091, 0.4938417077064514
Loss in 500 steps: 0.9865973591804504 0.4841611087322235, 0.5024362206459045
Loss in 600 steps: 0.9935063123703003 0.4865896701812744, 0.5069165825843811
Loss in 700 steps: 0.9809246063232422 0.47331586480140686, 0.5076086521148682
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9769947528839111 0.4806385934352875, 0.49635615944862366
Loss in 100 steps: 0.9743951559066772 0.4739813804626465, 0.5004137754440308
Loss in 200 steps: 0.9968596696853638 0.49300217628479004, 0.503857433795929
Loss in 300 steps: 0.96293044090271 0.46490615606307983, 0.4980243146419525
Loss in 400 steps: 0.9606946706771851 0.4662106931209564, 0.494484007358551
Loss in 500 steps: 0.98694908618927 0.48175525665283203, 0.505193829536438
Loss in 600 steps: 0.9775665402412415 0.47053733468055725, 0.5070291757583618
Loss in 700 steps: 0.9807031154632568 0.4725695252418518, 0.508133590221405
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9655348062515259 0.471545934677124, 0.4939888119697571
Loss in 100 steps: 0.9745426774024963 0.47573840618133545, 0.4988042414188385
Loss in 200 steps: 0.9946876764297485 0.4912501871585846, 0.5034375190734863
Loss in 300 steps: 0.9635603427886963 0.46385785937309265, 0.49970248341560364
Loss in 400 steps: 0.9533284902572632 0.4576605558395386, 0.4956679344177246
Loss in 500 steps: 0.9748152494430542 0.47460466623306274, 0.5002106428146362
Loss in 600 steps: 0.9873407483100891 0.4798557758331299, 0.5074849128723145
Loss in 700 steps: 0.9848867654800415 0.47680938243865967, 0.5080773830413818
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9699345231056213 0.47496721148490906, 0.49496734142303467
Loss in 100 steps: 0.9814282655715942 0.48081254959106445, 0.5006157159805298
Loss in 200 steps: 1.0033197402954102 0.5005393028259277, 0.5027803182601929
Loss in 300 steps: 0.9696929454803467 0.4697505235671997, 0.4999423921108246
Loss in 400 steps: 0.9508049488067627 0.4573223888874054, 0.4934825897216797
Loss in 500 steps: 0.9901465177536011 0.48329800367355347, 0.5068485140800476
Loss in 600 steps: 0.9741587042808533 0.4680978059768677, 0.5060608983039856
Loss in 700 steps: 0.9825218319892883 0.4750956594944, 0.5074262022972107
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9659451842308044 0.4708392918109894, 0.49510595202445984
Loss in 100 steps: 0.9743231534957886 0.4749707579612732, 0.4993523359298706
Loss in 200 steps: 0.9948337078094482 0.49070391058921814, 0.5041297674179077
Loss in 300 steps: 0.9645770788192749 0.4651901125907898, 0.49938690662384033
Loss in 400 steps: 0.9585320949554443 0.46517500281333923, 0.4933570921421051
Loss in 500 steps: 0.9810606837272644 0.48289594054222107, 0.4981646239757538
Loss in 600 steps: 0.9686462879180908 0.46289896965026855, 0.5057472586631775
Loss in 700 steps: 0.9818376302719116 0.4743539094924927, 0.507483720779419
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.978799045085907 0.48195192217826843, 0.49684715270996094
Loss in 100 steps: 0.9695535898208618 0.469573438167572, 0.4999801814556122
Loss in 200 steps: 1.0001357793807983 0.4973087012767792, 0.5028271079063416
Loss in 300 steps: 0.961938738822937 0.46178993582725525, 0.5001487731933594
Loss in 400 steps: 0.9514346122741699 0.45800650119781494, 0.493428111076355
Loss in 500 steps: 0.976025402545929 0.47249463200569153, 0.5035308003425598
Loss in 600 steps: 0.9790870547294617 0.4725510776042938, 0.5065358877182007
Loss in 700 steps: 0.9822136163711548 0.47515562176704407, 0.5070579648017883
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.955382764339447 0.46206268668174744, 0.493320107460022
Loss in 100 steps: 0.977084219455719 0.47602379322052, 0.501060426235199
Loss in 200 steps: 1.0020132064819336 0.49928465485572815, 0.5027285814285278
Loss in 300 steps: 0.976962149143219 0.47596755623817444, 0.5009946227073669
Loss in 400 steps: 0.9646196961402893 0.4689734876155853, 0.4956461489200592
Loss in 500 steps: 0.9836667776107788 0.4789700210094452, 0.504696786403656
Loss in 600 steps: 0.9836385846138 0.47512513399124146, 0.5085134506225586
Loss in 700 steps: 0.9885085225105286 0.4810834527015686, 0.5074251294136047
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9720208048820496 0.4759794771671295, 0.49604129791259766
Loss in 100 steps: 0.9770264029502869 0.4769168794155121, 0.5001095533370972
Loss in 200 steps: 0.9976073503494263 0.4940902888774872, 0.5035170316696167
Loss in 300 steps: 0.9750489592552185 0.4753396511077881, 0.4997093081474304
Loss in 400 steps: 0.9500473141670227 0.4552780091762543, 0.49476930499076843
Loss in 500 steps: 0.9938275814056396 0.48699951171875, 0.5068279504776001
Loss in 600 steps: 0.9819570779800415 0.47609972953796387, 0.5058574676513672
Loss in 700 steps: 0.9891429543495178 0.48077020049095154, 0.5083726644515991
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9680274128913879 0.4739920198917389, 0.4940353333950043
Loss in 100 steps: 0.9801430702209473 0.47915250062942505, 0.5009905099868774
Loss in 200 steps: 0.995952308177948 0.49326175451278687, 0.5026906132698059
Loss in 300 steps: 0.9715893268585205 0.4718630611896515, 0.49972623586654663
Loss in 400 steps: 0.9566186666488647 0.46254777908325195, 0.4940708875656128
Loss in 500 steps: 0.9935752153396606 0.488675594329834, 0.5048996210098267
Loss in 600 steps: 1.0022248029708862 0.4932083189487457, 0.5090164542198181
Loss in 700 steps: 0.9848418235778809 0.4774892330169678, 0.5073525309562683
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.970332145690918 0.4749528765678406, 0.4953792691230774
Loss in 100 steps: 0.975551426410675 0.4755162000656128, 0.5000352263450623
Loss in 200 steps: 0.9849862456321716 0.48172521591186523, 0.5032609701156616
Loss in 300 steps: 0.9833423495292664 0.4821157157421112, 0.5012266635894775
Loss in 400 steps: 0.958204448223114 0.4630371630191803, 0.4951673448085785
Loss in 500 steps: 0.9807085394859314 0.4778854548931122, 0.5028231143951416
Loss in 600 steps: 0.983905017375946 0.47797903418540955, 0.5059259533882141
Loss in 700 steps: 0.9857860207557678 0.4781796634197235, 0.5076064467430115
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9710224866867065 0.4742671549320221, 0.4967554211616516
Loss in 100 steps: 0.9736517667770386 0.47422099113464355, 0.4994308054447174
Loss in 200 steps: 0.9931733012199402 0.48970741033554077, 0.5034659504890442
Loss in 300 steps: 0.9741454124450684 0.4749543070793152, 0.4991910457611084
Loss in 400 steps: 0.9635283350944519 0.4671661853790283, 0.4963621497154236
Loss in 500 steps: 0.9939475059509277 0.4865702688694, 0.5073772668838501
Loss in 600 steps: 0.9900601506233215 0.483308881521225, 0.506751298904419
Loss in 700 steps: 0.9815848469734192 0.4744286835193634, 0.507156252861023
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9646711349487305 0.4697073996067047, 0.4949636459350586
Loss in 100 steps: 0.971516489982605 0.47226035594940186, 0.4992561638355255
Loss in 200 steps: 0.992840051651001 0.4877845346927643, 0.5050554871559143
Loss in 300 steps: 0.9733337163925171 0.47284430265426636, 0.5004894137382507
Loss in 400 steps: 0.9545932412147522 0.46066316962242126, 0.49393007159233093
Loss in 500 steps: 0.9775580763816833 0.4781838655471802, 0.4993741512298584
Loss in 600 steps: 0.9770076870918274 0.4706410765647888, 0.5063665509223938
Loss in 700 steps: 0.9756184816360474 0.46830543875694275, 0.5073129534721375
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9716905951499939 0.4761720895767212, 0.4955184757709503
Loss in 100 steps: 0.9627072215080261 0.46410587430000305, 0.49860134720802307
Loss in 200 steps: 1.0025266408920288 0.49897560477256775, 0.5035510659217834
Loss in 300 steps: 0.9712650775909424 0.4724029004573822, 0.4988621771335602
Loss in 400 steps: 0.9583519697189331 0.4623141586780548, 0.4960377514362335
Loss in 500 steps: 0.987701952457428 0.4859188199043274, 0.5017830729484558
Loss in 600 steps: 0.9941254258155823 0.485700398683548, 0.5084249377250671
Loss in 700 steps: 0.9763867259025574 0.4696906805038452, 0.5066959857940674
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9647122621536255 0.4700266420841217, 0.49468564987182617
Loss in 100 steps: 0.9738544225692749 0.47546982765197754, 0.49838459491729736
Loss in 200 steps: 0.9970741271972656 0.494219571352005, 0.5028545260429382
Loss in 300 steps: 0.969133734703064 0.4693397879600525, 0.49979403614997864
Loss in 400 steps: 0.955464780330658 0.46128177642822266, 0.4941830635070801
Loss in 500 steps: 0.9837557077407837 0.48281362652778625, 0.5009421110153198
Loss in 600 steps: 0.9776706695556641 0.4716227948665619, 0.5060479044914246
Loss in 700 steps: 0.9861164689064026 0.4785277843475342, 0.5075886845588684
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9695342779159546 0.47318035364151, 0.49635398387908936
Loss in 100 steps: 0.9801610708236694 0.47948190569877625, 0.5006791353225708
Loss in 200 steps: 0.9967041015625 0.4928475320339203, 0.5038566589355469
Loss in 300 steps: 0.9729416370391846 0.47258245944976807, 0.5003591179847717
Loss in 400 steps: 0.9631800055503845 0.46808382868766785, 0.4950961470603943
Loss in 500 steps: 0.9891269207000732 0.48486626148223877, 0.5042607188224792
Loss in 600 steps: 0.9932879209518433 0.48516884446144104, 0.5081191062927246
Loss in 700 steps: 0.9791629910469055 0.4717550575733185, 0.5074079036712646
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9628486037254333 0.4684460461139679, 0.49440261721611023
Loss in 100 steps: 0.9756882190704346 0.4760655164718628, 0.49962276220321655
Loss in 200 steps: 0.9917780160903931 0.4879739582538605, 0.503804087638855
Loss in 300 steps: 0.9786539673805237 0.47722572088241577, 0.5014282464981079
Loss in 400 steps: 0.9585137963294983 0.4651561379432678, 0.49335765838623047
Loss in 500 steps: 0.9791983962059021 0.48011741042137146, 0.499081015586853
Loss in 600 steps: 0.9875002503395081 0.4807190001010895, 0.5067812204360962
Loss in 700 steps: 0.9930978417396545 0.48443493247032166, 0.5086629390716553
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9634227752685547 0.4690391719341278, 0.4943836033344269
Loss in 100 steps: 0.9843822717666626 0.4833012521266937, 0.5010810494422913
Loss in 200 steps: 0.9976603984832764 0.49418163299560547, 0.5034787058830261
Loss in 300 steps: 0.972501814365387 0.47076091170310974, 0.5017408132553101
Loss in 400 steps: 0.9571865200996399 0.46266499161720276, 0.4945215582847595
Loss in 500 steps: 0.9997192025184631 0.4953947067260742, 0.5043244361877441
Loss in 600 steps: 0.9840027689933777 0.4766620397567749, 0.5073407888412476
Loss in 700 steps: 0.9833293557167053 0.4766964018344879, 0.5066330432891846
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9699829816818237 0.47452646493911743, 0.4954565763473511
Loss in 100 steps: 0.9681086540222168 0.4684445858001709, 0.4996640682220459
Loss in 200 steps: 0.9933620095252991 0.49110788106918335, 0.5022541284561157
Loss in 300 steps: 0.9798932075500488 0.478491872549057, 0.5014013051986694
Loss in 400 steps: 0.9566953182220459 0.46174129843711853, 0.49495401978492737
Loss in 500 steps: 0.9771715402603149 0.47604814171791077, 0.5011234283447266
Loss in 600 steps: 0.9826102256774902 0.47602224349975586, 0.5065880417823792
Loss in 700 steps: 0.9873467683792114 0.4812098443508148, 0.506136953830719
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.972476601600647 0.47502484917640686, 0.4974518120288849
Loss in 100 steps: 0.9725488424301147 0.4720246493816376, 0.5005241632461548
Loss in 200 steps: 0.9964484572410583 0.49261990189552307, 0.5038285255432129
Loss in 300 steps: 0.974607527256012 0.4740467369556427, 0.5005607008934021
Loss in 400 steps: 0.9652564525604248 0.4698444604873657, 0.49541205167770386
Loss in 500 steps: 0.9900436997413635 0.48553943634033203, 0.5045042634010315
Loss in 600 steps: 0.9889913201332092 0.4820687770843506, 0.5069225430488586
Loss in 700 steps: 0.9775890707969666 0.46996182203292847, 0.5076272487640381
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9684418439865112 0.4724150002002716, 0.49602681398391724
Loss in 100 steps: 0.9724666476249695 0.4724566340446472, 0.500010073184967
Loss in 200 steps: 0.9861493706703186 0.48276710510253906, 0.5033822059631348
Loss in 300 steps: 0.9746602773666382 0.47500428557395935, 0.4996558725833893
Loss in 400 steps: 0.9534234404563904 0.45904478430747986, 0.4943786561489105
Loss in 500 steps: 0.9923528432846069 0.4877391755580902, 0.5046136975288391
Loss in 600 steps: 0.9851087331771851 0.4790375828742981, 0.5060710906982422
Loss in 700 steps: 0.990480899810791 0.48295292258262634, 0.5075280070304871
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9792445302009583 0.4822973906993866, 0.49694710969924927
Loss in 100 steps: 0.9843928813934326 0.48339468240737915, 0.5009981989860535
Loss in 200 steps: 0.9873903393745422 0.48239633440971375, 0.5049940347671509
Loss in 300 steps: 0.9713606238365173 0.47284942865371704, 0.4985111951828003
Loss in 400 steps: 0.9620378613471985 0.4654900133609772, 0.4965479075908661
Loss in 500 steps: 0.9780224561691284 0.47623971104621887, 0.5017828345298767
Loss in 600 steps: 0.9745612144470215 0.4679914116859436, 0.5065698027610779
Loss in 700 steps: 0.9888799786567688 0.48008835315704346, 0.5087916851043701
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9696089625358582 0.473188579082489, 0.49642041325569153
Loss in 100 steps: 0.9705783128738403 0.47033238410949707, 0.500245988368988
Loss in 200 steps: 0.9936911463737488 0.4895345866680145, 0.5041565895080566
Loss in 300 steps: 0.9753931760787964 0.4764464497566223, 0.4989466667175293
Loss in 400 steps: 0.9635762572288513 0.4672248065471649, 0.4963514506816864
Loss in 500 steps: 0.9659212827682495 0.46907714009284973, 0.49684420228004456
Loss in 600 steps: 0.9863455295562744 0.4802432656288147, 0.5061023831367493
Loss in 700 steps: 0.9860946536064148 0.47852060198783875, 0.5075740218162537
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9666802883148193 0.4717685282230377, 0.4949117600917816
Loss in 100 steps: 0.9755619168281555 0.47589224576950073, 0.4996696710586548
Loss in 200 steps: 0.9852127432823181 0.48107919096946716, 0.5041335821151733
Loss in 300 steps: 0.9682807922363281 0.4674140214920044, 0.500866711139679
Loss in 400 steps: 0.9639014601707458 0.4672865867614746, 0.49661490321159363
Loss in 500 steps: 0.996788740158081 0.4917243421077728, 0.5050644874572754
Loss in 600 steps: 0.9839107990264893 0.4773609936237335, 0.5065498352050781
Loss in 700 steps: 0.9849328398704529 0.4786377251148224, 0.5062950849533081
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9771209955215454 0.47999128699302673, 0.4971296787261963
Loss in 100 steps: 0.9729059338569641 0.4748692512512207, 0.498036652803421
Loss in 200 steps: 0.9979882836341858 0.4937785565853119, 0.5042096972465515
Loss in 300 steps: 0.972894012928009 0.4736369252204895, 0.49925708770751953
Loss in 400 steps: 0.9568377733230591 0.46234941482543945, 0.49448832869529724
Loss in 500 steps: 0.961765706539154 0.46520698070526123, 0.4965587258338928
Loss in 600 steps: 0.995859682559967 0.48942330479621887, 0.5064364075660706
Loss in 700 steps: 0.9808926582336426 0.47416743636131287, 0.5067252516746521
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9668236374855042 0.47046181559562683, 0.4963618218898773
Loss in 100 steps: 0.9697692394256592 0.47019311785697937, 0.4995761811733246
Loss in 200 steps: 0.9881294369697571 0.48532843589782715, 0.5028010010719299
Loss in 300 steps: 0.9658847451210022 0.4693419933319092, 0.4965427815914154
Loss in 400 steps: 0.9584913849830627 0.46261724829673767, 0.4958741068840027
Loss in 500 steps: 0.9797356128692627 0.4764259457588196, 0.5033096075057983
Loss in 600 steps: 0.9935863018035889 0.4863200783729553, 0.5072662234306335
Loss in 700 steps: 0.9793609380722046 0.47346746921539307, 0.5058934688568115
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9663321375846863 0.47054946422576904, 0.49578264355659485
Loss in 100 steps: 0.9779933094978333 0.47789743542671204, 0.5000959038734436
Loss in 200 steps: 0.9880521893501282 0.4849129915237427, 0.5031392574310303
Loss in 300 steps: 0.9731407165527344 0.47515180706977844, 0.49798890948295593
Loss in 400 steps: 0.9640583395957947 0.46732789278030396, 0.4967304468154907
Loss in 500 steps: 0.9721100330352783 0.47118207812309265, 0.5009279847145081
Loss in 600 steps: 0.9878215193748474 0.48130667209625244, 0.5065149068832397
Loss in 700 steps: 0.98447585105896 0.4780106246471405, 0.5064651966094971
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9700648784637451 0.47446486353874207, 0.4956000745296478
Loss in 100 steps: 0.9656935334205627 0.4667237102985382, 0.4989698827266693
Loss in 200 steps: 0.9922885298728943 0.4900996685028076, 0.5021888613700867
Loss in 300 steps: 0.963560938835144 0.46540364623069763, 0.498157262802124
Loss in 400 steps: 0.9659945368766785 0.4713533818721771, 0.4946412444114685
Loss in 500 steps: 0.979260265827179 0.4790158271789551, 0.5002444386482239
Loss in 600 steps: 0.9728758931159973 0.4673066735267639, 0.5055692791938782
Loss in 700 steps: 0.9829881191253662 0.4759233593940735, 0.507064700126648
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9668866991996765 0.47101977467536926, 0.49586692452430725
Loss in 100 steps: 0.968464732170105 0.4700014591217041, 0.49846333265304565
Loss in 200 steps: 0.9943485260009766 0.49150219559669495, 0.502846360206604
Loss in 300 steps: 0.9590319991111755 0.4598606526851654, 0.49917134642601013
Loss in 400 steps: 0.9632729291915894 0.46812862157821655, 0.4951443374156952
Loss in 500 steps: 0.9722142219543457 0.47447675466537476, 0.49773740768432617
Loss in 600 steps: 0.9887176752090454 0.48213618993759155, 0.5065814852714539
Loss in 700 steps: 0.9848109483718872 0.47831791639328003, 0.5064929723739624
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9649291634559631 0.4710870087146759, 0.49384209513664246
Loss in 100 steps: 0.9745651483535767 0.47413572669029236, 0.5004295110702515
Loss in 200 steps: 0.9925753474235535 0.4894847571849823, 0.5030905604362488
Loss in 300 steps: 0.9681038856506348 0.4705290198326111, 0.4975748360157013
Loss in 400 steps: 0.9642295241355896 0.46910494565963745, 0.49512460827827454
Loss in 500 steps: 0.9719017744064331 0.472358763217926, 0.4995429217815399
Loss in 600 steps: 0.983749508857727 0.47571107745170593, 0.5080384016036987
Loss in 700 steps: 0.9898292422294617 0.48208317160606384, 0.507746160030365
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9621261358261108 0.465655118227005, 0.49647098779678345
Loss in 100 steps: 0.9748490452766418 0.4743186831474304, 0.5005303621292114
Loss in 200 steps: 0.9854003190994263 0.482067734003067, 0.5033324956893921
Loss in 300 steps: 0.9701586961746216 0.472009539604187, 0.4981491267681122
Loss in 400 steps: 0.966842532157898 0.4709414839744568, 0.49590107798576355
Loss in 500 steps: 0.9797475337982178 0.47750619053840637, 0.5022413730621338
Loss in 600 steps: 0.9869429469108582 0.4815165102481842, 0.5054264068603516
Loss in 700 steps: 0.9791030287742615 0.47196313738822937, 0.5071398615837097
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9710840582847595 0.4755549430847168, 0.49552905559539795
Loss in 100 steps: 0.9699058532714844 0.47075003385543823, 0.49915584921836853
Loss in 200 steps: 0.9966916441917419 0.49328044056892395, 0.5034111738204956
Loss in 300 steps: 0.9722560048103333 0.4746370315551758, 0.4976189434528351
Loss in 400 steps: 0.9632235169410706 0.467605859041214, 0.49561765789985657
Loss in 500 steps: 0.9828283786773682 0.48035499453544617, 0.5024734139442444
Loss in 600 steps: 0.9845967292785645 0.47857847809791565, 0.5060181617736816
Loss in 700 steps: 0.9839742183685303 0.47684380412101746, 0.5071303844451904
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9775694012641907 0.4820448160171509, 0.49552464485168457
Loss in 100 steps: 0.9632412791252136 0.46500495076179504, 0.49823635816574097
Loss in 200 steps: 1.0011699199676514 0.4978160262107849, 0.5033539533615112
Loss in 300 steps: 0.9700170755386353 0.47184228897094727, 0.498174786567688
Loss in 400 steps: 0.9521761536598206 0.4590533375740051, 0.49312281608581543
Loss in 500 steps: 0.9780586957931519 0.4764908254146576, 0.5015679001808167
Loss in 600 steps: 0.9798083901405334 0.47446104884147644, 0.5053473711013794
Loss in 700 steps: 0.9790579676628113 0.47254258394241333, 0.5065153241157532
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9803204536437988 0.4837087094783783, 0.4966118335723877
Loss in 100 steps: 0.9695584774017334 0.47012409567832947, 0.49943438172340393
Loss in 200 steps: 1.0018744468688965 0.49779921770095825, 0.5040751695632935
Loss in 300 steps: 0.9647817015647888 0.46608105301856995, 0.49870067834854126
Loss in 400 steps: 0.9711253046989441 0.47592633962631226, 0.49519893527030945
Loss in 500 steps: 0.9475213289260864 0.4541662633419037, 0.49335506558418274
Loss in 600 steps: 0.9869813919067383 0.4810178279876709, 0.5059635043144226
Loss in 700 steps: 0.9891958832740784 0.4808032512664795, 0.5083925724029541
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9720130562782288 0.47646820545196533, 0.49554479122161865
Loss in 100 steps: 0.9799860119819641 0.4798760712146759, 0.500109851360321
Loss in 200 steps: 1.0024287700653076 0.49907195568084717, 0.5033568739891052
Loss in 300 steps: 0.9646093249320984 0.4657645523548126, 0.49884483218193054
Loss in 400 steps: 0.9585620760917664 0.4658272862434387, 0.49273473024368286
Loss in 500 steps: 0.9507102966308594 0.4594583809375763, 0.4912518858909607
Loss in 600 steps: 0.9817616939544678 0.4759376347064972, 0.5058241486549377
Loss in 700 steps: 0.9879170656204224 0.48045387864112854, 0.507463276386261
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9762367010116577 0.48087581992149353, 0.4953609108924866
Loss in 100 steps: 0.9705536961555481 0.4710563123226166, 0.49949735403060913
Loss in 200 steps: 0.990688145160675 0.4879668354988098, 0.50272136926651
Loss in 300 steps: 0.9736127257347107 0.4745875895023346, 0.4990251362323761
Loss in 400 steps: 0.9640486240386963 0.4684431254863739, 0.4956055283546448
Loss in 500 steps: 0.9747084379196167 0.47758054733276367, 0.497127890586853
Loss in 600 steps: 0.9865612983703613 0.4805613160133362, 0.5059999823570251
Loss in 700 steps: 0.9835161566734314 0.475482314825058, 0.508033812046051
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9819177389144897 0.4841378629207611, 0.497779905796051
Loss in 100 steps: 0.968365490436554 0.468738853931427, 0.49962666630744934
Loss in 200 steps: 1.0007991790771484 0.4971989393234253, 0.5036001801490784
Loss in 300 steps: 0.9593778848648071 0.46238869428634644, 0.4969891607761383
Loss in 400 steps: 0.9782003164291382 0.4813586473464966, 0.49684157967567444
Loss in 500 steps: 0.948996901512146 0.45654359459877014, 0.49245336651802063
Loss in 600 steps: 0.9935120940208435 0.48712655901908875, 0.5063855648040771
Loss in 700 steps: 0.9715842604637146 0.4656069576740265, 0.505977213382721
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9836398363113403 0.4862833023071289, 0.4973565340042114
Loss in 100 steps: 0.9723212718963623 0.47356873750686646, 0.498752623796463
Loss in 200 steps: 0.9862925410270691 0.48336824774742126, 0.502924382686615
Loss in 300 steps: 0.965080738067627 0.46734219789505005, 0.4977385103702545
Loss in 400 steps: 0.9639936685562134 0.4687517285346985, 0.4952419698238373
Loss in 500 steps: 0.9674044251441956 0.47041055560112, 0.4969938099384308
Loss in 600 steps: 0.9963028430938721 0.48909303545951843, 0.5072097778320312
Loss in 700 steps: 0.9894804954528809 0.48301854729652405, 0.5064619183540344
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9719650745391846 0.4768281579017639, 0.4951370060443878
Loss in 100 steps: 0.9713449478149414 0.47286456823349, 0.4984803795814514
Loss in 200 steps: 0.993217408657074 0.4919986128807068, 0.5012187957763672
Loss in 300 steps: 0.9681823253631592 0.46875691413879395, 0.4994254410266876
Loss in 400 steps: 0.9606595039367676 0.46435192227363586, 0.4963076114654541
Loss in 500 steps: 0.9599131345748901 0.4630681276321411, 0.4968450963497162
Loss in 600 steps: 0.9800269603729248 0.47372710704803467, 0.5062999129295349
Loss in 700 steps: 0.9866448044776917 0.47906234860420227, 0.5075824856758118
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9743573665618896 0.47853589057922363, 0.4958215355873108
Loss in 100 steps: 0.9753931164741516 0.47583287954330444, 0.49956029653549194
Loss in 200 steps: 0.9898470044136047 0.4876064360141754, 0.5022406578063965
Loss in 300 steps: 0.9594529867172241 0.46107006072998047, 0.49838295578956604
Loss in 400 steps: 0.9660929441452026 0.4711019992828369, 0.49499085545539856
Loss in 500 steps: 0.9649564623832703 0.4675617814064026, 0.4973946511745453
Loss in 600 steps: 0.9814209938049316 0.4749128222465515, 0.5065082311630249
Loss in 700 steps: 0.9712451100349426 0.4655173122882843, 0.5057278275489807
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9788933992385864 0.48114582896232605, 0.49774760007858276
Loss in 100 steps: 0.9747072458267212 0.4756593704223633, 0.4990478754043579
Loss in 200 steps: 0.9830171465873718 0.47991272807121277, 0.5031043887138367
Loss in 300 steps: 0.9651985168457031 0.46833857893943787, 0.49685993790626526
Loss in 400 steps: 0.9612618684768677 0.46545541286468506, 0.495806485414505
Loss in 500 steps: 0.9508606791496277 0.4598058760166168, 0.49105483293533325
Loss in 600 steps: 0.9953592419624329 0.488071084022522, 0.5072880983352661
Loss in 700 steps: 0.980392575263977 0.4741128981113434, 0.506279706954956
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9706816673278809 0.4737543761730194, 0.4969273507595062
Loss in 100 steps: 0.9764983057975769 0.47849443554878235, 0.49800387024879456
Loss in 200 steps: 0.9760044813156128 0.47367486357688904, 0.5023297071456909
Loss in 300 steps: 0.9752233028411865 0.47725656628608704, 0.49796679615974426
Loss in 400 steps: 0.960073709487915 0.46474385261535645, 0.4953298568725586
Loss in 500 steps: 0.9512284398078918 0.4552484452724457, 0.49598005414009094
Loss in 600 steps: 0.9772814512252808 0.47151634097099304, 0.5057650208473206
Loss in 700 steps: 0.987586259841919 0.4810129702091217, 0.5065731406211853
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9722530245780945 0.47514960169792175, 0.49710339307785034
Loss in 100 steps: 0.977647066116333 0.4772533178329468, 0.5003937482833862
Loss in 200 steps: 0.9755204916000366 0.47452011704444885, 0.5010004639625549
Loss in 300 steps: 0.9780076742172241 0.4794099032878876, 0.4985977113246918
Loss in 400 steps: 0.9657161235809326 0.47064223885536194, 0.4950738847255707
Loss in 500 steps: 0.9690890908241272 0.4715179204940796, 0.4975711405277252
Loss in 600 steps: 0.985370934009552 0.4800402522087097, 0.5053306818008423
Loss in 700 steps: 0.9745338559150696 0.4679875671863556, 0.5065461993217468
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9782139658927917 0.4794500470161438, 0.4987638294696808
Loss in 100 steps: 0.9723856449127197 0.473141610622406, 0.49924400448799133
Loss in 200 steps: 0.9814473390579224 0.4798114597797394, 0.5016358494758606
Loss in 300 steps: 0.9714299440383911 0.47263097763061523, 0.49879905581474304
Loss in 400 steps: 0.9649658799171448 0.47060856223106384, 0.49435725808143616
Loss in 500 steps: 0.952696681022644 0.4602355360984802, 0.49246108531951904
Loss in 600 steps: 0.9868922829627991 0.47987762093544006, 0.5070146322250366
Loss in 700 steps: 0.9828982353210449 0.4761708378791809, 0.506727397441864
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9722216725349426 0.4759754240512848, 0.49624618887901306
Loss in 100 steps: 0.9789443016052246 0.47920364141464233, 0.4997406303882599
Loss in 200 steps: 0.9714218974113464 0.4698442220687866, 0.5015776753425598
Loss in 300 steps: 0.9617427587509155 0.4645300507545471, 0.4972127377986908
Loss in 400 steps: 0.9658558964729309 0.47182735800743103, 0.4940285086631775
Loss in 500 steps: 0.9579809904098511 0.46435999870300293, 0.49362093210220337
Loss in 600 steps: 0.9883453845977783 0.48181411623954773, 0.5065312385559082
Loss in 700 steps: 0.9847967624664307 0.4771042764186859, 0.5076925158500671
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9726887941360474 0.475955992937088, 0.49673280119895935
Loss in 100 steps: 0.966809093952179 0.46873903274536133, 0.49807009100914
Loss in 200 steps: 0.97409588098526 0.4707561731338501, 0.5033397078514099
Loss in 300 steps: 0.9651985764503479 0.46792954206466675, 0.49726900458335876
Loss in 400 steps: 0.9663798213005066 0.4723960757255554, 0.49398377537727356
Loss in 500 steps: 0.9581590294837952 0.46600496768951416, 0.4921540319919586
Loss in 600 steps: 0.9860062599182129 0.48107391595840454, 0.5049323439598083
Loss in 700 steps: 0.9793861508369446 0.47231894731521606, 0.5070672035217285
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9621688723564148 0.46763283014297485, 0.49453604221343994
Loss in 100 steps: 0.9750081896781921 0.4758957624435425, 0.49911242723464966
Loss in 200 steps: 0.9736126065254211 0.4725240468978882, 0.501088559627533
Loss in 300 steps: 0.963375985622406 0.4671538174152374, 0.4962221086025238
Loss in 400 steps: 0.9736208319664001 0.4776289463043213, 0.49599188566207886
Loss in 500 steps: 0.9487699866294861 0.4570496380329132, 0.49172037839889526
Loss in 600 steps: 0.9833552837371826 0.4767003655433655, 0.5066549181938171
Loss in 700 steps: 0.9783205389976501 0.47247859835624695, 0.5058419108390808
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.978309154510498 0.4811481237411499, 0.49716106057167053
Loss in 100 steps: 0.9727287888526917 0.4733941853046417, 0.4993346035480499
Loss in 200 steps: 0.9784529805183411 0.4770386815071106, 0.5014142990112305
Loss in 300 steps: 0.9694406390190125 0.4711124300956726, 0.49832820892333984
Loss in 400 steps: 0.9653325080871582 0.4713459014892578, 0.49398666620254517
Loss in 500 steps: 0.9665106534957886 0.47006210684776306, 0.4964485466480255
Loss in 600 steps: 0.9930095076560974 0.4875222146511078, 0.5054872632026672
Loss in 700 steps: 0.9793764352798462 0.4723992943763733, 0.5069770812988281
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9668468236923218 0.4710651636123657, 0.49578166007995605
Loss in 100 steps: 0.9760114550590515 0.478299081325531, 0.4977123737335205
Loss in 200 steps: 0.971528172492981 0.47096574306488037, 0.5005624294281006
Loss in 300 steps: 0.9660041332244873 0.4692862927913666, 0.49671781063079834
Loss in 400 steps: 0.9752702713012695 0.47932547330856323, 0.4959447681903839
Loss in 500 steps: 0.9704673290252686 0.475199431180954, 0.4952678680419922
Loss in 600 steps: 0.9903886914253235 0.4837508499622345, 0.5066378712654114
Loss in 700 steps: 0.9791635870933533 0.47273242473602295, 0.5064311623573303
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9623610377311707 0.4681522846221924, 0.4942087233066559
Loss in 100 steps: 0.9763616919517517 0.4791712760925293, 0.4971903860569
Loss in 200 steps: 0.9795901775360107 0.47728532552719116, 0.5023048520088196
Loss in 300 steps: 0.9636958837509155 0.46602410078048706, 0.49767178297042847
Loss in 400 steps: 0.9604977965354919 0.465620219707489, 0.4948776662349701
Loss in 500 steps: 0.9639373421669006 0.4688151776790619, 0.4951222538948059
Loss in 600 steps: 0.9918738007545471 0.48772069811820984, 0.5041530728340149
Loss in 700 steps: 0.9792557954788208 0.47238096594810486, 0.5068747401237488
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9616642594337463 0.46612027287483215, 0.4955439269542694
Loss in 100 steps: 0.9767298698425293 0.4769265055656433, 0.4998033940792084
Loss in 200 steps: 0.97406005859375 0.4723927080631256, 0.5016672611236572
Loss in 300 steps: 0.9714615941047668 0.4724166989326477, 0.49904483556747437
Loss in 400 steps: 0.9639652371406555 0.47004809975624084, 0.49391716718673706
Loss in 500 steps: 0.952320396900177 0.45781055092811584, 0.49450987577438354
Loss in 600 steps: 0.983854353427887 0.47831347584724426, 0.5055408477783203
Loss in 700 steps: 0.9707555174827576 0.46433740854263306, 0.5064180493354797
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9721402525901794 0.4745347201824188, 0.497605562210083
Loss in 100 steps: 0.9773378372192383 0.4795599579811096, 0.49777793884277344
Loss in 200 steps: 0.9651186466217041 0.4636109471321106, 0.5015077590942383
Loss in 300 steps: 0.9676961898803711 0.46855705976486206, 0.4991391599178314
Loss in 400 steps: 0.9637453556060791 0.46851715445518494, 0.4952281713485718
Loss in 500 steps: 0.9599347114562988 0.46315011382102966, 0.49678462743759155
Loss in 600 steps: 0.9874445199966431 0.4804125428199768, 0.5070319175720215
Loss in 700 steps: 0.9805457592010498 0.47319063544273376, 0.5073551535606384
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9789656400680542 0.48228025436401367, 0.4966854155063629
Loss in 100 steps: 0.9649007320404053 0.468138188123703, 0.49676254391670227
Loss in 200 steps: 0.9757674336433411 0.4734252691268921, 0.5023422837257385
Loss in 300 steps: 0.9639469385147095 0.46483612060546875, 0.49911078810691833
Loss in 400 steps: 0.9604993462562561 0.4676115810871124, 0.4928877353668213
Loss in 500 steps: 0.9554816484451294 0.4625249207019806, 0.4929567575454712
Loss in 600 steps: 0.9791895151138306 0.4741511642932892, 0.5050382614135742
Loss in 700 steps: 0.9755489826202393 0.4686294496059418, 0.5069195032119751
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9681670665740967 0.4732420742511749, 0.49492505192756653
Loss in 100 steps: 0.9661016464233398 0.46901413798332214, 0.4970875382423401
Loss in 200 steps: 0.9725738763809204 0.47071558237075806, 0.5018582940101624
Loss in 300 steps: 0.9631906151771545 0.46493449807167053, 0.498256117105484
Loss in 400 steps: 0.9549936652183533 0.4626992344856262, 0.49229440093040466
Loss in 500 steps: 0.9551060199737549 0.4630507528781891, 0.4920552968978882
Loss in 600 steps: 0.983377993106842 0.47749626636505127, 0.5058817267417908
Loss in 700 steps: 0.987166702747345 0.47948914766311646, 0.5076775550842285
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9733177423477173 0.4766170382499695, 0.4967006742954254
Loss in 100 steps: 0.9713619947433472 0.4727468490600586, 0.4986151456832886
Loss in 200 steps: 0.9734888672828674 0.4723002016544342, 0.5011887550354004
Loss in 300 steps: 0.9679501056671143 0.46966007351875305, 0.49828994274139404
Loss in 400 steps: 0.9578031897544861 0.46179836988449097, 0.4960048198699951
Loss in 500 steps: 0.9671438336372375 0.4721781313419342, 0.49496564269065857
Loss in 600 steps: 0.9921860694885254 0.48687097430229187, 0.5053151249885559
Loss in 700 steps: 0.9911996722221375 0.48398715257644653, 0.5072124600410461
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9743791222572327 0.4782145917415619, 0.496164470911026
Loss in 100 steps: 0.9620288610458374 0.4655623137950897, 0.4964665472507477
Loss in 200 steps: 0.9843814373016357 0.48220282793045044, 0.5021786093711853
Loss in 300 steps: 0.9711272716522217 0.47114577889442444, 0.4999814033508301
Loss in 400 steps: 0.9622003436088562 0.4674837291240692, 0.4947165846824646
Loss in 500 steps: 0.9623915553092957 0.467191219329834, 0.4952003061771393
Loss in 600 steps: 0.9942222237586975 0.48767027258872986, 0.5065518021583557
Loss in 700 steps: 0.9807531237602234 0.4738215208053589, 0.5069316625595093
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9692273139953613 0.47244346141815186, 0.49678394198417664
Loss in 100 steps: 0.9703847765922546 0.4733794331550598, 0.4970054030418396
Loss in 200 steps: 0.9757679104804993 0.47482138872146606, 0.500946581363678
Loss in 300 steps: 0.9651452302932739 0.4662952721118927, 0.498850017786026
Loss in 400 steps: 0.9494211673736572 0.4550635814666748, 0.4943576157093048
Loss in 500 steps: 0.9686989784240723 0.474342942237854, 0.49435606598854065
Loss in 600 steps: 0.9905647039413452 0.4847371578216553, 0.5058274865150452
Loss in 700 steps: 0.9850234985351562 0.4788101613521576, 0.5062133073806763
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9607979655265808 0.46688953042030334, 0.49390843510627747
Loss in 100 steps: 0.9741441011428833 0.4764687120914459, 0.4976753294467926
Loss in 200 steps: 0.9714991450309753 0.4703652262687683, 0.5011337995529175
Loss in 300 steps: 0.9613127112388611 0.4640553295612335, 0.4972573220729828
Loss in 400 steps: 0.9549379944801331 0.46061792969703674, 0.4943200647830963
Loss in 500 steps: 0.967546284198761 0.4711346924304962, 0.4964115023612976
Loss in 600 steps: 0.9827504754066467 0.47819140553474426, 0.5045590400695801
Loss in 700 steps: 0.9797773957252502 0.47177475690841675, 0.5080026388168335
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9741040468215942 0.47858548164367676, 0.49551859498023987
Loss in 100 steps: 0.9746604561805725 0.47707313299179077, 0.4975873827934265
Loss in 200 steps: 0.9714598655700684 0.47071096301078796, 0.5007488131523132
Loss in 300 steps: 0.9603351354598999 0.46287092566490173, 0.49746423959732056
Loss in 400 steps: 0.9603398442268372 0.46515002846717834, 0.4951898157596588
Loss in 500 steps: 0.9668375849723816 0.47054994106292725, 0.49628764390945435
Loss in 600 steps: 0.9816464185714722 0.47762152552604675, 0.5040249228477478
Loss in 700 steps: 0.9780043959617615 0.4701121747493744, 0.5078922510147095
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9629891514778137 0.4685722291469574, 0.4944169521331787
Loss in 100 steps: 0.9681133031845093 0.4706605076789856, 0.49745282530784607
Loss in 200 steps: 0.9667128920555115 0.4657187759876251, 0.5009942054748535
Loss in 300 steps: 0.9708462357521057 0.47169029712677, 0.4991559088230133
Loss in 400 steps: 0.9600850343704224 0.46612077951431274, 0.4939642548561096
Loss in 500 steps: 0.9670099020004272 0.47350892424583435, 0.4935010075569153
Loss in 600 steps: 0.9775382280349731 0.47188642621040344, 0.5056517720222473
Loss in 700 steps: 0.9835146069526672 0.47698649764060974, 0.5065281391143799
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9664539098739624 0.47047269344329834, 0.49598121643066406
Loss in 100 steps: 0.9795952439308167 0.48168089985847473, 0.49791431427001953
Loss in 200 steps: 0.9669696092605591 0.4656645357608795, 0.501305103302002
Loss in 300 steps: 0.9581935405731201 0.46080297231674194, 0.49739059805870056
Loss in 400 steps: 0.9570295810699463 0.4620639979839325, 0.49496564269065857
Loss in 500 steps: 0.9676874279975891 0.4714784622192383, 0.49620890617370605
Loss in 600 steps: 0.9865528345108032 0.48115256428718567, 0.5054003000259399
Loss in 700 steps: 0.9900835752487183 0.48267072439193726, 0.507412850856781
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9630755186080933 0.46874111890792847, 0.4943344295024872
Loss in 100 steps: 0.9758752584457397 0.47773122787475586, 0.4981440007686615
Loss in 200 steps: 0.9695878028869629 0.4685938060283661, 0.5009940266609192
Loss in 300 steps: 0.9636654257774353 0.46380096673965454, 0.49986451864242554
Loss in 400 steps: 0.9672261476516724 0.4734857380390167, 0.49374035000801086
Loss in 500 steps: 0.9714909195899963 0.4739993214607239, 0.49749165773391724
Loss in 600 steps: 0.9939882159233093 0.48842352628707886, 0.5055647492408752
Loss in 700 steps: 0.9881420135498047 0.48112207651138306, 0.5070199370384216
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9711238145828247 0.4743783175945282, 0.49674537777900696
Loss in 100 steps: 0.9761562347412109 0.47732898592948914, 0.4988272786140442
Loss in 200 steps: 0.9772617816925049 0.4773254692554474, 0.49993619322776794
Loss in 300 steps: 0.9667022228240967 0.46696773171424866, 0.4997345507144928
Loss in 400 steps: 0.9570181965827942 0.4635745584964752, 0.4934436082839966
Loss in 500 steps: 0.980893611907959 0.48038849234580994, 0.5005052089691162
Loss in 600 steps: 0.9768771529197693 0.47342655062675476, 0.5034505724906921
Loss in 700 steps: 0.9779850840568542 0.4715167284011841, 0.5064683556556702
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9578885436058044 0.4633215665817261, 0.49456682801246643
Loss in 100 steps: 0.984538197517395 0.48552146553993225, 0.4990167021751404
Loss in 200 steps: 0.9706165790557861 0.4697861671447754, 0.5008304715156555
Loss in 300 steps: 0.9710676074028015 0.4730214774608612, 0.4980461001396179
Loss in 400 steps: 0.9517595767974854 0.46086621284484863, 0.49089330434799194
Loss in 500 steps: 0.9547128677368164 0.4629335105419159, 0.4917793869972229
Loss in 600 steps: 1.0006884336471558 0.4944446086883545, 0.5062437653541565
Loss in 700 steps: 0.9826852679252625 0.4749622642993927, 0.5077231526374817
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9650397300720215 0.46956825256347656, 0.4954714775085449
Loss in 100 steps: 0.9648361206054688 0.4676699638366699, 0.4971661865711212
Loss in 200 steps: 0.9756584763526917 0.4754856526851654, 0.5001727938652039
Loss in 300 steps: 0.9662042856216431 0.46674877405166626, 0.4994555115699768
Loss in 400 steps: 0.9605575799942017 0.4673512876033783, 0.493206262588501
Loss in 500 steps: 0.9725858569145203 0.4760873317718506, 0.4964984953403473
Loss in 600 steps: 0.9824409484863281 0.47815731167793274, 0.504283607006073
Loss in 700 steps: 0.9918854832649231 0.4844527542591095, 0.5074326992034912
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9747157692909241 0.47957268357276917, 0.49514302611351013
Loss in 100 steps: 0.9714177846908569 0.4742472767829895, 0.49717050790786743
Loss in 200 steps: 0.9691558480262756 0.46838536858558655, 0.5007705092430115
Loss in 300 steps: 0.9622237086296082 0.46495145559310913, 0.4972722828388214
Loss in 400 steps: 0.9620608687400818 0.46814659237861633, 0.49391427636146545
Loss in 500 steps: 0.9718508124351501 0.47292235493659973, 0.498928427696228
Loss in 600 steps: 0.9692302346229553 0.46604007482528687, 0.5031901001930237
Loss in 700 steps: 0.9866026043891907 0.479602575302124, 0.5069999098777771
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9542583227157593 0.45902004837989807, 0.4952382445335388
Loss in 100 steps: 0.9768630266189575 0.4798315465450287, 0.49703145027160645
Loss in 200 steps: 0.9758880138397217 0.4742487967014313, 0.5016393065452576
Loss in 300 steps: 0.9683172702789307 0.4689312279224396, 0.4993860721588135
Loss in 400 steps: 0.9691230058670044 0.4730460047721863, 0.4960770308971405
Loss in 500 steps: 0.970568060874939 0.47554314136505127, 0.4950249493122101
Loss in 600 steps: 0.9859674572944641 0.4806714355945587, 0.5052959322929382
Loss in 700 steps: 0.9897570610046387 0.48199179768562317, 0.5077652335166931
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.962070643901825 0.4673565924167633, 0.49471405148506165
Loss in 100 steps: 0.9760778546333313 0.4784691631793976, 0.4976086914539337
Loss in 200 steps: 0.9750823974609375 0.47338801622390747, 0.5016943216323853
Loss in 300 steps: 0.971133828163147 0.4725774824619293, 0.4985562562942505
Loss in 400 steps: 0.9632943272590637 0.46980080008506775, 0.4934934675693512
Loss in 500 steps: 0.9855014085769653 0.48416978120803833, 0.5013316869735718
Loss in 600 steps: 0.9801074862480164 0.47627976536750793, 0.5038277506828308
Loss in 700 steps: 0.9871624112129211 0.47905421257019043, 0.5081081986427307
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.969830334186554 0.4731213450431824, 0.4967089891433716
Loss in 100 steps: 0.9843670129776001 0.4868093729019165, 0.497557669878006
Loss in 200 steps: 0.965160071849823 0.4638829529285431, 0.5012771487236023
Loss in 300 steps: 0.9697228670120239 0.47073888778686523, 0.4989839792251587
Loss in 400 steps: 0.9490711092948914 0.45755505561828613, 0.4915160536766052
Loss in 500 steps: 0.9642696976661682 0.4709846079349518, 0.49328503012657166
Loss in 600 steps: 0.9914386868476868 0.48675987124443054, 0.5046787858009338
Loss in 700 steps: 0.9869935512542725 0.47942855954170227, 0.5075649619102478
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9748978614807129 0.4772467315196991, 0.4976511597633362
Loss in 100 steps: 0.971686840057373 0.4747416079044342, 0.4969451427459717
Loss in 200 steps: 0.9717782139778137 0.4702625274658203, 0.5015157461166382
Loss in 300 steps: 0.965957760810852 0.46757885813713074, 0.4983788728713989
Loss in 400 steps: 0.9564061760902405 0.46507537364959717, 0.49133074283599854
Loss in 500 steps: 0.9747065901756287 0.4751112461090088, 0.4995953142642975
Loss in 600 steps: 0.9821221828460693 0.4790160655975342, 0.5031060576438904
Loss in 700 steps: 0.98786860704422 0.47992855310440063, 0.5079401135444641
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9627872109413147 0.4673272669315338, 0.4954599440097809
Loss in 100 steps: 0.9755615592002869 0.478351891040802, 0.49720972776412964
Loss in 200 steps: 0.9657680988311768 0.46423307061195374, 0.5015349984169006
Loss in 300 steps: 0.9647443294525146 0.4664594233036041, 0.4982849359512329
Loss in 400 steps: 0.9524973630905151 0.46036672592163086, 0.4921305775642395
Loss in 500 steps: 0.9764483571052551 0.4759904146194458, 0.5004580020904541
Loss in 600 steps: 0.9835168123245239 0.4796714186668396, 0.5038454532623291
Loss in 700 steps: 0.9803395867347717 0.47389426827430725, 0.5064452886581421
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9643937945365906 0.47023075819015503, 0.49416306614875793
Loss in 100 steps: 0.9746805429458618 0.478228896856308, 0.496451735496521
Loss in 200 steps: 0.9835264086723328 0.48177996277809143, 0.5017464756965637
Loss in 300 steps: 0.9651134014129639 0.46842536330223083, 0.49668800830841064
Loss in 400 steps: 0.9546095132827759 0.46302637457847595, 0.49158307909965515
Loss in 500 steps: 0.9912096858024597 0.49039238691329956, 0.5008174180984497
Loss in 600 steps: 0.9945225715637207 0.48872753977775574, 0.5057951211929321
Loss in 700 steps: 0.9935094714164734 0.4865695834159851, 0.5069398283958435
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9774337410926819 0.4828892946243286, 0.4945444166660309
Loss in 100 steps: 0.9648478031158447 0.4667312800884247, 0.49811646342277527
Loss in 200 steps: 0.9739060997962952 0.47248727083206177, 0.5014188289642334
Loss in 300 steps: 0.97466641664505 0.47642576694488525, 0.4982406795024872
Loss in 400 steps: 0.9670953154563904 0.4738270938396454, 0.493268221616745
Loss in 500 steps: 0.9887406826019287 0.48861515522003174, 0.500125527381897
Loss in 600 steps: 0.989885151386261 0.48438867926597595, 0.5054965019226074
Loss in 700 steps: 0.9953430891036987 0.4873877167701721, 0.5079554319381714
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.960373044013977 0.46553370356559753, 0.49483928084373474
Loss in 100 steps: 0.9736156463623047 0.47700703144073486, 0.4966086745262146
Loss in 200 steps: 0.9808862209320068 0.4808136820793152, 0.5000725984573364
Loss in 300 steps: 0.9710417985916138 0.47118091583251953, 0.49986088275909424
Loss in 400 steps: 0.9606386423110962 0.46695148944854736, 0.49368712306022644
Loss in 500 steps: 0.9745359420776367 0.47886958718299866, 0.4956662654876709
Loss in 600 steps: 0.978417158126831 0.47535577416419983, 0.5030613541603088
Loss in 700 steps: 0.9832913279533386 0.47668957710266113, 0.5066017508506775
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9698787927627563 0.4729822874069214, 0.4968964755535126
Loss in 100 steps: 0.9817599058151245 0.4829598069190979, 0.4988000690937042
Loss in 200 steps: 0.9757071137428284 0.47410598397254944, 0.5016011595726013
Loss in 300 steps: 0.9758148789405823 0.47796839475631714, 0.4978465139865875
Loss in 400 steps: 0.9591673612594604 0.4678896963596344, 0.4912776052951813
Loss in 500 steps: 0.9984240531921387 0.4948711097240448, 0.5035529136657715
Loss in 600 steps: 0.9968307614326477 0.4917164146900177, 0.5051143765449524
Loss in 700 steps: 0.9902622699737549 0.48372969031333923, 0.5065325498580933
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9694905877113342 0.47220122814178467, 0.49728938937187195
Loss in 100 steps: 0.9697854518890381 0.4726952314376831, 0.4970901608467102
Loss in 200 steps: 0.970924973487854 0.4689365029335022, 0.5019885301589966
Loss in 300 steps: 0.969963014125824 0.472337007522583, 0.49762600660324097
Loss in 400 steps: 0.948415219783783 0.4561975598335266, 0.4922176003456116
Loss in 500 steps: 0.9615506529808044 0.46672651171684265, 0.4948241710662842
Loss in 600 steps: 0.9902581572532654 0.4845675826072693, 0.5056905746459961
Loss in 700 steps: 0.9916132092475891 0.4842115640640259, 0.5074016451835632
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9726556539535522 0.47665339708328247, 0.4960022568702698
Loss in 100 steps: 0.9755776524543762 0.47841575741767883, 0.497161865234375
Loss in 200 steps: 0.9819621443748474 0.4800039231777191, 0.5019582509994507
Loss in 300 steps: 0.9585086107254028 0.4615274667739868, 0.49698111414909363
Loss in 400 steps: 0.9569347500801086 0.4634760022163391, 0.4934587776660919
Loss in 500 steps: 0.9693232178688049 0.4738330841064453, 0.495490163564682
Loss in 600 steps: 0.9869698286056519 0.48207321763038635, 0.5048966407775879
Loss in 700 steps: 0.9936106204986572 0.4869726598262787, 0.5066378712654114
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9675147533416748 0.47240981459617615, 0.49510493874549866
Loss in 100 steps: 0.9816532731056213 0.4836665093898773, 0.497986763715744
Loss in 200 steps: 0.978618860244751 0.478047639131546, 0.5005711913108826
Loss in 300 steps: 0.9742289185523987 0.47499290108680725, 0.49923601746559143
Loss in 400 steps: 0.9565449953079224 0.46418675780296326, 0.49235814809799194
Loss in 500 steps: 0.9894003868103027 0.48744821548461914, 0.5019521713256836
Loss in 600 steps: 0.9921796321868896 0.48689424991607666, 0.5052854418754578
Loss in 700 steps: 0.9836342930793762 0.4758777320384979, 0.5077565908432007
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9721238017082214 0.4751185178756714, 0.49700528383255005
Loss in 100 steps: 0.9793998003005981 0.48205000162124634, 0.4973497986793518
Loss in 200 steps: 0.973077118396759 0.4710191488265991, 0.5020580291748047
Loss in 300 steps: 0.9670276641845703 0.47022512555122375, 0.49680250883102417
Loss in 400 steps: 0.9601242542266846 0.4666292667388916, 0.49349498748779297
Loss in 500 steps: 0.9803601503372192 0.4815962314605713, 0.49876394867897034
Loss in 600 steps: 0.9800009727478027 0.4762488007545471, 0.5037521719932556
Loss in 700 steps: 0.98826003074646 0.4813382029533386, 0.5069218277931213
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9645267724990845 0.4676777422428131, 0.49684903025627136
Loss in 100 steps: 0.9841935634613037 0.48604410886764526, 0.49814948439598083
Loss in 200 steps: 0.9665346145629883 0.4651939272880554, 0.5013406872749329
Loss in 300 steps: 0.9734163880348206 0.47467681765556335, 0.4987395405769348
Loss in 400 steps: 0.9509788155555725 0.45925065875053406, 0.49172815680503845
Loss in 500 steps: 0.9731018543243408 0.4750420153141022, 0.49805983901023865
Loss in 600 steps: 0.9884539842605591 0.4828481078147888, 0.5056058764457703
Loss in 700 steps: 0.9853567481040955 0.47846105694770813, 0.5068957805633545
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9685415625572205 0.47161105275154114, 0.4969305396080017
Loss in 100 steps: 0.977608859539032 0.4792686104774475, 0.4983402192592621
Loss in 200 steps: 0.9802274107933044 0.47866860032081604, 0.501558780670166
Loss in 300 steps: 0.970901370048523 0.47216668725013733, 0.4987346827983856
Loss in 400 steps: 0.9500254988670349 0.4572220742702484, 0.4928034543991089
Loss in 500 steps: 0.9945484399795532 0.49090686440467834, 0.5036416053771973
Loss in 600 steps: 0.9819994568824768 0.47770795226097107, 0.5042915344238281
Loss in 700 steps: 0.9843791723251343 0.4771748185157776, 0.5072044134140015
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9657549858093262 0.46887147426605225, 0.4968835115432739
Loss in 100 steps: 0.9781643748283386 0.4805239140987396, 0.497640460729599
Loss in 200 steps: 0.9699351787567139 0.4684850871562958, 0.5014501810073853
Loss in 300 steps: 0.9734958410263062 0.4757784605026245, 0.49771741032600403
Loss in 400 steps: 0.9529612064361572 0.4605432450771332, 0.49241793155670166
Loss in 500 steps: 0.980252742767334 0.4804941415786743, 0.4997585415840149
Loss in 600 steps: 0.9903991222381592 0.48540419340133667, 0.5049949288368225
Loss in 700 steps: 0.9878159761428833 0.4813745319843292, 0.5064414739608765
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9609415531158447 0.46439871191978455, 0.4965427815914154
Loss in 100 steps: 0.9805900454521179 0.4820939898490906, 0.49849605560302734
Loss in 200 steps: 0.974236011505127 0.47369492053985596, 0.500541090965271
Loss in 300 steps: 0.969536542892456 0.4703387916088104, 0.49919775128364563
Loss in 400 steps: 0.9548978209495544 0.4625176787376404, 0.4923801124095917
Loss in 500 steps: 0.9857641458511353 0.48424652218818665, 0.501517653465271
Loss in 600 steps: 0.9904588460922241 0.4853101670742035, 0.5051486492156982
Loss in 700 steps: 0.9814563989639282 0.4758477210998535, 0.5056086778640747
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9694575667381287 0.4717143476009369, 0.4977432191371918
Loss in 100 steps: 0.9679772257804871 0.4698798358440399, 0.49809738993644714
Loss in 200 steps: 0.9803717136383057 0.4777390956878662, 0.5026326179504395
Loss in 300 steps: 0.973401665687561 0.47408002614974976, 0.4993216395378113
Loss in 400 steps: 0.9451234340667725 0.45307546854019165, 0.49204787611961365
Loss in 500 steps: 0.9870649576187134 0.4851318895816803, 0.5019331574440002
Loss in 600 steps: 0.9898512959480286 0.4840770661830902, 0.505774199962616
Loss in 700 steps: 0.9860960245132446 0.4792957901954651, 0.5068002343177795
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9623808264732361 0.4662154018878937, 0.49616536498069763
Loss in 100 steps: 0.9692086577415466 0.47227248549461365, 0.496936172246933
Loss in 200 steps: 0.9743685126304626 0.47228649258613586, 0.5020820498466492
Loss in 300 steps: 0.9723888635635376 0.47311335802078247, 0.49927544593811035
Loss in 400 steps: 0.9396899938583374 0.4509044289588928, 0.4887855648994446
Loss in 500 steps: 0.9733451008796692 0.47611159086227417, 0.497233510017395
Loss in 600 steps: 0.9834684729576111 0.4784116744995117, 0.5050567984580994
Loss in 700 steps: 0.991057276725769 0.48496001958847046, 0.5060972571372986
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9696415662765503 0.47384512424468994, 0.49579647183418274
Loss in 100 steps: 0.9757215976715088 0.47966504096984863, 0.4960564970970154
Loss in 200 steps: 0.9731181859970093 0.47134751081466675, 0.5017707347869873
Loss in 300 steps: 0.9765907526016235 0.4775538742542267, 0.49903687834739685
Loss in 400 steps: 0.9510305523872375 0.46046188473701477, 0.49056872725486755
Loss in 500 steps: 0.9712908864021301 0.4743468761444092, 0.49694404006004333
Loss in 600 steps: 0.9945807456970215 0.48871520161628723, 0.5058655738830566
Loss in 700 steps: 0.9910619258880615 0.4849047064781189, 0.5061572194099426
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.959646463394165 0.46228960156440735, 0.49735692143440247
Loss in 100 steps: 0.9711145758628845 0.4755931794643402, 0.4955213665962219
Loss in 200 steps: 0.9796417355537415 0.4778491258621216, 0.5017926692962646
Loss in 300 steps: 0.9664006233215332 0.46807166934013367, 0.4983290433883667
Loss in 400 steps: 0.955126166343689 0.4633624255657196, 0.491763710975647
Loss in 500 steps: 0.9638156294822693 0.4685191810131073, 0.49529650807380676
Loss in 600 steps: 0.984630286693573 0.4796302020549774, 0.5050000548362732
Loss in 700 steps: 0.9889100790023804 0.4823422431945801, 0.5065678954124451
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9612149000167847 0.46567144989967346, 0.4955434501171112
Loss in 100 steps: 0.9771568179130554 0.4795604944229126, 0.49759629368782043
Loss in 200 steps: 0.972672700881958 0.47117874026298523, 0.5014939308166504
Loss in 300 steps: 0.9714507460594177 0.4734102785587311, 0.49804043769836426
Loss in 400 steps: 0.9505888819694519 0.4599880576133728, 0.4906008839607239
Loss in 500 steps: 0.9870984554290771 0.48349475860595703, 0.5036036968231201
Loss in 600 steps: 0.9706494212150574 0.4667869508266449, 0.5038624405860901
Loss in 700 steps: 0.9950810074806213 0.4875282347202301, 0.5075526833534241
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9655362963676453 0.46896225214004517, 0.4965739846229553
Loss in 100 steps: 0.9671890735626221 0.47049087285995483, 0.496698260307312
Loss in 200 steps: 0.9624361395835876 0.46094655990600586, 0.501489520072937
Loss in 300 steps: 0.9735112190246582 0.47508949041366577, 0.49842187762260437
Loss in 400 steps: 0.9435577392578125 0.452343225479126, 0.4912145137786865
Loss in 500 steps: 0.9717057347297668 0.475646436214447, 0.49605923891067505
Loss in 600 steps: 0.9770324230194092 0.47282785177230835, 0.504204511642456
Loss in 700 steps: 0.984960675239563 0.4784482419490814, 0.506512463092804
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9728266000747681 0.4769732356071472, 0.49585336446762085
Loss in 100 steps: 0.9689754843711853 0.4727451503276825, 0.4962303936481476
Loss in 200 steps: 0.9762999415397644 0.47525712847709656, 0.5010427236557007
Loss in 300 steps: 0.9723657369613647 0.4732527732849121, 0.4991130232810974
Loss in 400 steps: 0.9694652557373047 0.4748188257217407, 0.49464643001556396
Loss in 500 steps: 0.9774346947669983 0.48075535893440247, 0.49667927622795105
Loss in 600 steps: 0.9870843291282654 0.48186424374580383, 0.5052201151847839
Loss in 700 steps: 0.9857078194618225 0.4790038466453552, 0.5067039728164673
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9649980068206787 0.4688221216201782, 0.49617594480514526
Loss in 100 steps: 0.9754491448402405 0.4778510332107544, 0.49759814143180847
Loss in 200 steps: 0.9772639274597168 0.4763384461402893, 0.5009254813194275
Loss in 300 steps: 0.9658135175704956 0.46627089381217957, 0.4995426535606384
Loss in 400 steps: 0.9468591809272766 0.457889586687088, 0.4889695942401886
Loss in 500 steps: 0.9659243822097778 0.4704135060310364, 0.4955107569694519
Loss in 600 steps: 0.9737170934677124 0.46964001655578613, 0.5040770173072815
Loss in 700 steps: 0.9786087870597839 0.47163838148117065, 0.5069704055786133
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9726239442825317 0.47555306553840637, 0.497070848941803
Loss in 100 steps: 0.9668795466423035 0.4701383709907532, 0.4967411458492279
Loss in 200 steps: 0.9785749912261963 0.47507593035697937, 0.5034990906715393
Loss in 300 steps: 0.9692667126655579 0.4710889756679535, 0.49817779660224915
Loss in 400 steps: 0.9508844614028931 0.459473580121994, 0.49141085147857666
Loss in 500 steps: 0.9654003977775574 0.4690089523792267, 0.4963914752006531
Loss in 600 steps: 0.9763468503952026 0.47273293137550354, 0.5036140084266663
Loss in 700 steps: 0.984054684638977 0.478000283241272, 0.5060544610023499
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9676997661590576 0.47168174386024475, 0.49601802229881287
Loss in 100 steps: 0.9757503271102905 0.4785975515842438, 0.49715283513069153
Loss in 200 steps: 0.9728643298149109 0.47095370292663574, 0.5019106864929199
Loss in 300 steps: 0.9701611399650574 0.4705932140350342, 0.4995679557323456
Loss in 400 steps: 0.9522255659103394 0.46102550625801086, 0.4912000298500061
Loss in 500 steps: 0.9835613965988159 0.4815005660057068, 0.5020607113838196
Loss in 600 steps: 0.9708142876625061 0.4671750068664551, 0.5036392211914062
Loss in 700 steps: 0.9814395904541016 0.4753965437412262, 0.5060431361198425
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9678799510002136 0.4699028730392456, 0.49797695875167847
Loss in 100 steps: 0.9726138114929199 0.4751640260219574, 0.49744969606399536
Loss in 200 steps: 0.9715901017189026 0.4700806736946106, 0.5015094876289368
Loss in 300 steps: 0.975719690322876 0.4775990843772888, 0.4981204867362976
Loss in 400 steps: 0.9466889500617981 0.4546571671962738, 0.4920317828655243
Loss in 500 steps: 0.9561901688575745 0.4603855609893799, 0.4958045780658722
Loss in 600 steps: 0.9870166778564453 0.4807707965373993, 0.5062458515167236
Loss in 700 steps: 0.9736891984939575 0.46766021847724915, 0.506028950214386
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9722912907600403 0.47518065571784973, 0.49711063504219055
Loss in 100 steps: 0.9773473143577576 0.48039010167121887, 0.4969572424888611
Loss in 200 steps: 0.9660762548446655 0.464616060256958, 0.5014601945877075
Loss in 300 steps: 0.9729905128479004 0.47420820593833923, 0.49878230690956116
Loss in 400 steps: 0.941878080368042 0.45271530747413635, 0.48916277289390564
Loss in 500 steps: 0.9709873795509338 0.47077035903930664, 0.5002170205116272
Loss in 600 steps: 0.9854452610015869 0.4802737236022949, 0.5051714777946472
Loss in 700 steps: 0.9776487350463867 0.4720989465713501, 0.5055497884750366
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9743266701698303 0.47636282444000244, 0.4979638457298279
Loss in 100 steps: 0.9643120765686035 0.4681093692779541, 0.496202677488327
Loss in 200 steps: 0.9700940251350403 0.4695894718170166, 0.5005044937133789
Loss in 300 steps: 0.9711617231369019 0.47153255343437195, 0.4996291697025299
Loss in 400 steps: 0.9367189407348633 0.44884827733039856, 0.48787063360214233
Loss in 500 steps: 0.9744482040405273 0.4794043004512787, 0.49504396319389343
Loss in 600 steps: 0.9891744256019592 0.48460108041763306, 0.5045732855796814
Loss in 700 steps: 0.9770678281784058 0.470474511384964, 0.5065934062004089
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9739223122596741 0.47784918546676636, 0.4960731267929077
Loss in 100 steps: 0.984039843082428 0.4864368438720703, 0.4976029694080353
Loss in 200 steps: 0.970759928226471 0.4683510959148407, 0.5024088621139526
Loss in 300 steps: 0.9769442081451416 0.4780392348766327, 0.4989050626754761
Loss in 400 steps: 0.9509868025779724 0.4617041051387787, 0.48928266763687134
Loss in 500 steps: 0.9560307860374451 0.46334463357925415, 0.4926861822605133
Loss in 600 steps: 0.9749876260757446 0.470522940158844, 0.5044646859169006
Loss in 700 steps: 0.9854196310043335 0.4787091910839081, 0.5067104697227478
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9719504117965698 0.47510847449302673, 0.4968419075012207
Loss in 100 steps: 0.9840538501739502 0.4866008758544922, 0.49745306372642517
Loss in 200 steps: 0.9746713638305664 0.4726702868938446, 0.5020011067390442
Loss in 300 steps: 0.9682088494300842 0.470572829246521, 0.49763599038124084
Loss in 400 steps: 0.9534111618995667 0.4631306529045105, 0.4902805685997009
Loss in 500 steps: 0.983604371547699 0.48167771100997925, 0.5019267201423645
Loss in 600 steps: 0.9628261923789978 0.458844929933548, 0.5039811730384827
Loss in 700 steps: 0.9755644798278809 0.4698711931705475, 0.5056933164596558
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9636233448982239 0.46810969710350037, 0.49551358819007874
Loss in 100 steps: 0.9880728721618652 0.48950907588005066, 0.49856385588645935
Loss in 200 steps: 0.9687069058418274 0.4664815664291382, 0.5022253394126892
Loss in 300 steps: 0.9726787209510803 0.4743310511112213, 0.49834775924682617
Loss in 400 steps: 0.9343456029891968 0.44599947333335876, 0.4883460998535156
Loss in 500 steps: 0.9614018797874451 0.46302878856658936, 0.4983730614185333
Loss in 600 steps: 0.9774664640426636 0.47129026055336, 0.506176233291626
Loss in 700 steps: 0.9725069999694824 0.46612289547920227, 0.5063841938972473
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.97103351354599 0.474224328994751, 0.4968091547489166
Loss in 100 steps: 0.9775511622428894 0.4796306788921356, 0.4979204535484314
Loss in 200 steps: 0.9640154838562012 0.4617152214050293, 0.5023002028465271
Loss in 300 steps: 0.9571971893310547 0.4592948853969574, 0.49790236353874207
Loss in 400 steps: 0.9548915028572083 0.46484577655792236, 0.4900457561016083
Loss in 500 steps: 0.9740965366363525 0.4761994779109955, 0.49789708852767944
Loss in 600 steps: 0.9638744592666626 0.45893076062202454, 0.5049437284469604
Loss in 700 steps: 0.9796674847602844 0.4731804132461548, 0.5064870715141296
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9635389447212219 0.46718287467956543, 0.4963560104370117
Loss in 100 steps: 0.9681716561317444 0.4713667929172516, 0.4968048334121704
Loss in 200 steps: 0.9801465272903442 0.47834447026252747, 0.5018020272254944
Loss in 300 steps: 0.9603406190872192 0.4627766013145447, 0.49756401777267456
Loss in 400 steps: 0.9499461054801941 0.4618346393108368, 0.48811137676239014
Loss in 500 steps: 0.9587035179138184 0.46629413962364197, 0.4924094080924988
Loss in 600 steps: 0.9886674284934998 0.48190629482269287, 0.5067611336708069
Loss in 700 steps: 0.9881320595741272 0.4804808497428894, 0.5076512098312378
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9654151201248169 0.47078937292099, 0.4946258068084717
Loss in 100 steps: 0.979387104511261 0.4814159572124481, 0.49797120690345764
Loss in 200 steps: 0.9820103645324707 0.4798663556575775, 0.5021440386772156
Loss in 300 steps: 0.973708987236023 0.47570380568504333, 0.498005211353302
Loss in 400 steps: 0.9539268612861633 0.46292153000831604, 0.49100539088249207
Loss in 500 steps: 0.9628856182098389 0.46579688787460327, 0.4970887303352356
Loss in 600 steps: 0.9963339567184448 0.48904624581336975, 0.5072876811027527
Loss in 700 steps: 0.9801267385482788 0.4742448627948761, 0.5058817863464355
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9676855206489563 0.47051504254341125, 0.49717050790786743
Loss in 100 steps: 0.9836525321006775 0.48566094040870667, 0.4979914724826813
Loss in 200 steps: 0.987171471118927 0.4849091172218323, 0.5022624135017395
Loss in 300 steps: 0.9613542556762695 0.46481558680534363, 0.4965386688709259
Loss in 400 steps: 0.9506285190582275 0.45842328667640686, 0.4922052025794983
Loss in 500 steps: 0.9396293759346008 0.45065179467201233, 0.4889776110649109
Loss in 600 steps: 0.9908477663993835 0.48508957028388977, 0.5057582259178162
Loss in 700 steps: 0.9908448457717896 0.484079509973526, 0.5067654848098755
Namespace(add_phase_shift=0, batch_size=128, do_eval=0, emb_dimension=50, from_scatch=1, iterations=200, log_step=100, lr=0.001, output='nyt_yao_tiny.txt.norm-output50', text='nyt_yao_tiny.txt.norm', time_scale=1, time_type='word_mixed_fixed', use_time=1, verbose=0, weight_decay=0, years=30)
Loss in 0 steps: 0.9585420489311218 0.4629128575325012, 0.4956292510032654
Loss in 100 steps: 0.9811615943908691 0.4820574223995209, 0.49910423159599304
Loss in 200 steps: 0.9639419913291931 0.4618230164051056, 0.5021188855171204
Loss in 300 steps: 0.9633710384368896 0.4652799069881439, 0.49809110164642334
Loss in 400 steps: 0.9545575976371765 0.4651030898094177, 0.4894544780254364
Loss in 500 steps: 0.9753105044364929 0.4754030406475067, 0.4999074637889862
Loss in 600 steps: 0.989355206489563 0.4837068021297455, 0.5056483745574951
Loss in 700 steps: 0.9903147220611572 0.4838939607143402, 0.5064207315444946
